// AVX-512 IFMA Vector Multiplication for 4-word Fields
// Generated for prototype testing - DO NOT USE IN PRODUCTION

#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

// mulVecIFMA(res, a, b *Element, n uint64)
// Performs n multiplications using AVX-512 IFMA instructions
// Processes 8 elements in parallel using radix-52 representation
TEXT Â·mulVecIFMA(SB), NOSPLIT, $0-32
	MOVQ res+0(FP), R14
	MOVQ a+8(FP), R13
	MOVQ b+16(FP), CX
	MOVQ n+24(FP), BX

	// Load constants for radix-52 conversion and reduction
	MOVQ         $0xFFFFFFFFFFFFF, AX
	VPBROADCASTQ AX, Z31
	MOVQ         $const_qInvNeg, AX
	ANDQ         $0xFFFFFFFFFFFFF, AX
	VPBROADCASTQ AX, Z30

	// Load modulus in radix-52 form
	// q in radix-52: Z25=ql0, Z26=ql1, Z27=ql2, Z28=ql3, Z29=ql4
	// Load q0-q3 and convert to radix-52
	MOVQ         $const_q0, AX
	MOVQ         $const_q1, BX
	MOVQ         $const_q2, CX
	MOVQ         $const_q3, DX
	MOVQ         AX, R8
	ANDQ         $0xFFFFFFFFFFFFF, R8
	VPBROADCASTQ R8, Z25
	SHRQ         $52, AX
	MOVQ         BX, R8
	SHLQ         $12, R8
	ORQ          AX, R8
	ANDQ         $0xFFFFFFFFFFFFF, R8
	VPBROADCASTQ R8, Z26
	SHRQ         $40, BX
	MOVQ         CX, R8
	SHLQ         $24, R8
	ORQ          BX, R8
	ANDQ         $0xFFFFFFFFFFFFF, R8
	VPBROADCASTQ R8, Z27
	SHRQ         $28, CX
	MOVQ         DX, R8
	SHLQ         $36, R8
	ORQ          CX, R8
	ANDQ         $0xFFFFFFFFFFFFF, R8
	VPBROADCASTQ R8, Z28
	SHRQ         $16, DX
	VPBROADCASTQ DX, Z29

loop_1:
	TESTQ BX, BX
	JEQ   done_2 // n == 0, we are done

	// Process 8 elements in parallel
	// Load and convert 8 elements from a[] to radix-52
	// Load 8 elements from R13
	// Load element words using gather pattern
	VMOVDQU64 0(R13), Z10
	VMOVDQU64 64(R13), Z11
	VMOVDQU64 128(R13), Z12
	VMOVDQU64 192(R13), Z13

	// Transpose 8 elements for vertical SIMD processing
	// 8x4 transpose using AVX-512 shuffles
	VSHUFI64X2 $0x44, Z11, Z10, Z18
	VSHUFI64X2 $0xEE, Z11, Z10, Z19
	VSHUFI64X2 $0x44, Z13, Z12, Z20
	VSHUFI64X2 $0xEE, Z13, Z12, Z21
	VSHUFI64X2 $0x88, Z20, Z18, Z14
	VSHUFI64X2 $0xDD, Z20, Z18, Z16
	VSHUFI64X2 $0x88, Z21, Z19, Z15
	VSHUFI64X2 $0xDD, Z21, Z19, Z17
	VPERMQ     $0xD8, Z14, Z14
	VPERMQ     $0xD8, Z15, Z15
	VPERMQ     $0xD8, Z16, Z16
	VPERMQ     $0xD8, Z17, Z17

	// Convert to radix-52
	VPANDQ Z31, Z14, Z0
	VPSRLQ $52, Z14, Z18
	VPSLLQ $12, Z15, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z1
	VPSRLQ $40, Z15, Z18
	VPSLLQ $24, Z16, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z2
	VPSRLQ $28, Z16, Z18
	VPSLLQ $36, Z17, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z3
	VPSRLQ $16, Z17, Z4

	// Load and convert 8 elements from b[] to radix-52
	// Load 8 elements from CX
	// Load element words using gather pattern
	VMOVDQU64 0(CX), Z10
	VMOVDQU64 64(CX), Z11
	VMOVDQU64 128(CX), Z12
	VMOVDQU64 192(CX), Z13

	// Transpose 8 elements for vertical SIMD processing
	// 8x4 transpose using AVX-512 shuffles
	VSHUFI64X2 $0x44, Z11, Z10, Z18
	VSHUFI64X2 $0xEE, Z11, Z10, Z19
	VSHUFI64X2 $0x44, Z13, Z12, Z20
	VSHUFI64X2 $0xEE, Z13, Z12, Z21
	VSHUFI64X2 $0x88, Z20, Z18, Z14
	VSHUFI64X2 $0xDD, Z20, Z18, Z16
	VSHUFI64X2 $0x88, Z21, Z19, Z15
	VSHUFI64X2 $0xDD, Z21, Z19, Z17
	VPERMQ     $0xD8, Z14, Z14
	VPERMQ     $0xD8, Z15, Z15
	VPERMQ     $0xD8, Z16, Z16
	VPERMQ     $0xD8, Z17, Z17

	// Convert to radix-52
	VPANDQ Z31, Z14, Z5
	VPSRLQ $52, Z14, Z18
	VPSLLQ $12, Z15, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z6
	VPSRLQ $40, Z15, Z18
	VPSLLQ $24, Z16, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z7
	VPSRLQ $28, Z16, Z18
	VPSLLQ $36, Z17, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z8
	VPSRLQ $16, Z17, Z9

	// Montgomery multiplication using IFMA (BPS method)
	// Schoolbook multiplication using IFMA
	// A = [Z0, Z1, Z2, Z3, Z4], B = [Z5, Z6, Z7, Z8, Z9]
	VPXORQ      Z10, Z10, Z10
	VPXORQ      Z11, Z11, Z11
	VPXORQ      Z12, Z12, Z12
	VPXORQ      Z13, Z13, Z13
	VPXORQ      Z14, Z14, Z14
	VPXORQ      Z15, Z15, Z15
	VPXORQ      Z16, Z16, Z16
	VPXORQ      Z17, Z17, Z17
	VPXORQ      Z18, Z18, Z18
	VPXORQ      Z19, Z19, Z19
	VPMADD52LUQ Z5, Z0, Z10
	VPMADD52HUQ Z5, Z0, Z11
	VPMADD52LUQ Z6, Z0, Z11
	VPMADD52HUQ Z6, Z0, Z12
	VPMADD52LUQ Z7, Z0, Z12
	VPMADD52HUQ Z7, Z0, Z13
	VPMADD52LUQ Z8, Z0, Z13
	VPMADD52HUQ Z8, Z0, Z14
	VPMADD52LUQ Z9, Z0, Z14
	VPMADD52HUQ Z9, Z0, Z15
	VPMADD52LUQ Z5, Z1, Z11
	VPMADD52HUQ Z5, Z1, Z12
	VPMADD52LUQ Z6, Z1, Z12
	VPMADD52HUQ Z6, Z1, Z13
	VPMADD52LUQ Z7, Z1, Z13
	VPMADD52HUQ Z7, Z1, Z14
	VPMADD52LUQ Z8, Z1, Z14
	VPMADD52HUQ Z8, Z1, Z15
	VPMADD52LUQ Z9, Z1, Z15
	VPMADD52HUQ Z9, Z1, Z16
	VPMADD52LUQ Z5, Z2, Z12
	VPMADD52HUQ Z5, Z2, Z13
	VPMADD52LUQ Z6, Z2, Z13
	VPMADD52HUQ Z6, Z2, Z14
	VPMADD52LUQ Z7, Z2, Z14
	VPMADD52HUQ Z7, Z2, Z15
	VPMADD52LUQ Z8, Z2, Z15
	VPMADD52HUQ Z8, Z2, Z16
	VPMADD52LUQ Z9, Z2, Z16
	VPMADD52HUQ Z9, Z2, Z17
	VPMADD52LUQ Z5, Z3, Z13
	VPMADD52HUQ Z5, Z3, Z14
	VPMADD52LUQ Z6, Z3, Z14
	VPMADD52HUQ Z6, Z3, Z15
	VPMADD52LUQ Z7, Z3, Z15
	VPMADD52HUQ Z7, Z3, Z16
	VPMADD52LUQ Z8, Z3, Z16
	VPMADD52HUQ Z8, Z3, Z17
	VPMADD52LUQ Z9, Z3, Z17
	VPMADD52HUQ Z9, Z3, Z18
	VPMADD52LUQ Z5, Z4, Z14
	VPMADD52HUQ Z5, Z4, Z15
	VPMADD52LUQ Z6, Z4, Z15
	VPMADD52HUQ Z6, Z4, Z16
	VPMADD52LUQ Z7, Z4, Z16
	VPMADD52HUQ Z7, Z4, Z17
	VPMADD52LUQ Z8, Z4, Z17
	VPMADD52HUQ Z8, Z4, Z18
	VPMADD52LUQ Z9, Z4, Z18
	VPMADD52HUQ Z9, Z4, Z19

	// Montgomery reduction
	// Round 0: reduce T[0]
	VPMULUDQ    Z10, Z30, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z11

	// Round 1: reduce T[1]
	VPMULUDQ    Z11, Z30, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z11
	VPMADD52HUQ Z25, Z20, Z12
	VPMADD52LUQ Z26, Z20, Z12
	VPMADD52HUQ Z26, Z20, Z13
	VPMADD52LUQ Z27, Z20, Z13
	VPMADD52HUQ Z27, Z20, Z14
	VPMADD52LUQ Z28, Z20, Z14
	VPMADD52HUQ Z28, Z20, Z15
	VPMADD52LUQ Z29, Z20, Z15
	VPMADD52HUQ Z29, Z20, Z16
	VPSRLQ      $52, Z11, Z20
	VPADDQ      Z20, Z12, Z12

	// Round 2: reduce T[2]
	VPMULUDQ    Z12, Z30, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z12
	VPMADD52HUQ Z25, Z20, Z13
	VPMADD52LUQ Z26, Z20, Z13
	VPMADD52HUQ Z26, Z20, Z14
	VPMADD52LUQ Z27, Z20, Z14
	VPMADD52HUQ Z27, Z20, Z15
	VPMADD52LUQ Z28, Z20, Z15
	VPMADD52HUQ Z28, Z20, Z16
	VPMADD52LUQ Z29, Z20, Z16
	VPMADD52HUQ Z29, Z20, Z17
	VPSRLQ      $52, Z12, Z20
	VPADDQ      Z20, Z13, Z13

	// Round 3: reduce T[3]
	VPMULUDQ    Z13, Z30, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z13
	VPMADD52HUQ Z25, Z20, Z14
	VPMADD52LUQ Z26, Z20, Z14
	VPMADD52HUQ Z26, Z20, Z15
	VPMADD52LUQ Z27, Z20, Z15
	VPMADD52HUQ Z27, Z20, Z16
	VPMADD52LUQ Z28, Z20, Z16
	VPMADD52HUQ Z28, Z20, Z17
	VPMADD52LUQ Z29, Z20, Z17
	VPMADD52HUQ Z29, Z20, Z18
	VPSRLQ      $52, Z13, Z20
	VPADDQ      Z20, Z14, Z14

	// Round 4: reduce T[4]
	VPMULUDQ    Z14, Z30, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z14
	VPMADD52HUQ Z25, Z20, Z15
	VPMADD52LUQ Z26, Z20, Z15
	VPMADD52HUQ Z26, Z20, Z16
	VPMADD52LUQ Z27, Z20, Z16
	VPMADD52HUQ Z27, Z20, Z17
	VPMADD52LUQ Z28, Z20, Z17
	VPMADD52HUQ Z28, Z20, Z18
	VPMADD52LUQ Z29, Z20, Z18
	VPMADD52HUQ Z29, Z20, Z19

	// Copy reduced result to Z0-Z4
	VMOVDQA64 Z15, Z0
	VMOVDQA64 Z16, Z1
	VMOVDQA64 Z17, Z2
	VMOVDQA64 Z18, Z3
	VMOVDQA64 Z19, Z4

	// Final normalization
	VPSRLQ $52, Z0, Z20
	VPANDQ Z31, Z0, Z0
	VPADDQ Z20, Z1, Z1
	VPSRLQ $52, Z1, Z20
	VPANDQ Z31, Z1, Z1
	VPADDQ Z20, Z2, Z2
	VPSRLQ $52, Z2, Z20
	VPANDQ Z31, Z2, Z2
	VPADDQ Z20, Z3, Z3
	VPSRLQ $52, Z3, Z20
	VPANDQ Z31, Z3, Z3
	VPADDQ Z20, Z4, Z4

	// Conditional subtraction if >= q
	VPSUBQ  Z25, Z0, Z10
	VPSUBQ  Z26, Z1, Z11
	VPSUBQ  Z27, Z2, Z12
	VPSUBQ  Z28, Z3, Z13
	VPSUBQ  Z29, Z4, Z14
	VPSRAQ  $63, Z10, Z20
	VPADDQ  Z20, Z11, Z11
	VPSRAQ  $63, Z11, Z20
	VPADDQ  Z20, Z12, Z12
	VPSRAQ  $63, Z12, Z20
	VPADDQ  Z20, Z13, Z13
	VPSRAQ  $63, Z13, Z20
	VPADDQ  Z20, Z14, Z14
	VPSRAQ  $63, Z14, Z20
	VPANDQ  Z20, Z0, Z0
	VPANDNQ Z10, Z20, Z10
	VPORQ   Z10, Z0, Z0
	VPANDQ  Z20, Z1, Z1
	VPANDNQ Z11, Z20, Z11
	VPORQ   Z11, Z1, Z1
	VPANDQ  Z20, Z2, Z2
	VPANDNQ Z12, Z20, Z12
	VPORQ   Z12, Z2, Z2
	VPANDQ  Z20, Z3, Z3
	VPANDNQ Z13, Z20, Z13
	VPORQ   Z13, Z3, Z3
	VPANDQ  Z20, Z4, Z4
	VPANDNQ Z14, Z20, Z14
	VPORQ   Z14, Z4, Z4

	// Convert result from radix-52 back to radix-64 and store
	// Convert from radix-52 to radix-64
	VPSLLQ $52, Z1, Z18
	VPORQ  Z18, Z0, Z14
	VPSRLQ $12, Z1, Z18
	VPSLLQ $40, Z2, Z19
	VPORQ  Z19, Z18, Z15
	VPSRLQ $24, Z2, Z18
	VPSLLQ $28, Z3, Z19
	VPORQ  Z19, Z18, Z16
	VPSRLQ $36, Z3, Z18
	VPSLLQ $16, Z4, Z19
	VPORQ  Z19, Z18, Z17

	// Transpose back to AoS format and store
	// 4x8 reverse transpose
	VPUNPCKLQDQ Z15, Z14, Z18
	VPUNPCKHQDQ Z15, Z14, Z19
	VPUNPCKLQDQ Z17, Z16, Z20
	VPUNPCKHQDQ Z17, Z16, Z21
	VSHUFI64X2  $0x88, Z20, Z18, Z10
	VSHUFI64X2  $0xDD, Z20, Z18, Z12
	VSHUFI64X2  $0x88, Z21, Z19, Z11
	VSHUFI64X2  $0xDD, Z21, Z19, Z13
	VMOVDQU64   Z10, 0(R14)
	VMOVDQU64   Z11, 64(R14)
	VMOVDQU64   Z12, 128(R14)
	VMOVDQU64   Z13, 192(R14)

	// Advance pointers
	ADDQ $256, R13
	ADDQ $256, CX
	ADDQ $256, R14
	SUBQ $8, BX
	JMP  loop_1

done_2:
	RET
