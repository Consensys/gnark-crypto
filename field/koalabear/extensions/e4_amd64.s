//go:build !purego

// Code generated by gnark-crypto/generator. DO NOT EDIT.
// Refer to the generator for more documentation.

#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

TEXT ·mulAccByElement_avx512(SB), NOSPLIT, $0-32
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z1
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         alpha+0(FP), R13
	MOVQ         scale+8(FP), R14
	MOVQ         res+16(FP), CX
	MOVQ         N+24(FP), BX
	VMOVDQU32    0(R13), X2
	VINSERTI64X2 $1, X2, Y2, Y2
	VINSERTI64X4 $1, Y2, Z2, Z2
	VPSRLQ       $32, Z2, Z3
	SHRQ         $2, BX

loop_1:
	TESTQ        BX, BX
	JEQ          done_2
	DECQ         BX
	VMOVDQU32    0(CX), Z4
	VPBROADCASTD 0(R14), X5
	VPBROADCASTD 4(R14), X6
	VPBROADCASTD 8(R14), X7
	VPBROADCASTD 12(R14), X8
	VINSERTI64X2 $1, X6, Y5, Y5
	VINSERTI64X2 $1, X8, Y7, Y7
	VINSERTI64X4 $1, Y7, Z5, Z5
	VPMULUDQ     Z5, Z2, Z10
	VPMULUDQ     Z5, Z3, Z11
	VPMULUDQ     Z10, Z1, Z12
	VPMULUDQ     Z11, Z1, Z13
	VPMULUDQ     Z12, Z0, Z12
	VPADDQ       Z10, Z12, Z10
	VPMULUDQ     Z13, Z0, Z13
	VPADDQ       Z11, Z13, Z11
	VMOVSHDUP    Z10, K3, Z11
	VPSUBD       Z0, Z11, Z12
	VPMINUD      Z11, Z12, Z9
	VPADDD       Z4, Z9, Z4     // result = result + acc
	VPSUBD       Z0, Z4, Z9
	VPMINUD      Z4, Z9, Z4
	VMOVDQU32    Z4, 0(CX)
	ADDQ         $64, CX
	ADDQ         $16, R14
	JMP          loop_1

done_2:
	RET

TEXT ·vectorAdd_avx512(SB), NOSPLIT, $0-32
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVQ         res+0(FP), R13
	MOVQ         a+8(FP), R14
	MOVQ         b+16(FP), CX
	MOVQ         N+24(FP), BX
	SHRQ         $2, BX

loop_3:
	TESTQ     BX, BX
	JEQ       done_4
	DECQ      BX
	VMOVDQU32 0(R14), Z1
	VMOVDQU32 0(CX), Z2

#define ADD(in0, in1, in2, in3, in4) \
	VPADDD  in1, in0, in4 \
	VPSUBD  in2, in4, in3 \
	VPMINUD in4, in3, in4 \

	ADD(Z1, Z2, Z0, Z4, Z3)
	VMOVDQU32 Z3, 0(R13)
	ADDQ      $64, R13
	ADDQ      $64, CX
	ADDQ      $64, R14
	JMP       loop_3

done_4:
	RET

TEXT ·vectorSub_avx512(SB), NOSPLIT, $0-32
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVQ         res+0(FP), R13
	MOVQ         a+8(FP), R14
	MOVQ         b+16(FP), CX
	MOVQ         N+24(FP), BX
	SHRQ         $2, BX

loop_5:
	TESTQ     BX, BX
	JEQ       done_6
	DECQ      BX
	VMOVDQU32 0(R14), Z1
	VMOVDQU32 0(CX), Z2

#define SUB(in0, in1, in2, in3, in4) \
	VPSUBD  in1, in0, in4 \
	VPADDD  in2, in4, in3 \
	VPMINUD in4, in3, in4 \

	SUB(Z1, Z2, Z0, Z4, Z3)
	VMOVDQU32 Z3, 0(R13)
	ADDQ      $64, R13
	ADDQ      $64, CX
	ADDQ      $64, R14
	JMP       loop_5

done_6:
	RET

TEXT ·vectorMul_avx512(SB), NOSPLIT, $0-32
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z1
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         res+0(FP), R13
	MOVQ         a+8(FP), R14
	MOVQ         b+16(FP), CX
	MOVQ         N+24(FP), BX
	MOVQ         $0xffffffffffffffff, SI
	MOVQ         ·indexGather4+0(SB), DI
	VMOVDQU32    0(DI), Z2
	SHRQ         $4, BX

loop_7:
	TESTQ      BX, BX
	JEQ        done_8
	DECQ       BX
	KMOVD      SI, K1
	KMOVD      SI, K2
	VPGATHERDD 0(R14)(Z2*4), K1, Z3
	VPGATHERDD 0(CX)(Z2*4), K2, Z7
	KMOVD      SI, K1
	KMOVD      SI, K2
	VPGATHERDD 4(R14)(Z2*4), K1, Z4
	VPGATHERDD 4(CX)(Z2*4), K2, Z8
	KMOVD      SI, K1
	KMOVD      SI, K2
	VPGATHERDD 8(R14)(Z2*4), K1, Z5
	VPGATHERDD 8(CX)(Z2*4), K2, Z9
	KMOVD      SI, K1
	KMOVD      SI, K2
	VPGATHERDD 12(R14)(Z2*4), K1, Z6
	VPGATHERDD 12(CX)(Z2*4), K2, Z10
	ADD(Z3, Z5, Z0, Z13, Z11)
	ADD(Z4, Z6, Z0, Z14, Z12)
	ADD(Z7, Z9, Z0, Z17, Z15)
	ADD(Z8, Z10, Z0, Z18, Z16)
	ADD(Z3, Z4, Z0, Z24, Z20)
	VPADDD     Z8, Z7, Z21

#define MUL_5W(in0, in1, in2, in3, in4, in5, in6, in7) \
	VPSRLQ    $32, in0, in2 \
	VPSRLQ    $32, in1, in3 \
	VPMULUDQ  in0, in1, in4 \
	VPMULUDQ  in2, in3, in5 \
	VPMULUDQ  in4, Z1, in6  \
	VPMULUDQ  in5, Z1, in3  \
	VPMULUDQ  in6, Z0, in6  \
	VPADDQ    in4, in6, in4 \
	VPMULUDQ  in3, Z0, in3  \
	VPADDQ    in5, in3, in7 \
	VMOVSHDUP in4, K3, in7  \

	MUL_5W(Z3, Z7, Z25, Z26, Z27, Z28, Z29, Z19)

#define REDUCE1Q(in0, in1, in2) \
	VPSUBD  in0, in1, in2 \
	VPMINUD in1, in2, in1 \

	REDUCE1Q(Z0, Z19, Z30)
	MUL_5W(Z4, Z8, Z31, Z13, Z14, Z17, Z18, Z22)
	REDUCE1Q(Z0, Z22, Z24)
	MUL_5W(Z20, Z21, Z25, Z26, Z27, Z28, Z29, Z20)
	REDUCE1Q(Z0, Z20, Z30)
	ADD(Z19, Z22, Z0, Z31, Z23)
	SUB(Z20, Z23, Z0, Z13, Z20)
	ADD(Z23, Z22, Z0, Z14, Z19)
	ADD(Z19, Z22, Z0, Z17, Z19)
	ADD(Z5, Z6, Z0, Z28, Z24)
	VPADDD      Z10, Z9, Z25
	MUL_5W(Z5, Z9, Z29, Z30, Z31, Z13, Z14, Z18)
	REDUCE1Q(Z0, Z18, Z17)
	MUL_5W(Z6, Z10, Z21, Z22, Z23, Z28, Z29, Z26)
	REDUCE1Q(Z0, Z26, Z30)
	MUL_5W(Z24, Z25, Z31, Z13, Z14, Z17, Z21, Z24)
	REDUCE1Q(Z0, Z24, Z22)
	ADD(Z18, Z26, Z0, Z23, Z27)
	SUB(Z24, Z27, Z0, Z28, Z24)
	ADD(Z27, Z26, Z0, Z29, Z18)
	ADD(Z18, Z26, Z0, Z30, Z18)
	ADD(Z11, Z12, Z0, Z22, Z13)
	VPADDD      Z16, Z15, Z14
	MUL_5W(Z11, Z15, Z23, Z28, Z29, Z30, Z25, Z31)
	REDUCE1Q(Z0, Z31, Z26)
	MUL_5W(Z12, Z16, Z27, Z22, Z23, Z28, Z29, Z17)
	REDUCE1Q(Z0, Z17, Z30)
	MUL_5W(Z13, Z14, Z25, Z26, Z27, Z22, Z23, Z13)
	REDUCE1Q(Z0, Z13, Z28)
	ADD(Z31, Z17, Z0, Z29, Z21)
	SUB(Z13, Z21, Z0, Z30, Z13)
	ADD(Z21, Z17, Z0, Z25, Z31)
	ADD(Z31, Z17, Z0, Z26, Z31)
	ADD(Z19, Z18, Z0, Z23, Z27)
	ADD(Z20, Z24, Z0, Z28, Z22)
	ADD(Z11, Z15, Z0, Z25, Z29)
	ADD(Z12, Z16, Z0, Z26, Z30)
	SUB(Z31, Z27, Z0, Z14, Z5)
	SUB(Z13, Z22, Z0, Z17, Z6)
	ADD(Z24, Z24, Z0, Z21, Z11)
	ADD(Z24, Z19, Z0, Z23, Z19)
	ADD(Z18, Z20, Z0, Z28, Z4)
	ADD(Z11, Z19, Z0, Z25, Z3)
	KMOVD       SI, K1
	VPSCATTERDD Z3, K1, 0(R13)(Z2*4)
	KMOVD       SI, K1
	VPSCATTERDD Z4, K1, 4(R13)(Z2*4)
	KMOVD       SI, K1
	VPSCATTERDD Z5, K1, 8(R13)(Z2*4)
	KMOVD       SI, K1
	VPSCATTERDD Z6, K1, 12(R13)(Z2*4)
	ADDQ        $256, R13
	ADDQ        $256, R14
	ADDQ        $256, CX
	JMP         loop_7

done_8:
	RET

TEXT ·vectorScalarMul_avx512(SB), NOSPLIT, $0-32
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z1
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         res+0(FP), R13
	MOVQ         a+8(FP), R14
	MOVQ         b+16(FP), CX
	MOVQ         N+24(FP), BX
	MOVQ         $0xffffffffffffffff, SI
	MOVQ         ·indexGather4+0(SB), DI
	VMOVDQU32    0(DI), Z2
	MOVD         0(CX), AX
	VPBROADCASTD AX, Z7
	MOVD         4(CX), AX
	VPBROADCASTD AX, Z8
	MOVD         8(CX), AX
	VPBROADCASTD AX, Z9
	MOVD         12(CX), AX
	VPBROADCASTD AX, Z10
	SHRQ         $4, BX

loop_9:
	TESTQ       BX, BX
	JEQ         done_10
	DECQ        BX
	KMOVD       SI, K1
	VPGATHERDD  0(R14)(Z2*4), K1, Z3
	KMOVD       SI, K1
	VPGATHERDD  4(R14)(Z2*4), K1, Z4
	KMOVD       SI, K1
	VPGATHERDD  8(R14)(Z2*4), K1, Z5
	KMOVD       SI, K1
	VPGATHERDD  12(R14)(Z2*4), K1, Z6
	ADD(Z3, Z5, Z0, Z13, Z11)
	ADD(Z4, Z6, Z0, Z14, Z12)
	ADD(Z7, Z9, Z0, Z17, Z15)
	ADD(Z8, Z10, Z0, Z18, Z16)
	ADD(Z3, Z4, Z0, Z24, Z20)
	VPADDD      Z8, Z7, Z21
	MUL_5W(Z3, Z7, Z25, Z26, Z27, Z28, Z29, Z19)
	REDUCE1Q(Z0, Z19, Z30)
	MUL_5W(Z4, Z8, Z31, Z13, Z14, Z17, Z18, Z22)
	REDUCE1Q(Z0, Z22, Z24)
	MUL_5W(Z20, Z21, Z25, Z26, Z27, Z28, Z29, Z20)
	REDUCE1Q(Z0, Z20, Z30)
	ADD(Z19, Z22, Z0, Z31, Z23)
	SUB(Z20, Z23, Z0, Z13, Z20)
	ADD(Z23, Z22, Z0, Z14, Z19)
	ADD(Z19, Z22, Z0, Z17, Z19)
	ADD(Z5, Z6, Z0, Z28, Z24)
	VPADDD      Z10, Z9, Z25
	MUL_5W(Z5, Z9, Z29, Z30, Z31, Z13, Z14, Z18)
	REDUCE1Q(Z0, Z18, Z17)
	MUL_5W(Z6, Z10, Z21, Z22, Z23, Z28, Z29, Z26)
	REDUCE1Q(Z0, Z26, Z30)
	MUL_5W(Z24, Z25, Z31, Z13, Z14, Z17, Z21, Z24)
	REDUCE1Q(Z0, Z24, Z22)
	ADD(Z18, Z26, Z0, Z23, Z27)
	SUB(Z24, Z27, Z0, Z28, Z24)
	ADD(Z27, Z26, Z0, Z29, Z18)
	ADD(Z18, Z26, Z0, Z30, Z18)
	ADD(Z11, Z12, Z0, Z22, Z13)
	VPADDD      Z16, Z15, Z14
	MUL_5W(Z11, Z15, Z23, Z28, Z29, Z30, Z25, Z31)
	REDUCE1Q(Z0, Z31, Z26)
	MUL_5W(Z12, Z16, Z27, Z22, Z23, Z28, Z29, Z17)
	REDUCE1Q(Z0, Z17, Z30)
	MUL_5W(Z13, Z14, Z25, Z26, Z27, Z22, Z23, Z13)
	REDUCE1Q(Z0, Z13, Z28)
	ADD(Z31, Z17, Z0, Z29, Z21)
	SUB(Z13, Z21, Z0, Z30, Z13)
	ADD(Z21, Z17, Z0, Z25, Z31)
	ADD(Z31, Z17, Z0, Z26, Z31)
	ADD(Z19, Z18, Z0, Z23, Z27)
	ADD(Z20, Z24, Z0, Z28, Z22)
	ADD(Z11, Z15, Z0, Z25, Z29)
	ADD(Z12, Z16, Z0, Z26, Z30)
	SUB(Z31, Z27, Z0, Z14, Z5)
	SUB(Z13, Z22, Z0, Z17, Z6)
	ADD(Z24, Z24, Z0, Z21, Z11)
	ADD(Z24, Z19, Z0, Z23, Z19)
	ADD(Z18, Z20, Z0, Z28, Z4)
	ADD(Z11, Z19, Z0, Z25, Z3)
	KMOVD       SI, K1
	VPSCATTERDD Z3, K1, 0(R13)(Z2*4)
	KMOVD       SI, K1
	VPSCATTERDD Z4, K1, 4(R13)(Z2*4)
	KMOVD       SI, K1
	VPSCATTERDD Z5, K1, 8(R13)(Z2*4)
	KMOVD       SI, K1
	VPSCATTERDD Z6, K1, 12(R13)(Z2*4)
	ADDQ        $256, R13
	ADDQ        $256, R14
	JMP         loop_9

done_10:
	RET

TEXT ·vectorInnerProduct_avx512(SB), NOSPLIT, $0-32
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z1
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         res+0(FP), R13
	MOVQ         a+8(FP), R14
	MOVQ         b+16(FP), CX
	MOVQ         N+24(FP), BX
	MOVQ         $0xffffffffffffffff, SI
	MOVQ         ·indexGather4+0(SB), DI
	VMOVDQU32    0(DI), Z2
	VXORPS       Z11, Z11, Z11
	VMOVDQA64    Z11, Z12
	VMOVDQA64    Z11, Z13
	VMOVDQA64    Z11, Z14
	SHRQ         $4, BX

loop_11:
	TESTQ      BX, BX
	JEQ        done_12
	DECQ       BX
	KMOVD      SI, K1
	KMOVD      SI, K2
	VPGATHERDD 0(R14)(Z2*4), K1, Z3
	VPGATHERDD 0(CX)(Z2*4), K2, Z7
	KMOVD      SI, K1
	KMOVD      SI, K2
	VPGATHERDD 4(R14)(Z2*4), K1, Z4
	VPGATHERDD 4(CX)(Z2*4), K2, Z8
	KMOVD      SI, K1
	KMOVD      SI, K2
	VPGATHERDD 8(R14)(Z2*4), K1, Z5
	VPGATHERDD 8(CX)(Z2*4), K2, Z9
	KMOVD      SI, K1
	KMOVD      SI, K2
	VPGATHERDD 12(R14)(Z2*4), K1, Z6
	VPGATHERDD 12(CX)(Z2*4), K2, Z10
	ADD(Z3, Z5, Z0, Z17, Z15)
	ADD(Z4, Z6, Z0, Z18, Z16)
	ADD(Z7, Z9, Z0, Z21, Z19)
	ADD(Z8, Z10, Z0, Z22, Z20)
	ADD(Z3, Z4, Z0, Z28, Z24)
	VPADDD     Z8, Z7, Z25
	MUL_5W(Z3, Z7, Z29, Z30, Z31, Z17, Z18, Z23)
	REDUCE1Q(Z0, Z23, Z21)
	MUL_5W(Z4, Z8, Z22, Z28, Z29, Z30, Z31, Z26)
	REDUCE1Q(Z0, Z26, Z17)
	MUL_5W(Z24, Z25, Z18, Z21, Z22, Z28, Z29, Z24)
	REDUCE1Q(Z0, Z24, Z30)
	ADD(Z23, Z26, Z0, Z31, Z27)
	SUB(Z24, Z27, Z0, Z17, Z24)
	ADD(Z27, Z26, Z0, Z18, Z23)
	ADD(Z23, Z26, Z0, Z21, Z23)
	ADD(Z5, Z6, Z0, Z17, Z28)
	VPADDD     Z10, Z9, Z29
	MUL_5W(Z5, Z9, Z18, Z21, Z25, Z26, Z27, Z22)
	REDUCE1Q(Z0, Z22, Z17)
	MUL_5W(Z6, Z10, Z18, Z21, Z25, Z26, Z27, Z30)
	REDUCE1Q(Z0, Z30, Z17)
	MUL_5W(Z28, Z29, Z18, Z21, Z25, Z26, Z27, Z28)
	REDUCE1Q(Z0, Z28, Z17)
	ADD(Z22, Z30, Z0, Z18, Z31)
	SUB(Z28, Z31, Z0, Z21, Z28)
	ADD(Z31, Z30, Z0, Z25, Z22)
	ADD(Z22, Z30, Z0, Z26, Z22)
	ADD(Z15, Z16, Z0, Z26, Z17)
	VPADDD     Z20, Z19, Z18

#define MUL_4W(in0, in1, in2, in3, in4, in5, in6) \
	VPSRLQ    $32, in0, in2 \
	VPSRLQ    $32, in1, in3 \
	VPMULUDQ  in0, in1, in4 \
	VPMULUDQ  in2, in3, in5 \
	VPMULUDQ  in4, Z1, in2  \
	VPMULUDQ  in5, Z1, in3  \
	VPMULUDQ  in2, Z0, in2  \
	VPADDQ    in4, in2, in4 \
	VPMULUDQ  in3, Z0, in3  \
	VPADDQ    in5, in3, in6 \
	VMOVSHDUP in4, K3, in6  \

	MUL_4W(Z15, Z19, Z29, Z30, Z31, Z26, Z27)
	REDUCE1Q(Z0, Z27, Z29)
	MUL_4W(Z16, Z20, Z30, Z31, Z26, Z29, Z21)
	REDUCE1Q(Z0, Z21, Z30)
	MUL_4W(Z17, Z18, Z31, Z26, Z29, Z30, Z17)
	REDUCE1Q(Z0, Z17, Z31)
	ADD(Z27, Z21, Z0, Z26, Z25)
	SUB(Z17, Z25, Z0, Z29, Z17)
	ADD(Z25, Z21, Z0, Z30, Z27)
	ADD(Z27, Z21, Z0, Z31, Z27)
	ADD(Z23, Z22, Z0, Z30, Z26)
	ADD(Z24, Z28, Z0, Z31, Z29)
	ADD(Z15, Z19, Z0, Z25, Z18)
	ADD(Z16, Z20, Z0, Z30, Z21)
	SUB(Z27, Z26, Z0, Z31, Z5)
	SUB(Z17, Z29, Z0, Z25, Z6)
	ADD(Z28, Z28, Z0, Z30, Z15)
	ADD(Z28, Z23, Z0, Z31, Z23)
	ADD(Z22, Z24, Z0, Z25, Z4)
	ADD(Z15, Z23, Z0, Z30, Z3)
	VEXTRACTI64X4 $1, Z3, Y7
	ADD(Y3, Y7, Y0, Y31, Y7)
	VEXTRACTI64X2 $1, Y7, X3
	VPADDD        X7, X3, X3
	VPMOVZXDQ     X3, Y7
	VPADDQ        Y7, Y11, Y11
	VEXTRACTI64X4 $1, Z4, Y8
	ADD(Y4, Y8, Y0, Y25, Y8)
	VEXTRACTI64X2 $1, Y8, X4
	VPADDD        X8, X4, X4
	VPMOVZXDQ     X4, Y8
	VPADDQ        Y8, Y12, Y12
	VEXTRACTI64X4 $1, Z5, Y9
	ADD(Y5, Y9, Y0, Y30, Y9)
	VEXTRACTI64X2 $1, Y9, X5
	VPADDD        X9, X5, X5
	VPMOVZXDQ     X5, Y9
	VPADDQ        Y9, Y13, Y13
	VEXTRACTI64X4 $1, Z6, Y10
	ADD(Y6, Y10, Y0, Y31, Y10)
	VEXTRACTI64X2 $1, Y10, X6
	VPADDD        X10, X6, X6
	VPMOVZXDQ     X6, Y10
	VPADDQ        Y10, Y14, Y14
	ADDQ          $256, R14
	ADDQ          $256, CX
	JMP           loop_11

done_12:
	VMOVDQU64 Y11, 0(R13)
	VMOVDQU64 Y12, 64(R13)
	VMOVDQU64 Y13, 128(R13)
	VMOVDQU64 Y14, 192(R13)
	RET

TEXT ·vectorSum_avx512(SB), NOSPLIT, $0-24
	MOVQ   res+0(FP), R13
	MOVQ   a+8(FP), R14
	MOVQ   N+16(FP), CX
	VXORPS Z0, Z0, Z0     // vSum = 0
	SHRQ   $1, CX

loop_13:
	TESTQ     CX, CX
	JEQ       done_14
	DECQ      CX
	VPMOVZXDQ 0(R14), Z1 // load 2 E4 into vA
	VPADDQ    Z1, Z0, Z0 // vSum += vA
	ADDQ      $32, R14
	JMP       loop_13

done_14:
	VEXTRACTI64X4 $1, Z0, Y2
	VPADDQ        Y2, Y0, Y0
	VMOVDQU64     Y0, 0(R13)
	RET

TEXT ·vectorMulByElement_avx512(SB), NOSPLIT, $0-32
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z1
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         res+0(FP), R13
	MOVQ         a+8(FP), R14
	MOVQ         b+16(FP), CX
	MOVQ         N+24(FP), BX
	MOVQ         ·maskPermD+0(SB), SI
	VMOVDQU32    0(SI), Z5
	SHRQ         $2, BX

loop_15:
	TESTQ     BX, BX
	JEQ       done_16
	DECQ      BX
	VMOVDQU32 0(R14), Z2
	VMOVDQU32 0(CX), X3
	VPERMD    Z3, Z5, Z3
	MUL_5W(Z2, Z3, Z6, Z7, Z8, Z9, Z10, Z4)
	REDUCE1Q(Z0, Z4, Z11)
	VMOVDQU32 Z4, 0(R13)
	ADDQ      $64, R13
	ADDQ      $64, R14
	ADDQ      $16, CX
	JMP       loop_15

done_16:
	RET
