//go:build !purego

// Code generated by gnark-crypto/generator. DO NOT EDIT.
// Refer to the generator for more documentation.
// Some sub-functions are derived from Plonky3:
// https://github.com/Plonky3/Plonky3/blob/36e619f3c6526ee86e2e5639a24b3224e1c1700f/monty-31/src/x86_64_avx512/packing.rs#L319

#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

#define BUTTERFLYD1Q(in0, in1, in2, in3, in4) \
	VPADDD  in0, in1, in3 \
	VPSUBD  in1, in0, in1 \
	VPSUBD  in2, in3, in0 \
	VPMINUD in3, in0, in0 \
	VPADDD  in2, in1, in4 \
	VPMINUD in4, in1, in1 \

#define BUTTERFLYD2Q(in0, in1, in2, in3) \
	VPADDD  in0, in1, in3 \
	VPSUBD  in1, in0, in1 \
	VPSUBD  in2, in3, in0 \
	VPMINUD in3, in0, in0 \
	VPADDD  in2, in1, in1 \

#define BUTTERFLYD2Q2Q(in0, in1, in2, in3) \
	VPSUBD in1, in0, in3 \
	VPADDD in0, in1, in0 \
	VPADDD in2, in3, in1 \

#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9) \
	VPSRLQ    $32, in0, in2 \
	VPSRLQ    $32, in1, in3 \
	VPMULUDQ  in0, in1, in4 \
	VPMULUDQ  in2, in3, in5 \
	VPMULUDQ  in4, in9, in6 \
	VPMULUDQ  in5, in9, in7 \
	VPMULUDQ  in6, in8, in6 \
	VPADDQ    in4, in6, in4 \
	VPMULUDQ  in7, in8, in7 \
	VPADDQ    in5, in7, in5 \
	VMOVSHDUP in4, K3, in5  \
	VPSUBD    in8, in5, in7 \
	VPMINUD   in5, in7, in0 \

#define PERMUTE8X8(in0, in1, in2) \
	VSHUFI64X2 $0x000000000000004e, in1, in0, in2 \
	VPBLENDMQ  in0, in2, K1, in0                  \
	VPBLENDMQ  in2, in1, K1, in1                  \

#define PERMUTE4X4(in0, in1, in2, in3) \
	VMOVDQA64 in2, in3          \
	VPERMI2Q  in1, in0, in3     \
	VPBLENDMQ in0, in3, K2, in0 \
	VPBLENDMQ in3, in1, K2, in1 \

#define PERMUTE2X2(in0, in1, in2) \
	VSHUFPD   $0x0000000000000055, in1, in0, in2 \
	VPBLENDMQ in0, in2, K3, in0                  \
	VPBLENDMQ in2, in1, K3, in1                  \

#define PERMUTE1X1(in0, in1, in2) \
	VPSHRDQ   $32, in1, in0, in2 \
	VPBLENDMD in0, in2, K3, in0  \
	VPBLENDMD in2, in1, K3, in1  \

#define LOAD_Q(in0, in1) \
	MOVD         $const_q, AX       \
	VPBROADCASTD AX, in0            \
	MOVD         $const_qInvNeg, AX \
	VPBROADCASTD AX, in1            \

#define LOAD_MASKS() \
	MOVQ  $0x0000000000000f0f, AX \
	KMOVQ AX, K1                  \
	MOVQ  $0x0000000000000033, AX \
	KMOVQ AX, K2                  \
	MOVQ  $0x0000000000005555, AX \
	KMOVD AX, K3                  \

#define BUTTERFLY_MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11, in12, in13) \
BUTTERFLYD2Q(in0, in1, in2, in3)                           \
MULD(in4, in5, in6, in7, in8, in9, in10, in11, in12, in13) \

TEXT ·sis512_16_avx512(SB), $1024-120
	LOAD_Q(Z0, Z1)
	LOAD_MASKS()
	MOVQ k256+0(FP), R15
	MOVQ cosets+24(FP), CX
	MOVQ twiddles+48(FP), SI
	MOVQ rag+72(FP), R8
	MOVQ res+96(FP), R9
	MOVQ 0(SI), DI               // twiddles[0]
	MOVQ R15, DX
	MOVQ CX, BX
	ADDQ $0x0000000000000200, DX
	ADDQ $0x0000000000000400, BX

#define FROMMONTGOMERY(in0, in1, in2, in3, in4, in5, in6) \
	VPSRLQ    $32, in0, in2     \
	VPMULUDQ  in0, in6, in3     \
	VPMULUDQ  in2, in6, in4     \
	VPMULUDQ  in3, in5, in3     \
	VPMULUDQ  in4, in5, in4     \
	VPANDD.Z  in0, in0, K3, in1 \
	VPADDQ    in1, in3, in1     \
	VPADDQ    in2, in4, in2     \
	VMOVSHDUP in1, K3, in2      \
	VPSUBD    in5, in2, in4     \
	VPMINUD   in2, in4, in0     \

	VMOVDQU32     0(R15), Z12
	FROMMONTGOMERY(Z12, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z12, Y13
	VPMOVZXWD     Y12, Z12
	VPMOVZXWD     Y13, Z13
	VMOVDQU32     0(CX), Z10
	VMOVDQU32     64(CX), Z8
	MULD(Z12, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z13, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     0(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     0(BX), Z10
	VMOVDQU32     64(BX), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	BUTTERFLYD2Q(Z12, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z13, Z9, Z0, Z2)
	VMOVDQU32     0(DI), Z10
	VMOVDQU32     64(DI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     Z11, 0(SP)
	VMOVDQU32     Z9, 64(SP)
	VMOVDQU32     64(R15), Z14
	FROMMONTGOMERY(Z14, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z14, Y15
	VPMOVZXWD     Y14, Z14
	VPMOVZXWD     Y15, Z15
	VMOVDQU32     128(CX), Z10
	VMOVDQU32     192(CX), Z8
	MULD(Z14, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z15, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     64(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     128(BX), Z10
	VMOVDQU32     192(BX), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	BUTTERFLYD2Q(Z14, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z15, Z9, Z0, Z2)
	VMOVDQU32     128(DI), Z10
	VMOVDQU32     192(DI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     Z11, 128(SP)
	VMOVDQU32     Z9, 192(SP)
	VMOVDQU32     128(R15), Z16
	FROMMONTGOMERY(Z16, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z16, Y17
	VPMOVZXWD     Y16, Z16
	VPMOVZXWD     Y17, Z17
	VMOVDQU32     256(CX), Z10
	VMOVDQU32     320(CX), Z8
	MULD(Z16, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z17, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     128(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     256(BX), Z10
	VMOVDQU32     320(BX), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	BUTTERFLYD2Q(Z16, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z17, Z9, Z0, Z2)
	VMOVDQU32     256(DI), Z10
	VMOVDQU32     320(DI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     Z11, 256(SP)
	VMOVDQU32     Z9, 320(SP)
	VMOVDQU32     192(R15), Z18
	FROMMONTGOMERY(Z18, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z18, Y19
	VPMOVZXWD     Y18, Z18
	VPMOVZXWD     Y19, Z19
	VMOVDQU32     384(CX), Z10
	VMOVDQU32     448(CX), Z8
	MULD(Z18, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z19, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     192(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     384(BX), Z10
	VMOVDQU32     448(BX), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	BUTTERFLYD2Q(Z18, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z19, Z9, Z0, Z2)
	VMOVDQU32     384(DI), Z10
	VMOVDQU32     448(DI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     Z11, 384(SP)
	VMOVDQU32     Z9, 448(SP)
	VMOVDQU32     256(R15), Z20
	FROMMONTGOMERY(Z20, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z20, Y21
	VPMOVZXWD     Y20, Z20
	VPMOVZXWD     Y21, Z21
	VMOVDQU32     512(CX), Z10
	VMOVDQU32     576(CX), Z8
	MULD(Z20, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z21, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     256(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     512(BX), Z10
	VMOVDQU32     576(BX), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	BUTTERFLYD2Q(Z20, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z21, Z9, Z0, Z2)
	VMOVDQU32     512(DI), Z10
	VMOVDQU32     576(DI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     Z11, 512(SP)
	VMOVDQU32     Z9, 576(SP)
	VMOVDQU32     320(R15), Z22
	FROMMONTGOMERY(Z22, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z22, Y23
	VPMOVZXWD     Y22, Z22
	VPMOVZXWD     Y23, Z23
	VMOVDQU32     640(CX), Z10
	VMOVDQU32     704(CX), Z8
	MULD(Z22, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z23, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     320(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     640(BX), Z10
	VMOVDQU32     704(BX), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	BUTTERFLYD2Q(Z22, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z23, Z9, Z0, Z2)
	VMOVDQU32     640(DI), Z10
	VMOVDQU32     704(DI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     Z11, 640(SP)
	VMOVDQU32     Z9, 704(SP)
	VMOVDQU32     384(R15), Z24
	FROMMONTGOMERY(Z24, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z24, Y25
	VPMOVZXWD     Y24, Z24
	VPMOVZXWD     Y25, Z25
	VMOVDQU32     768(CX), Z10
	VMOVDQU32     832(CX), Z8
	MULD(Z24, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z25, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     384(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     768(BX), Z10
	VMOVDQU32     832(BX), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	BUTTERFLYD2Q(Z24, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z25, Z9, Z0, Z2)
	VMOVDQU32     768(DI), Z10
	VMOVDQU32     832(DI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     Z11, 768(SP)
	VMOVDQU32     Z9, 832(SP)
	VMOVDQU32     448(R15), Z26
	FROMMONTGOMERY(Z26, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z26, Y27
	VPMOVZXWD     Y26, Z26
	VPMOVZXWD     Y27, Z27
	VMOVDQU32     896(CX), Z10
	VMOVDQU32     960(CX), Z8
	MULD(Z26, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z27, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     448(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     896(BX), Z10
	VMOVDQU32     960(BX), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	BUTTERFLYD2Q(Z26, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z27, Z9, Z0, Z2)
	VMOVDQU32     896(DI), Z10
	VMOVDQU32     960(DI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1)
	VMOVDQU32     Z11, 896(SP)
	VMOVDQU32     Z9, 960(SP)
	ADDQ          $24, SI
	MOVQ          SI, DI
	MOVQ          $2, R10

fft256_2:
	MOVQ         0(SI), R11
	VMOVDQU32    0(R11), Z28
	VMOVDQU32    64(R11), Z29
	VMOVDQU32    128(R11), Z30
	VMOVDQU32    192(R11), Z31
	VMOVDQU32    256(R11), Z2
	VMOVDQU32    320(R11), Z3
	VMOVDQU32    384(R11), Z4
	VMOVDQU32    448(R11), Z5
	BUTTERFLY_MULD(Z12, Z20, Z0, Z6, Z20, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z13, Z21, Z0, Z6, Z21, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z14, Z22, Z0, Z6, Z22, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z15, Z23, Z0, Z6, Z23, Z31, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z16, Z24, Z0, Z6, Z24, Z2, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z17, Z25, Z0, Z6, Z25, Z3, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z18, Z26, Z0, Z6, Z26, Z4, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z19, Z27, Z0, Z6, Z27, Z5, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	ADDQ         $24, SI
	MOVQ         0(SI), R11
	VMOVDQU32    0(R11), Z28
	VMOVDQU32    64(R11), Z29
	VMOVDQU32    128(R11), Z30
	VMOVDQU32    192(R11), Z31
	BUTTERFLY_MULD(Z12, Z16, Z0, Z6, Z16, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z13, Z17, Z0, Z6, Z17, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z14, Z18, Z0, Z6, Z18, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z15, Z19, Z0, Z6, Z19, Z31, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z20, Z24, Z0, Z6, Z24, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z21, Z25, Z0, Z6, Z25, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z22, Z26, Z0, Z6, Z26, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z23, Z27, Z0, Z6, Z27, Z31, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	ADDQ         $24, SI
	MOVQ         0(SI), R11
	VMOVDQU32    0(R11), Z28
	VMOVDQU32    64(R11), Z29
	BUTTERFLY_MULD(Z12, Z14, Z0, Z6, Z14, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z13, Z15, Z0, Z6, Z15, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z16, Z18, Z0, Z6, Z18, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z17, Z19, Z0, Z6, Z19, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z20, Z22, Z0, Z6, Z22, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z21, Z23, Z0, Z6, Z23, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z24, Z26, Z0, Z6, Z26, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z25, Z27, Z0, Z6, Z27, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	ADDQ         $24, SI
	MOVQ         0(SI), R11
	VMOVDQU32    0(R11), Z28
	BUTTERFLY_MULD(Z12, Z13, Z0, Z6, Z13, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z14, Z15, Z0, Z6, Z15, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z16, Z17, Z0, Z6, Z17, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z18, Z19, Z0, Z6, Z19, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z20, Z21, Z0, Z6, Z21, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z22, Z23, Z0, Z6, Z23, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z24, Z25, Z0, Z6, Z25, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	BUTTERFLY_MULD(Z26, Z27, Z0, Z6, Z27, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	ADDQ         $24, SI
	MOVQ         ·vInterleaveIndices+0(SB), R12
	VMOVDQU64    0(R12), Z2
	MOVQ         0(SI), R11
	VMOVDQU32    0(R11), Y28
	VINSERTI64X4 $1, Y28, Z28, Z28
	MOVQ         24(SI), R11
	VMOVDQU32    0(R11), X29
	VINSERTI64X2 $1, X29, Z29, Z29
	VINSERTI64X2 $0x0000000000000002, X29, Z29, Z29
	VINSERTI64X2 $0x0000000000000003, X29, Z29, Z29
	MOVQ         48(SI), R11
	VPBROADCASTD 0(R11), Z30
	VPBROADCASTD 4(R11), Z31
	VPBLENDMD    Z30, Z31, K3, Z30
	PERMUTE8X8(Z12, Z13, Z6)
	BUTTERFLY_MULD(Z12, Z13, Z0, Z6, Z13, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE8X8(Z14, Z15, Z6)
	BUTTERFLY_MULD(Z14, Z15, Z0, Z6, Z15, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE8X8(Z16, Z17, Z6)
	BUTTERFLY_MULD(Z16, Z17, Z0, Z6, Z17, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE8X8(Z18, Z19, Z6)
	BUTTERFLY_MULD(Z18, Z19, Z0, Z6, Z19, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE8X8(Z20, Z21, Z6)
	BUTTERFLY_MULD(Z20, Z21, Z0, Z6, Z21, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE8X8(Z22, Z23, Z6)
	BUTTERFLY_MULD(Z22, Z23, Z0, Z6, Z23, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE8X8(Z24, Z25, Z6)
	BUTTERFLY_MULD(Z24, Z25, Z0, Z6, Z25, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE8X8(Z26, Z27, Z6)
	BUTTERFLY_MULD(Z26, Z27, Z0, Z6, Z27, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE4X4(Z12, Z13, Z2, Z6)
	BUTTERFLY_MULD(Z12, Z13, Z0, Z6, Z13, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE4X4(Z14, Z15, Z2, Z6)
	BUTTERFLY_MULD(Z14, Z15, Z0, Z6, Z15, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE4X4(Z16, Z17, Z2, Z6)
	BUTTERFLY_MULD(Z16, Z17, Z0, Z6, Z17, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE4X4(Z18, Z19, Z2, Z6)
	BUTTERFLY_MULD(Z18, Z19, Z0, Z6, Z19, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE4X4(Z20, Z21, Z2, Z6)
	BUTTERFLY_MULD(Z20, Z21, Z0, Z6, Z21, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE4X4(Z22, Z23, Z2, Z6)
	BUTTERFLY_MULD(Z22, Z23, Z0, Z6, Z23, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE4X4(Z24, Z25, Z2, Z6)
	BUTTERFLY_MULD(Z24, Z25, Z0, Z6, Z25, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE4X4(Z26, Z27, Z2, Z6)
	BUTTERFLY_MULD(Z26, Z27, Z0, Z6, Z27, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE2X2(Z12, Z13, Z6)
	BUTTERFLY_MULD(Z12, Z13, Z0, Z6, Z13, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE2X2(Z14, Z15, Z6)
	BUTTERFLY_MULD(Z14, Z15, Z0, Z6, Z15, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE2X2(Z16, Z17, Z6)
	BUTTERFLY_MULD(Z16, Z17, Z0, Z6, Z17, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE2X2(Z18, Z19, Z6)
	BUTTERFLY_MULD(Z18, Z19, Z0, Z6, Z19, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE2X2(Z20, Z21, Z6)
	BUTTERFLY_MULD(Z20, Z21, Z0, Z6, Z21, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE2X2(Z22, Z23, Z6)
	BUTTERFLY_MULD(Z22, Z23, Z0, Z6, Z23, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE2X2(Z24, Z25, Z6)
	BUTTERFLY_MULD(Z24, Z25, Z0, Z6, Z25, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE2X2(Z26, Z27, Z6)
	BUTTERFLY_MULD(Z26, Z27, Z0, Z6, Z27, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1)
	PERMUTE1X1(Z12, Z13, Z6)
	BUTTERFLYD2Q2Q(Z12, Z13, Z0, Z6)
	PERMUTE1X1(Z14, Z15, Z6)
	BUTTERFLYD2Q2Q(Z14, Z15, Z0, Z6)
	PERMUTE1X1(Z16, Z17, Z6)
	BUTTERFLYD2Q2Q(Z16, Z17, Z0, Z6)
	PERMUTE1X1(Z18, Z19, Z6)
	BUTTERFLYD2Q2Q(Z18, Z19, Z0, Z6)
	PERMUTE1X1(Z20, Z21, Z6)
	BUTTERFLYD2Q2Q(Z20, Z21, Z0, Z6)
	PERMUTE1X1(Z22, Z23, Z6)
	BUTTERFLYD2Q2Q(Z22, Z23, Z0, Z6)
	PERMUTE1X1(Z24, Z25, Z6)
	BUTTERFLYD2Q2Q(Z24, Z25, Z0, Z6)
	PERMUTE1X1(Z26, Z27, Z6)
	BUTTERFLYD2Q2Q(Z26, Z27, Z0, Z6)
	VMOVDQU32    0(R8), Z31
	VMOVDQU32    64(R8), Z6
	MULD(Z12, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	MULD(Z13, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	VMOVDQU32    128(R8), Z31
	VMOVDQU32    192(R8), Z6
	MULD(Z14, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	MULD(Z15, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	VMOVDQU32    256(R8), Z31
	VMOVDQU32    320(R8), Z6
	MULD(Z16, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	MULD(Z17, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	VMOVDQU32    384(R8), Z31
	VMOVDQU32    448(R8), Z6
	MULD(Z18, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	MULD(Z19, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	VMOVDQU32    512(R8), Z31
	VMOVDQU32    576(R8), Z6
	MULD(Z20, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	MULD(Z21, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	VMOVDQU32    640(R8), Z31
	VMOVDQU32    704(R8), Z6
	MULD(Z22, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	MULD(Z23, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	VMOVDQU32    768(R8), Z31
	VMOVDQU32    832(R8), Z6
	MULD(Z24, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	MULD(Z25, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	VMOVDQU32    896(R8), Z31
	VMOVDQU32    960(R8), Z6
	MULD(Z26, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	MULD(Z27, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1)
	VMOVDQU32    0(R9), Z31
	VMOVDQU32    64(R9), Z6
	VPADDD       Z31, Z12, Z12
	VPSUBD       Z0, Z12, Z29
	VPMINUD      Z29, Z12, Z12
	VPADDD       Z6, Z13, Z13
	VPSUBD       Z0, Z13, Z30
	VPMINUD      Z30, Z13, Z13
	VMOVDQU32    Z12, 0(R9)
	VMOVDQU32    Z13, 64(R9)
	VMOVDQU32    128(R9), Z31
	VMOVDQU32    192(R9), Z6
	VPADDD       Z31, Z14, Z14
	VPSUBD       Z0, Z14, Z29
	VPMINUD      Z29, Z14, Z14
	VPADDD       Z6, Z15, Z15
	VPSUBD       Z0, Z15, Z30
	VPMINUD      Z30, Z15, Z15
	VMOVDQU32    Z14, 128(R9)
	VMOVDQU32    Z15, 192(R9)
	VMOVDQU32    256(R9), Z31
	VMOVDQU32    320(R9), Z6
	VPADDD       Z31, Z16, Z16
	VPSUBD       Z0, Z16, Z29
	VPMINUD      Z29, Z16, Z16
	VPADDD       Z6, Z17, Z17
	VPSUBD       Z0, Z17, Z30
	VPMINUD      Z30, Z17, Z17
	VMOVDQU32    Z16, 256(R9)
	VMOVDQU32    Z17, 320(R9)
	VMOVDQU32    384(R9), Z31
	VMOVDQU32    448(R9), Z6
	VPADDD       Z31, Z18, Z18
	VPSUBD       Z0, Z18, Z29
	VPMINUD      Z29, Z18, Z18
	VPADDD       Z6, Z19, Z19
	VPSUBD       Z0, Z19, Z30
	VPMINUD      Z30, Z19, Z19
	VMOVDQU32    Z18, 384(R9)
	VMOVDQU32    Z19, 448(R9)
	VMOVDQU32    512(R9), Z31
	VMOVDQU32    576(R9), Z6
	VPADDD       Z31, Z20, Z20
	VPSUBD       Z0, Z20, Z29
	VPMINUD      Z29, Z20, Z20
	VPADDD       Z6, Z21, Z21
	VPSUBD       Z0, Z21, Z30
	VPMINUD      Z30, Z21, Z21
	VMOVDQU32    Z20, 512(R9)
	VMOVDQU32    Z21, 576(R9)
	VMOVDQU32    640(R9), Z31
	VMOVDQU32    704(R9), Z6
	VPADDD       Z31, Z22, Z22
	VPSUBD       Z0, Z22, Z29
	VPMINUD      Z29, Z22, Z22
	VPADDD       Z6, Z23, Z23
	VPSUBD       Z0, Z23, Z30
	VPMINUD      Z30, Z23, Z23
	VMOVDQU32    Z22, 640(R9)
	VMOVDQU32    Z23, 704(R9)
	VMOVDQU32    768(R9), Z31
	VMOVDQU32    832(R9), Z6
	VPADDD       Z31, Z24, Z24
	VPSUBD       Z0, Z24, Z29
	VPMINUD      Z29, Z24, Z24
	VPADDD       Z6, Z25, Z25
	VPSUBD       Z0, Z25, Z30
	VPMINUD      Z30, Z25, Z25
	VMOVDQU32    Z24, 768(R9)
	VMOVDQU32    Z25, 832(R9)
	VMOVDQU32    896(R9), Z31
	VMOVDQU32    960(R9), Z6
	VPADDD       Z31, Z26, Z26
	VPSUBD       Z0, Z26, Z29
	VPMINUD      Z29, Z26, Z26
	VPADDD       Z6, Z27, Z27
	VPSUBD       Z0, Z27, Z30
	VPMINUD      Z30, Z27, Z27
	VMOVDQU32    Z26, 896(R9)
	VMOVDQU32    Z27, 960(R9)
	DECQ         R10
	TESTQ        R10, R10
	JEQ          done_1
	MOVQ         DI, SI
	VMOVDQU32    0(SP), Z12
	VMOVDQU32    64(SP), Z13
	VMOVDQU32    128(SP), Z14
	VMOVDQU32    192(SP), Z15
	VMOVDQU32    256(SP), Z16
	VMOVDQU32    320(SP), Z17
	VMOVDQU32    384(SP), Z18
	VMOVDQU32    448(SP), Z19
	VMOVDQU32    512(SP), Z20
	VMOVDQU32    576(SP), Z21
	VMOVDQU32    640(SP), Z22
	VMOVDQU32    704(SP), Z23
	VMOVDQU32    768(SP), Z24
	VMOVDQU32    832(SP), Z25
	VMOVDQU32    896(SP), Z26
	VMOVDQU32    960(SP), Z27
	ADDQ         $0x0000000000000400, R8
	ADDQ         $0x0000000000000400, R9
	JMP          fft256_2

done_1:
	RET

TEXT ·sisShuffle_avx512(SB), NOSPLIT, $0-24
	MOVQ      a+0(FP), R15
	MOVQ      a_len+8(FP), DX
	SHRQ      $5, DX
	LOAD_MASKS()
	MOVQ      ·vInterleaveIndices+0(SB), CX
	VMOVDQU64 0(CX), Z3

loop_4:
	TESTQ     DX, DX
	JEQ       done_3      // n == 0, we are done
	VMOVDQU32 0(R15), Z1  // load a[i]
	VMOVDQU32 64(R15), Z2 // load a[i+16]
	PERMUTE8X8(Z1, Z2, Z0)
	PERMUTE4X4(Z1, Z2, Z3, Z0)
	PERMUTE2X2(Z1, Z2, Z0)
	PERMUTE1X1(Z1, Z2, Z0)
	VMOVDQU32 Z1, 0(R15)  // store a[i]
	VMOVDQU32 Z2, 64(R15) // store a[i+16]
	ADDQ      $128, R15
	DECQ      DX          // decrement n
	JMP       loop_4

done_3:
	RET

TEXT ·sisUnshuffle_avx512(SB), NOSPLIT, $0-24
	MOVQ      a+0(FP), R15
	MOVQ      a_len+8(FP), DX
	SHRQ      $5, DX
	LOAD_MASKS()
	MOVQ      ·vInterleaveIndices+0(SB), CX
	VMOVDQU64 0(CX), Z3

loop_6:
	TESTQ      DX, DX
	JEQ        done_5      // n == 0, we are done
	VMOVDQU32  0(R15), Z1  // load a[i]
	VMOVDQU32  64(R15), Z2 // load a[i+16]
	VPUNPCKLDQ Z2, Z1, Z0
	VPUNPCKHDQ Z2, Z1, Z2
	VMOVDQA32  Z0, Z1
	PERMUTE4X4(Z1, Z2, Z3, Z0)
	PERMUTE8X8(Z1, Z2, Z0)
	VMOVDQU32  Z1, 0(R15)  // store a[i]
	VMOVDQU32  Z2, 64(R15) // store a[i+16]
	ADDQ       $128, R15
	DECQ       DX          // decrement n
	JMP        loop_6

done_5:
	RET
