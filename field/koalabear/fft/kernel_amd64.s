//go:build !purego
// Code generated by gnark-crypto/generator. DO NOT EDIT.
#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

// performs a butterfly between 2 vectors of dwords
// in0 = (in0 + in1) mod q
// in1 = (in0 - in1) mod 2q
// in2: q broadcasted on all dwords lanes
// in3: temporary Z register
#define BUTTERFLYD2Q(in0, in1, in2, in3) \
	VPADDD  in0, in1, in3 \
	VPSUBD  in1, in0, in1 \
	VPSUBD  in2, in3, in0 \
	VPMINUD in3, in0, in0 \
	VPADDD  in2, in1, in1 \

// same as butterflyD2Q but reduces in1 to [0,q)
#define BUTTERFLYD1Q(in0, in1, in2, in3, in4) \
	VPADDD  in0, in1, in3 \
	VPSUBD  in1, in0, in1 \
	VPSUBD  in2, in3, in0 \
	VPMINUD in3, in0, in0 \
	VPADDD  in2, in1, in4 \
	VPMINUD in4, in1, in1 \

// same as butterflyD2Q but for qwords
// in2: must be broadcasted on all qwords lanes
#define BUTTERFLYQ2Q(in0, in1, in2, in3) \
	VPADDQ  in0, in1, in3 \
	VPSUBQ  in1, in0, in1 \
	VPSUBQ  in2, in3, in0 \
	VPMINUQ in3, in0, in0 \
	VPADDQ  in2, in1, in1 \

#define BUTTERFLYQ1Q(in0, in1, in2, in3, in4) \
	VPADDQ  in0, in1, in3 \
	VPSUBQ  in1, in0, in1 \
	VPSUBQ  in2, in3, in0 \
	VPMINUQ in3, in0, in0 \
	VPADDQ  in2, in1, in4 \
	VPMINUQ in4, in1, in1 \

// performs a multiplication in place between 2 vectors of qwords (values should be dwords zero extended)
// in0 = (in0 * in1) mod q
// in1: second operand
// in2: mask for low dword in each qword
// in3: q broadcasted on all qwords lanes
// in4: qInvNeg broadcasted on all qwords lanes
// in5: temporary Z register
// in6: temporary Z register
#define MUL(in0, in1, in2, in3, in4, in5, in6) \
	VPMULUDQ in0, in1, in5 \
	VPANDQ   in2, in5, in6 \
	VPMULUDQ in6, in4, in6 \
	VPANDQ   in2, in6, in6 \
	VPMULUDQ in6, in3, in6 \
	VPADDQ   in5, in6, in5 \
	VPSRLQ   $32, in5, in5 \
	VPSUBQ   in3, in5, in6 \
	VPMINUQ  in5, in6, in0 \

// goes from
// Z1 = A A A A B B B B
// Z2 = C C C C D D D D
// we want
// Z1 = A A A A C C C C
// Z2 = B B B B D D D D
#define PERMUTE4X4(in0, in1, in2, in3) \
	VSHUFI64X2 $0x000000000000004e, in1, in0, in2 \
	VPBLENDMQ  in0, in2, in3, in0                 \
	VPBLENDMQ  in2, in1, in3, in1                 \

// Z1 = A A B B C C D D
// Z2 = L L M M N N O O
// we want
// Z1 = A A L L C C N N
// Z2 = B B M M D D O O
#define PERMUTE2X2(in0, in1, in2, in3, in4) \
	VMOVDQA64 in2, in3           \
	VPERMI2Q  in1, in0, in3      \
	VPBLENDMQ in0, in3, in4, in0 \
	VPBLENDMQ in3, in1, in4, in1 \

#define PERMUTE1X1(in0, in1, in2, in3) \
	VPSHRDQ   $32, in1, in0, in2 \
	VPBLENDMD in0, in2, in3, in0 \
	VPBLENDMD in2, in1, in3, in1 \

#define PACK_DWORDS(in0, in1, in2, in3) \
	VPMOVQD      in0, in1          \
	VPMOVQD      in2, in3          \
	VINSERTI64X4 $1, in3, in0, in0 \

TEXT ·innerDITWithTwiddles_avx512(SB), NOSPLIT, $0-72
	// prepare constants needed for mul and reduce ops
	MOVD         $const_q, AX
	VPBROADCASTQ AX, Z8
	MOVD         $const_qInvNeg, AX
	VPBROADCASTQ AX, Z9
	VPCMPEQB     Y0, Y0, Y0
	VPMOVZXDQ    Y0, Z11

	// load arguments
	MOVQ a+0(FP), R15
	MOVQ twiddles+24(FP), CX
	MOVQ end+56(FP), SI
	MOVQ m+64(FP), BX
	CMPQ BX, $0x0000000000000008
	JL   smallerThan8_1          // m < 8
	SHRQ $3, SI                  // we are processing 8 elements at a time
	SHLQ $2, BX                  // offset = m * 4bytes
	MOVQ R15, DX
	ADDQ BX, DX

loop_3:
	TESTQ     SI, SI
	JEQ       done_2     // n == 0, we are done
	VPMOVZXDQ 0(R15), Z0 // load a[i]
	VPMOVZXDQ 0(DX), Z1  // load a[i+m]
	VPMOVZXDQ 0(CX), Z15
	MUL(Z1, Z15, Z11, Z8, Z9, Z12, Z10)
	BUTTERFLYQ1Q(Z0, Z1, Z8, Z3, Z4)
	VPMOVQD   Z0, 0(R15) // store a[i]
	VPMOVQD   Z1, 0(DX)  // store a[i+m]
	ADDQ      $32, R15
	ADDQ      $32, DX
	ADDQ      $32, CX
	DECQ      SI         // decrement n
	JMP       loop_3

done_2:
	RET

smallerThan8_1:
	// m < 8, we call the generic one
	// note that this should happen only when doing a FFT smaller than the smallest generated kernel
	MOVQ a+0(FP), AX
	MOVQ AX, (SP)
	MOVQ twiddles+24(FP), AX
	MOVQ AX, 24(SP)
	MOVQ start+48(FP), AX
	MOVQ AX, 48(SP)
	MOVQ end+56(FP), AX
	MOVQ AX, 56(SP)
	MOVQ m+64(FP), AX
	MOVQ AX, 64(SP)
	CALL ·innerDITWithTwiddlesGeneric(SB)
	RET

TEXT ·innerDIFWithTwiddles_avx512(SB), NOSPLIT, $0-72
	// prepare constants needed for mul and reduce ops
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z2
	VPBROADCASTQ AX, Z8
	MOVD         $const_qInvNeg, AX
	VPBROADCASTQ AX, Z9
	VPCMPEQB     Y0, Y0, Y0
	VPMOVZXDQ    Y0, Z11

	// load arguments
	MOVQ a+0(FP), R15
	MOVQ twiddles+24(FP), CX
	MOVQ end+56(FP), SI
	MOVQ m+64(FP), BX
	CMPQ BX, $0x0000000000000010
	JL   smallerThan16_4         // m < 16
	SHRQ $4, SI                  // we are processing 16 elements at a time
	SHLQ $2, BX                  // offset = m * 4bytes
	MOVQ R15, DX
	ADDQ BX, DX

loop_6:
	TESTQ         SI, SI
	JEQ           done_5      // n == 0, we are done
	VMOVDQU32     0(R15), Z0  // load a[i]
	VMOVDQU32     0(DX), Z1   // load a[i+m]
	BUTTERFLYD2Q(Z0, Z1, Z2, Z3)
	VMOVDQU32     Z0, 0(R15)  // store a[i]
	VEXTRACTI32X8 $0, Z1, Y20
	VEXTRACTI32X8 $1, Z1, Y21
	VPMOVZXDQ     Y20, Z13
	VPMOVZXDQ     Y21, Z14
	VPMOVZXDQ     0(CX), Z15
	VPMOVZXDQ     32(CX), Z16
	MUL(Z13, Z15, Z11, Z8, Z9, Z12, Z10)
	MUL(Z14, Z16, Z11, Z8, Z9, Z12, Z10)
	VPMOVQD       Z13, 0(DX)
	VPMOVQD       Z14, 32(DX)
	ADDQ          $64, R15
	ADDQ          $64, DX
	ADDQ          $64, CX
	DECQ          SI          // decrement n
	JMP           loop_6

done_5:
	RET

smallerThan16_4:
	// m < 16, we call the generic one
	// note that this should happen only when doing a FFT smaller than the smallest generated kernel
	MOVQ a+0(FP), AX
	MOVQ AX, (SP)
	MOVQ twiddles+24(FP), AX
	MOVQ AX, 24(SP)
	MOVQ start+48(FP), AX
	MOVQ AX, 48(SP)
	MOVQ end+56(FP), AX
	MOVQ AX, 56(SP)
	MOVQ m+64(FP), AX
	MOVQ AX, 64(SP)
	CALL ·innerDIFWithTwiddlesGeneric(SB)
	RET

// kerDIFNP_128_avx512(a []{{ .FF }}.Element, twiddles [][]{{ .FF }}.Element, stage int)
TEXT ·kerDIFNP_128_avx512(SB), NOSPLIT, $0-56
	// prepare constants needed for mul and reduce ops
	MOVD         $const_q, AX
	VPBROADCASTQ AX, Z17
	VPBROADCASTD AX, Z18
	MOVD         $const_qInvNeg, AX
	VPBROADCASTQ AX, Z19
	VPCMPEQB     Y0, Y0, Y0
	VPMOVZXDQ    Y0, Z21

	// load arguments
	MOVQ  a+0(FP), R15
	MOVQ  twiddles+24(FP), CX
	MOVQ  stage+48(FP), AX
	IMULQ $24, AX
	ADDQ  AX, CX                  // we want twiddles[stage] as starting point
	MOVQ  $0x0000000000000f0f, AX
	KMOVQ AX, K1

	// load a[:128] in registers
	VPMOVZXDQ    0(R15), Z0
	VPMOVZXDQ    32(R15), Z1
	VPMOVZXDQ    64(R15), Z2
	VPMOVZXDQ    96(R15), Z3
	VPMOVZXDQ    128(R15), Z4
	VPMOVZXDQ    160(R15), Z5
	VPMOVZXDQ    192(R15), Z6
	VPMOVZXDQ    224(R15), Z7
	VPMOVZXDQ    256(R15), Z8
	VPMOVZXDQ    288(R15), Z9
	VPMOVZXDQ    320(R15), Z10
	VPMOVZXDQ    352(R15), Z11
	VPMOVZXDQ    384(R15), Z12
	VPMOVZXDQ    416(R15), Z13
	VPMOVZXDQ    448(R15), Z14
	VPMOVZXDQ    480(R15), Z15
	MOVQ         0(CX), BX
	VPMOVZXDQ    0(BX), Z24
	VPMOVZXDQ    32(BX), Z25
	VPMOVZXDQ    64(BX), Z26
	VPMOVZXDQ    96(BX), Z27
	VPMOVZXDQ    128(BX), Z28
	VPMOVZXDQ    160(BX), Z29
	VPMOVZXDQ    192(BX), Z30
	VPMOVZXDQ    224(BX), Z31
	BUTTERFLYQ2Q(Z0, Z8, Z17, Z22)
	MUL(Z8, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z1, Z9, Z17, Z22)
	MUL(Z9, Z25, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z2, Z10, Z17, Z22)
	MUL(Z10, Z26, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z3, Z11, Z17, Z22)
	MUL(Z11, Z27, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z4, Z12, Z17, Z22)
	MUL(Z12, Z28, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z5, Z13, Z17, Z22)
	MUL(Z13, Z29, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z6, Z14, Z17, Z22)
	MUL(Z14, Z30, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z7, Z15, Z17, Z22)
	MUL(Z15, Z31, Z21, Z17, Z19, Z16, Z20)
	ADDQ         $24, CX
	MOVQ         0(CX), BX
	VPMOVZXDQ    0(BX), Z24
	VPMOVZXDQ    32(BX), Z25
	VPMOVZXDQ    64(BX), Z26
	VPMOVZXDQ    96(BX), Z27
	BUTTERFLYQ2Q(Z0, Z4, Z17, Z22)
	MUL(Z4, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z1, Z5, Z17, Z22)
	MUL(Z5, Z25, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z2, Z6, Z17, Z22)
	MUL(Z6, Z26, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z3, Z7, Z17, Z22)
	MUL(Z7, Z27, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z8, Z12, Z17, Z22)
	MUL(Z12, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z9, Z13, Z17, Z22)
	MUL(Z13, Z25, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z10, Z14, Z17, Z22)
	MUL(Z14, Z26, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z11, Z15, Z17, Z22)
	MUL(Z15, Z27, Z21, Z17, Z19, Z16, Z20)
	ADDQ         $24, CX
	MOVQ         0(CX), BX
	VPMOVZXDQ    0(BX), Z24
	VPMOVZXDQ    32(BX), Z25
	BUTTERFLYQ2Q(Z0, Z2, Z17, Z22)
	MUL(Z2, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z1, Z3, Z17, Z22)
	MUL(Z3, Z25, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z4, Z6, Z17, Z22)
	MUL(Z6, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z5, Z7, Z17, Z22)
	MUL(Z7, Z25, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z8, Z10, Z17, Z22)
	MUL(Z10, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z9, Z11, Z17, Z22)
	MUL(Z11, Z25, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z12, Z14, Z17, Z22)
	MUL(Z14, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z13, Z15, Z17, Z22)
	MUL(Z15, Z25, Z21, Z17, Z19, Z16, Z20)
	ADDQ         $24, CX
	MOVQ         0(CX), BX
	VPMOVZXDQ    0(BX), Z24
	BUTTERFLYQ2Q(Z0, Z1, Z17, Z22)
	MUL(Z1, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z2, Z3, Z17, Z22)
	MUL(Z3, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z4, Z5, Z17, Z22)
	MUL(Z5, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z6, Z7, Z17, Z22)
	MUL(Z7, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z8, Z9, Z17, Z22)
	MUL(Z9, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z10, Z11, Z17, Z22)
	MUL(Z11, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z12, Z13, Z17, Z22)
	MUL(Z13, Z24, Z21, Z17, Z19, Z16, Z20)
	BUTTERFLYQ2Q(Z14, Z15, Z17, Z22)
	MUL(Z15, Z24, Z21, Z17, Z19, Z16, Z20)
	ADDQ         $24, CX
	MOVQ         0(CX), BX
	VPMOVZXDQ    0(BX), Y24                         // zero extend 4x uint32 to 4x uint64
	VINSERTI64X4 $1, Y24, Z24, Z24
	PERMUTE4X4(Z0, Z1, Z22, K1)
	BUTTERFLYQ2Q(Z0, Z1, Z17, Z22)
	MUL(Z1, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE4X4(Z0, Z1, Z22, K1)
	PERMUTE4X4(Z2, Z3, Z22, K1)
	BUTTERFLYQ2Q(Z2, Z3, Z17, Z22)
	MUL(Z3, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE4X4(Z2, Z3, Z22, K1)
	PERMUTE4X4(Z4, Z5, Z22, K1)
	BUTTERFLYQ2Q(Z4, Z5, Z17, Z22)
	MUL(Z5, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE4X4(Z4, Z5, Z22, K1)
	PERMUTE4X4(Z6, Z7, Z22, K1)
	BUTTERFLYQ2Q(Z6, Z7, Z17, Z22)
	MUL(Z7, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE4X4(Z6, Z7, Z22, K1)
	PERMUTE4X4(Z8, Z9, Z22, K1)
	BUTTERFLYQ2Q(Z8, Z9, Z17, Z22)
	MUL(Z9, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE4X4(Z8, Z9, Z22, K1)
	PERMUTE4X4(Z10, Z11, Z22, K1)
	BUTTERFLYQ2Q(Z10, Z11, Z17, Z22)
	MUL(Z11, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE4X4(Z10, Z11, Z22, K1)
	PERMUTE4X4(Z12, Z13, Z22, K1)
	BUTTERFLYQ2Q(Z12, Z13, Z17, Z22)
	MUL(Z13, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE4X4(Z12, Z13, Z22, K1)
	PERMUTE4X4(Z14, Z15, Z22, K1)
	BUTTERFLYQ2Q(Z14, Z15, Z17, Z22)
	MUL(Z15, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE4X4(Z14, Z15, Z22, K1)
	ADDQ         $24, CX
	MOVQ         0(CX), BX
	VPMOVZXDQ    0(BX), X24                         // zero extend 2x uint32 to 2x uint64
	VINSERTI64X2 $1, X24, Z24, Z24
	VINSERTI64X2 $0x0000000000000002, X24, Z24, Z24
	VINSERTI64X2 $0x0000000000000003, X24, Z24, Z24
	MOVQ         $0x0000000000000033, AX
	KMOVQ        AX, K2
	MOVQ         ·vInterleaveIndices+0(SB), DI
	VMOVDQU64    0(DI), Z31
	PERMUTE2X2(Z0, Z1, Z31, Z30, K2)
	BUTTERFLYQ2Q(Z0, Z1, Z17, Z22)
	MUL(Z1, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE2X2(Z0, Z1, Z31, Z30, K2)
	PERMUTE2X2(Z2, Z3, Z31, Z30, K2)
	BUTTERFLYQ2Q(Z2, Z3, Z17, Z22)
	MUL(Z3, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE2X2(Z2, Z3, Z31, Z30, K2)
	PERMUTE2X2(Z4, Z5, Z31, Z30, K2)
	BUTTERFLYQ2Q(Z4, Z5, Z17, Z22)
	MUL(Z5, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE2X2(Z4, Z5, Z31, Z30, K2)
	PERMUTE2X2(Z6, Z7, Z31, Z30, K2)
	BUTTERFLYQ2Q(Z6, Z7, Z17, Z22)
	MUL(Z7, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE2X2(Z6, Z7, Z31, Z30, K2)
	PERMUTE2X2(Z8, Z9, Z31, Z30, K2)
	BUTTERFLYQ2Q(Z8, Z9, Z17, Z22)
	MUL(Z9, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE2X2(Z8, Z9, Z31, Z30, K2)
	PERMUTE2X2(Z10, Z11, Z31, Z30, K2)
	BUTTERFLYQ2Q(Z10, Z11, Z17, Z22)
	MUL(Z11, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE2X2(Z10, Z11, Z31, Z30, K2)
	PERMUTE2X2(Z12, Z13, Z31, Z30, K2)
	BUTTERFLYQ2Q(Z12, Z13, Z17, Z22)
	MUL(Z13, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE2X2(Z12, Z13, Z31, Z30, K2)
	PERMUTE2X2(Z14, Z15, Z31, Z30, K2)
	BUTTERFLYQ2Q(Z14, Z15, Z17, Z22)
	MUL(Z15, Z24, Z21, Z17, Z19, Z16, Z20)
	PERMUTE2X2(Z14, Z15, Z31, Z30, K2)
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	PACK_DWORDS(Z0, Y0, Z1, Y1)
	PACK_DWORDS(Z2, Y2, Z3, Y3)
	PERMUTE1X1(Z0, Z2, Z22, K3)
	BUTTERFLYD1Q(Z0, Z2, Z18, Z22, Z23)
	PERMUTE1X1(Z0, Z2, Z22, K3)
	PACK_DWORDS(Z4, Y4, Z5, Y5)
	PACK_DWORDS(Z6, Y6, Z7, Y7)
	PERMUTE1X1(Z4, Z6, Z22, K3)
	BUTTERFLYD1Q(Z4, Z6, Z18, Z22, Z23)
	PERMUTE1X1(Z4, Z6, Z22, K3)
	PACK_DWORDS(Z8, Y8, Z9, Y9)
	PACK_DWORDS(Z10, Y10, Z11, Y11)
	PERMUTE1X1(Z8, Z10, Z22, K3)
	BUTTERFLYD1Q(Z8, Z10, Z18, Z22, Z23)
	PERMUTE1X1(Z8, Z10, Z22, K3)
	PACK_DWORDS(Z12, Y12, Z13, Y13)
	PACK_DWORDS(Z14, Y14, Z15, Y15)
	PERMUTE1X1(Z12, Z14, Z22, K3)
	BUTTERFLYD1Q(Z12, Z14, Z18, Z22, Z23)
	PERMUTE1X1(Z12, Z14, Z22, K3)

	// store a[:128] in memory
	VMOVDQU32 Z0, 0(R15)
	VMOVDQU32 Z2, 64(R15)
	VMOVDQU32 Z4, 128(R15)
	VMOVDQU32 Z6, 192(R15)
	VMOVDQU32 Z8, 256(R15)
	VMOVDQU32 Z10, 320(R15)
	VMOVDQU32 Z12, 384(R15)
	VMOVDQU32 Z14, 448(R15)
	RET

TEXT ·SISToRefactor(SB), $2112-144
	MOVQ SP, DI
	ANDQ $-64, DI

	// prepare constants needed for mul and reduce ops
	VPCMPEQB     Y0, Y0, Y0
	VPMOVZXDQ    Y0, Z3
	MOVD         $const_q, AX
	VPBROADCASTQ AX, Z0
	VPBROADCASTD AX, Z1
	MOVD         $const_qInvNeg, AX
	VPBROADCASTQ AX, Z2
	MOVQ         k256+0(FP), R15
	MOVQ         k512+24(FP), DX
	MOVQ         cosets+48(FP), SI
	MOVQ         twiddles+72(FP), R14
	MOVQ         0(R14), R8              // twiddles[0]
	MOVQ         R15, CX
	MOVQ         DX, BX
	MOVQ         SI, R9
	ADDQ         $0x0000000000000200, CX
	ADDQ         $0x0000000000000400, BX
	ADDQ         $0x0000000000000400, R9

#define FROMMONTGOMERY(in0) \
	VPMULUDQ in0, Z2, Z5   \
	VPANDQ   Z3, Z5, Z5    \
	VPMULUDQ Z5, Z0, Z5    \
	VPADDQ   in0, Z5, in0  \
	VPSRLQ   $32, in0, in0 \
	VPSUBQ   Z0, in0, Z5   \
	VPMINUQ  in0, Z5, in0  \

#define LIMBSPLIT(in0) \
	VPSHUFLW $0x00000000000000dc, in0, in0 \
	VPSHUFHW $0x00000000000000dc, in0, in0 \

#define SPLITDWORDS(in0, in1, in2, in3) \
	VEXTRACTI32X8 $1, in0, in3 \
	VPMOVZXDQ     in3, in2     \
	VPMOVZXDQ     in1, in0     \

	MOVD         0(SI), AX
	VPBROADCASTQ AX, Z12
	VPMOVZXDQ    0(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    0(SI), Z8
	VPMOVZXDQ    32(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 0(DX)
	VPMOVQD      Z7, 32(DX)
	VPMOVZXDQ    0(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    0(R9), Z8
	VPMOVZXDQ    32(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 0(BX)
	VPMOVQD      Z11, 32(BX)
	VPMOVZXDQ    32(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    64(SI), Z8
	VPMOVZXDQ    96(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 64(DX)
	VPMOVQD      Z7, 96(DX)
	VPMOVZXDQ    32(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    64(R9), Z8
	VPMOVZXDQ    96(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 64(BX)
	VPMOVQD      Z11, 96(BX)
	VPMOVZXDQ    64(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    128(SI), Z8
	VPMOVZXDQ    160(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 128(DX)
	VPMOVQD      Z7, 160(DX)
	VPMOVZXDQ    64(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    128(R9), Z8
	VPMOVZXDQ    160(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 128(BX)
	VPMOVQD      Z11, 160(BX)
	VPMOVZXDQ    96(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    192(SI), Z8
	VPMOVZXDQ    224(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 192(DX)
	VPMOVQD      Z7, 224(DX)
	VPMOVZXDQ    96(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    192(R9), Z8
	VPMOVZXDQ    224(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 192(BX)
	VPMOVQD      Z11, 224(BX)
	VPMOVZXDQ    128(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    256(SI), Z8
	VPMOVZXDQ    288(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 256(DX)
	VPMOVQD      Z7, 288(DX)
	VPMOVZXDQ    128(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    256(R9), Z8
	VPMOVZXDQ    288(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 256(BX)
	VPMOVQD      Z11, 288(BX)
	VPMOVZXDQ    160(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    320(SI), Z8
	VPMOVZXDQ    352(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 320(DX)
	VPMOVQD      Z7, 352(DX)
	VPMOVZXDQ    160(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    320(R9), Z8
	VPMOVZXDQ    352(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 320(BX)
	VPMOVQD      Z11, 352(BX)
	VPMOVZXDQ    192(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    384(SI), Z8
	VPMOVZXDQ    416(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 384(DX)
	VPMOVQD      Z7, 416(DX)
	VPMOVZXDQ    192(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    384(R9), Z8
	VPMOVZXDQ    416(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 384(BX)
	VPMOVQD      Z11, 416(BX)
	VPMOVZXDQ    224(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    448(SI), Z8
	VPMOVZXDQ    480(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 448(DX)
	VPMOVQD      Z7, 480(DX)
	VPMOVZXDQ    224(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    448(R9), Z8
	VPMOVZXDQ    480(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 448(BX)
	VPMOVQD      Z11, 480(BX)
	VPMOVZXDQ    256(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    512(SI), Z8
	VPMOVZXDQ    544(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 512(DX)
	VPMOVQD      Z7, 544(DX)
	VPMOVZXDQ    256(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    512(R9), Z8
	VPMOVZXDQ    544(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 512(BX)
	VPMOVQD      Z11, 544(BX)
	VPMOVZXDQ    288(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    576(SI), Z8
	VPMOVZXDQ    608(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 576(DX)
	VPMOVQD      Z7, 608(DX)
	VPMOVZXDQ    288(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    576(R9), Z8
	VPMOVZXDQ    608(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 576(BX)
	VPMOVQD      Z11, 608(BX)
	VPMOVZXDQ    320(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    640(SI), Z8
	VPMOVZXDQ    672(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 640(DX)
	VPMOVQD      Z7, 672(DX)
	VPMOVZXDQ    320(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    640(R9), Z8
	VPMOVZXDQ    672(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 640(BX)
	VPMOVQD      Z11, 672(BX)
	VPMOVZXDQ    352(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    704(SI), Z8
	VPMOVZXDQ    736(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 704(DX)
	VPMOVQD      Z7, 736(DX)
	VPMOVZXDQ    352(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    704(R9), Z8
	VPMOVZXDQ    736(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 704(BX)
	VPMOVQD      Z11, 736(BX)
	VPMOVZXDQ    384(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    768(SI), Z8
	VPMOVZXDQ    800(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 768(DX)
	VPMOVQD      Z7, 800(DX)
	VPMOVZXDQ    384(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    768(R9), Z8
	VPMOVZXDQ    800(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 768(BX)
	VPMOVQD      Z11, 800(BX)
	VPMOVZXDQ    416(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    832(SI), Z8
	VPMOVZXDQ    864(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 832(DX)
	VPMOVQD      Z7, 864(DX)
	VPMOVZXDQ    416(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    832(R9), Z8
	VPMOVZXDQ    864(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 832(BX)
	VPMOVQD      Z11, 864(BX)
	VPMOVZXDQ    448(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    896(SI), Z8
	VPMOVZXDQ    928(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 896(DX)
	VPMOVQD      Z7, 928(DX)
	VPMOVZXDQ    448(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    896(R9), Z8
	VPMOVZXDQ    928(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 896(BX)
	VPMOVQD      Z11, 928(BX)
	VPMOVZXDQ    480(R15), Z6
	FROMMONTGOMERY(Z6)
	LIMBSPLIT(Z6)
	VPMOVZXDQ    960(SI), Z8
	VPMOVZXDQ    992(SI), Z9
	SPLITDWORDS(Z6, Y6, Z7, Y7)
	MUL(Z6, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z7, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z6, 960(DX)
	VPMOVQD      Z7, 992(DX)
	VPMOVZXDQ    480(CX), Z10
	FROMMONTGOMERY(Z10)
	LIMBSPLIT(Z10)
	VPMOVZXDQ    960(R9), Z8
	VPMOVZXDQ    992(R9), Z9
	SPLITDWORDS(Z10, Y10, Z11, Y11)
	MUL(Z10, Z8, Z3, Z0, Z2, Z4, Z5)
	MUL(Z11, Z9, Z3, Z0, Z2, Z4, Z5)
	VPMOVQD      Z10, 960(BX)
	VPMOVQD      Z11, 992(BX)
	RET
