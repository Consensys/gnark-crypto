//go:build !purego

// Code generated by gnark-crypto/generator. DO NOT EDIT.
// Refer to the generator for more documentation.

#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

TEXT ·permutation24_avx512(SB), NOSPLIT, $0-48
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         $1, AX
	KMOVQ        AX, K2
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z1
	MOVQ         input+0(FP), R15
	MOVQ         roundKeys+24(FP), R14
	VMOVDQU32    0(R15), Z2
	VMOVDQU32    64(R15), Y3
	MOVQ         ·diag24+0(SB), CX
	VMOVDQU32    0(CX), Z18
	VMOVDQU32    64(CX), Y20
	VPSRLQ       $32, Z18, Z19
	VPSRLQ       $32, Y20, Y21

#define ADD(in0, in1, in2, in3, in4) \
	VPADDD  in0, in1, in4 \
	VPSUBD  in2, in4, in3 \
	VPMINUD in4, in3, in4 \

#define MAT_MUL_M4(in0, in1, in2, in3, in4, in5) \
	VPSHUFD $0x000000000000004e, in0, in1 \
	ADD(in1, in0, in4, in5, in1)          \
	VPSHUFD $0x00000000000000b1, in1, in2 \
	ADD(in1, in2, in4, in5, in1)          \
	VPSHUFD $0x0000000000000039, in0, in3 \
	VPSLLD  $1, in3, in3                  \
	VPSUBD  in4, in3, in5                 \
	VPMINUD in3, in5, in3                 \
	ADD(in0, in1, in4, in5, in0)          \
	ADD(in0, in3, in4, in5, in0)          \

#define MAT_MUL_EXTERNAL() \
	MAT_MUL_M4(Z2, Z6, Z7, Z8, Z0, Z11) \
	MAT_MUL_M4(Y3, Y6, Y7, Y8, Y0, Y11) \
	VEXTRACTI64X4 $1, Z2, Y16           \
	ADD(Y16, Y2, Y0, Y11, Y16)          \
	ADD(Y16, Y3, Y0, Y11, Y16)          \
	VSHUFF64X2    $1, Y16, Y16, Y17     \
	ADD(Y16, Y17, Y0, Y11, Y16)         \
	VINSERTI64X4  $1, Y16, Z16, Z16     \
	ADD(Y3, Y16, Y0, Y9, Y3)            \
	ADD(Z2, Z16, Z0, Z11, Z2)           \

#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10) \
	VPSRLQ    $32, in0, in2  \
	VPSRLQ    $32, in1, in3  \
	VPMULUDQ  in0, in1, in4  \
	VPMULUDQ  in2, in3, in5  \
	VPMULUDQ  in4, in9, in6  \
	VPMULUDQ  in5, in9, in7  \
	VPMULUDQ  in6, in8, in6  \
	VPADDQ    in4, in6, in4  \
	VPMULUDQ  in7, in8, in7  \
	VPADDQ    in5, in7, in10 \
	VMOVSHDUP in4, K3, in10  \

#define REDUCE1Q(in0, in1, in2) \
	VPSUBD  in0, in1, in2 \
	VPMINUD in1, in2, in1 \

#define SBOX_FULL() \
	MULD(Z2, Z2, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z8) \
	MULD(Z2, Z8, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z2) \
	REDUCE1Q(Z0, Z2, Z15)                                \
	MULD(Y3, Y3, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y7) \
	MULD(Y3, Y7, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y3) \
	REDUCE1Q(Y0, Y3, Y15)                                \

#define SBOX_PARTIAL() \
	VPMULUDQ X5, X5, X6   \
	VPMULUDQ X6, X1, X14  \
	VPMULUDQ X14, X0, X14 \
	VPADDQ   X6, X14, X6  \
	VPSRLQ   $32, X6, X8  \
	VPMULUDQ X5, X8, X6   \
	VPMULUDQ X6, X1, X14  \
	VPMULUDQ X14, X0, X14 \
	VPADDQ   X6, X14, X6  \
	VPSRLQ   $32, X6, X5  \
	VPSUBD   X0, X5, X14  \
	VPMINUD  X5, X14, X5  \

#define SUM_STATE() \
	VEXTRACTI64X4 $1, Z2, Y16                   \
	ADD(Y16, Y3, Y0, Y11, Y16)                  \
	ADD(Y16, Y10, Y0, Y11, Y16)                 \
	VSHUFF64X2    $1, Y16, Y16, Y17             \
	ADD(Y16, Y17, Y0, Y11, Y16)                 \
	VPSHUFD       $0x000000000000004e, Y16, Y17 \
	ADD(Y16, Y17, Y0, Y11, Y16)                 \
	VPSHUFD       $0x00000000000000b1, Y16, Y17 \
	ADD(Y16, Y17, Y0, Y11, Y16)                 \
	VINSERTI64X4  $1, Y16, Z16, Z16             \

#define FULL_ROUND() \
	VMOVDQU32 0(BX), Z4      \
	VMOVDQU32 64(BX), Y5     \
	ADD(Z2, Z4, Z0, Z11, Z2) \
	ADD(Y3, Y5, Y0, Y8, Y3)  \
	SBOX_FULL()              \
	MAT_MUL_EXTERNAL()       \

	MAT_MUL_EXTERNAL()
	MOVQ 0(R14), BX
	FULL_ROUND()
	MOVQ 24(R14), BX
	FULL_ROUND()
	MOVQ 48(R14), BX
	FULL_ROUND()

	// loop over the partial rounds
	MOVQ $0x0000000000000015, SI // nb partial rounds --> 21
	MOVQ R14, DI
	ADDQ $0x0000000000000048, DI

loop_1:
	TESTQ     SI, SI
	JEQ       done_2
	DECQ      SI
	MOVQ      0(DI), BX
	VMOVD     0(BX), X4
	VMOVDQA32 Z2, Z10
	ADD(X10, X4, X0, X14, X5)
	SBOX_PARTIAL()
	VPBLENDMD Z5, Z10, K2, Z10
	VPSRLQ    $32, Y3, Y12
	VPMULUDQ  Y3, Y20, Y6
	VPMULUDQ  Y12, Y21, Y7
	VPMULUDQ  Y6, Y1, Y14
	VPMULUDQ  Y7, Y1, Y15
	VPMULUDQ  Y14, Y0, Y14
	VPADDQ    Y6, Y14, Y6
	VPMULUDQ  Y15, Y0, Y15
	VPADDQ    Y7, Y15, Y9
	VMOVSHDUP Y6, K3, Y9
	VPSUBD    Y0, Y9, Y11
	VPMINUD   Y9, Y11, Y9
	VPSRLQ    $32, Z2, Z12
	VPMULUDQ  Z12, Z19, Z8
	VPMULUDQ  Z8, Z1, Z15
	VPMULUDQ  Z15, Z0, Z15
	VPADDQ    Z8, Z15, Z8
	SUM_STATE()
	VPMULUDQ  Z10, Z18, Z6
	VPMULUDQ  Z6, Z1, Z14
	VPMULUDQ  Z14, Z0, Z14
	VPADDQ    Z6, Z14, Z6
	VMOVSHDUP Z6, K3, Z8
	VPSUBD    Z0, Z8, Z11
	VPMINUD   Z8, Z11, Z2
	ADD(Z2, Z16, Z0, Z11, Z2)
	ADD(Y9, Y16, Y0, Y5, Y3)
	ADDQ      $24, DI
	JMP       loop_1

done_2:
	MOVQ      576(R14), BX
	FULL_ROUND()
	MOVQ      600(R14), BX
	FULL_ROUND()
	MOVQ      624(R14), BX
	FULL_ROUND()
	VMOVDQU32 Z2, 0(R15)
	VMOVDQU32 Y3, 64(R15)
	RET

TEXT ·permutation16x24_avx512(SB), NOSPLIT, $0-48
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z24
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z25
	MOVQ         $0xffffffffffffffff, R14
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         ·indexScatter8+0(SB), DI
	MOVQ         ·indexGather512+0(SB), R8
	MOVQ         input+0(FP), R15
	MOVQ         nbBlocks+8(FP), CX
	MOVQ         res+16(FP), BX
	MOVQ         roundKeys+24(FP), SI
	VXORPS       Z0, Z0, Z0
	VXORPS       Z1, Z1, Z1
	VXORPS       Z2, Z2, Z2
	VXORPS       Z3, Z3, Z3
	VXORPS       Z4, Z4, Z4
	VXORPS       Z5, Z5, Z5
	VXORPS       Z6, Z6, Z6
	VXORPS       Z7, Z7, Z7

#define DOUBLE(in0, in1, in2, in3) \
	VPSLLD  $1, in0, in3  \
	VPSUBD  in1, in3, in2 \
	VPMINUD in3, in2, in3 \

#define SUB(in0, in1, in2, in3, in4) \
	VPSUBD  in1, in0, in4 \
	VPADDD  in2, in4, in3 \
	VPMINUD in4, in3, in4 \

#define MUL_W(in0, in1, in2, in3, in4, in5, in6, in7) \
	VPSRLQ    $32, in0, in2 \
	VPSRLQ    $32, in1, in3 \
	VPMULUDQ  in0, in1, in4 \
	VPMULUDQ  in2, in3, in5 \
	VPMULUDQ  in4, Z25, in6 \
	VPMULUDQ  in5, Z25, in3 \
	VPMULUDQ  in6, Z24, in6 \
	VPADDQ    in4, in6, in4 \
	VPMULUDQ  in3, Z24, in3 \
	VPADDQ    in5, in3, in7 \
	VMOVSHDUP in4, K3, in7  \

#define MUL2EXPNEGN(in0, in1, in2, in3, in4, in5, in6, in7, in8) \
	VPSRLQ    $32, in0, in5 \
	VPSLLQ    $32, in0, in0 \
	VPSRLQ    in3, in0, in1 \
	VPSLLQ    in4, in5, in6 \
	VPMULUDQ  in1, Z25, in7 \
	VPMULUDQ  in6, Z25, in8 \
	VPMULUDQ  in7, Z24, in7 \
	VPADDQ    in1, in7, in1 \
	VPMULUDQ  in8, Z24, in8 \
	VPADDQ    in6, in8, in2 \
	VMOVSHDUP in1, K3, in2  \
	REDUCE1Q(Z24, in2, in1) \

#define SBOX_W(in0, in1, in2, in3, in4, in5, in6) \
MUL_W(in0, in0, in1, in2, in3, in4, in5, in6) \
MUL_W(in0, in6, in1, in2, in3, in4, in5, in0) \
REDUCE1Q(Z24, in0, in3)                       \

#define MAT_MUL_4_W(in0, in1, in2, in3, in4, in5) \
ADD(Z0, Z1, Z24, in5, in0)   \
ADD(in0, Z2, Z24, in5, in0)  \
ADD(in0, Z3, Z24, in5, in0)  \
DOUBLE(Z0, Z24, in5, in1)    \
DOUBLE(Z1, Z24, in5, in2)    \
DOUBLE(Z2, Z24, in5, in3)    \
DOUBLE(Z3, Z24, in5, in4)    \
ADD(Z0, in0, Z24, in5, Z0)   \
ADD(Z0, in2, Z24, in5, Z0)   \
ADD(Z1, in0, Z24, in5, Z1)   \
ADD(Z1, in3, Z24, in5, Z1)   \
ADD(Z2, in0, Z24, in5, Z2)   \
ADD(Z2, in4, Z24, in5, Z2)   \
ADD(Z3, in0, Z24, in5, Z3)   \
ADD(Z3, in1, Z24, in5, Z3)   \
ADD(Z4, Z5, Z24, in5, in0)   \
ADD(in0, Z6, Z24, in5, in0)  \
ADD(in0, Z7, Z24, in5, in0)  \
DOUBLE(Z4, Z24, in5, in1)    \
DOUBLE(Z5, Z24, in5, in2)    \
DOUBLE(Z6, Z24, in5, in3)    \
DOUBLE(Z7, Z24, in5, in4)    \
ADD(Z4, in0, Z24, in5, Z4)   \
ADD(Z4, in2, Z24, in5, Z4)   \
ADD(Z5, in0, Z24, in5, Z5)   \
ADD(Z5, in3, Z24, in5, Z5)   \
ADD(Z6, in0, Z24, in5, Z6)   \
ADD(Z6, in4, Z24, in5, Z6)   \
ADD(Z7, in0, Z24, in5, Z7)   \
ADD(Z7, in1, Z24, in5, Z7)   \
ADD(Z8, Z9, Z24, in5, in0)   \
ADD(in0, Z10, Z24, in5, in0) \
ADD(in0, Z11, Z24, in5, in0) \
DOUBLE(Z8, Z24, in5, in1)    \
DOUBLE(Z9, Z24, in5, in2)    \
DOUBLE(Z10, Z24, in5, in3)   \
DOUBLE(Z11, Z24, in5, in4)   \
ADD(Z8, in0, Z24, in5, Z8)   \
ADD(Z8, in2, Z24, in5, Z8)   \
ADD(Z9, in0, Z24, in5, Z9)   \
ADD(Z9, in3, Z24, in5, Z9)   \
ADD(Z10, in0, Z24, in5, Z10) \
ADD(Z10, in4, Z24, in5, Z10) \
ADD(Z11, in0, Z24, in5, Z11) \
ADD(Z11, in1, Z24, in5, Z11) \
ADD(Z12, Z13, Z24, in5, in0) \
ADD(in0, Z14, Z24, in5, in0) \
ADD(in0, Z15, Z24, in5, in0) \
DOUBLE(Z12, Z24, in5, in1)   \
DOUBLE(Z13, Z24, in5, in2)   \
DOUBLE(Z14, Z24, in5, in3)   \
DOUBLE(Z15, Z24, in5, in4)   \
ADD(Z12, in0, Z24, in5, Z12) \
ADD(Z12, in2, Z24, in5, Z12) \
ADD(Z13, in0, Z24, in5, Z13) \
ADD(Z13, in3, Z24, in5, Z13) \
ADD(Z14, in0, Z24, in5, Z14) \
ADD(Z14, in4, Z24, in5, Z14) \
ADD(Z15, in0, Z24, in5, Z15) \
ADD(Z15, in1, Z24, in5, Z15) \
ADD(Z16, Z17, Z24, in5, in0) \
ADD(in0, Z18, Z24, in5, in0) \
ADD(in0, Z19, Z24, in5, in0) \
DOUBLE(Z16, Z24, in5, in1)   \
DOUBLE(Z17, Z24, in5, in2)   \
DOUBLE(Z18, Z24, in5, in3)   \
DOUBLE(Z19, Z24, in5, in4)   \
ADD(Z16, in0, Z24, in5, Z16) \
ADD(Z16, in2, Z24, in5, Z16) \
ADD(Z17, in0, Z24, in5, Z17) \
ADD(Z17, in3, Z24, in5, Z17) \
ADD(Z18, in0, Z24, in5, Z18) \
ADD(Z18, in4, Z24, in5, Z18) \
ADD(Z19, in0, Z24, in5, Z19) \
ADD(Z19, in1, Z24, in5, Z19) \
ADD(Z20, Z21, Z24, in5, in0) \
ADD(in0, Z22, Z24, in5, in0) \
ADD(in0, Z23, Z24, in5, in0) \
DOUBLE(Z20, Z24, in5, in1)   \
DOUBLE(Z21, Z24, in5, in2)   \
DOUBLE(Z22, Z24, in5, in3)   \
DOUBLE(Z23, Z24, in5, in4)   \
ADD(Z20, in0, Z24, in5, Z20) \
ADD(Z20, in2, Z24, in5, Z20) \
ADD(Z21, in0, Z24, in5, Z21) \
ADD(Z21, in3, Z24, in5, Z21) \
ADD(Z22, in0, Z24, in5, Z22) \
ADD(Z22, in4, Z24, in5, Z22) \
ADD(Z23, in0, Z24, in5, Z23) \
ADD(Z23, in1, Z24, in5, Z23) \

#define MAT_MUL_EXTERNAL_W(in0, in1, in2, in3, in4, in5) \
MAT_MUL_4_W(in0, in1, in2, in3, in4, in5) \
ADD(Z0, Z4, Z24, in5, in1)                \
ADD(Z1, Z5, Z24, in5, in2)                \
ADD(Z2, Z6, Z24, in5, in3)                \
ADD(Z3, Z7, Z24, in5, in4)                \
ADD(Z8, in1, Z24, in5, in1)               \
ADD(Z9, in2, Z24, in5, in2)               \
ADD(Z10, in3, Z24, in5, in3)              \
ADD(Z11, in4, Z24, in5, in4)              \
ADD(Z12, in1, Z24, in5, in1)              \
ADD(Z13, in2, Z24, in5, in2)              \
ADD(Z14, in3, Z24, in5, in3)              \
ADD(Z15, in4, Z24, in5, in4)              \
ADD(Z16, in1, Z24, in5, in1)              \
ADD(Z17, in2, Z24, in5, in2)              \
ADD(Z18, in3, Z24, in5, in3)              \
ADD(Z19, in4, Z24, in5, in4)              \
ADD(Z20, in1, Z24, in5, in1)              \
ADD(Z21, in2, Z24, in5, in2)              \
ADD(Z22, in3, Z24, in5, in3)              \
ADD(Z23, in4, Z24, in5, in4)              \
ADD(Z0, in1, Z24, in5, Z0)                \
ADD(Z1, in2, Z24, in5, Z1)                \
ADD(Z2, in3, Z24, in5, Z2)                \
ADD(Z3, in4, Z24, in5, Z3)                \
ADD(Z4, in1, Z24, in5, Z4)                \
ADD(Z5, in2, Z24, in5, Z5)                \
ADD(Z6, in3, Z24, in5, Z6)                \
ADD(Z7, in4, Z24, in5, Z7)                \
ADD(Z8, in1, Z24, in5, Z8)                \
ADD(Z9, in2, Z24, in5, Z9)                \
ADD(Z10, in3, Z24, in5, Z10)              \
ADD(Z11, in4, Z24, in5, Z11)              \
ADD(Z12, in1, Z24, in5, Z12)              \
ADD(Z13, in2, Z24, in5, Z13)              \
ADD(Z14, in3, Z24, in5, Z14)              \
ADD(Z15, in4, Z24, in5, Z15)              \
ADD(Z16, in1, Z24, in5, Z16)              \
ADD(Z17, in2, Z24, in5, Z17)              \
ADD(Z18, in3, Z24, in5, Z18)              \
ADD(Z19, in4, Z24, in5, Z19)              \
ADD(Z20, in1, Z24, in5, Z20)              \
ADD(Z21, in2, Z24, in5, Z21)              \
ADD(Z22, in3, Z24, in5, Z22)              \
ADD(Z23, in4, Z24, in5, Z23)              \

outer_loop_3:
	TESTQ      CX, CX
	JEQ        outer_loop_end_4
	VMOVDQU32  0(R8), Z26
	KMOVD      R14, K1
	VPGATHERDD 0(R15)(Z26*4), K1, Z8
	KMOVD      R14, K1
	VPGATHERDD 4(R15)(Z26*4), K1, Z9
	KMOVD      R14, K1
	VPGATHERDD 8(R15)(Z26*4), K1, Z10
	KMOVD      R14, K1
	VPGATHERDD 12(R15)(Z26*4), K1, Z11
	KMOVD      R14, K1
	VPGATHERDD 16(R15)(Z26*4), K1, Z12
	KMOVD      R14, K1
	VPGATHERDD 20(R15)(Z26*4), K1, Z13
	KMOVD      R14, K1
	VPGATHERDD 24(R15)(Z26*4), K1, Z14
	KMOVD      R14, K1
	VPGATHERDD 28(R15)(Z26*4), K1, Z15
	KMOVD      R14, K1
	VPGATHERDD 32(R15)(Z26*4), K1, Z16
	KMOVD      R14, K1
	VPGATHERDD 36(R15)(Z26*4), K1, Z17
	KMOVD      R14, K1
	VPGATHERDD 40(R15)(Z26*4), K1, Z18
	KMOVD      R14, K1
	VPGATHERDD 44(R15)(Z26*4), K1, Z19
	KMOVD      R14, K1
	VPGATHERDD 48(R15)(Z26*4), K1, Z20
	KMOVD      R14, K1
	VPGATHERDD 52(R15)(Z26*4), K1, Z21
	KMOVD      R14, K1
	VPGATHERDD 56(R15)(Z26*4), K1, Z22
	KMOVD      R14, K1
	VPGATHERDD 60(R15)(Z26*4), K1, Z23
	ADDQ       $0x0000000000000040, R15
	MAT_MUL_EXTERNAL_W(Z27, Z28, Z29, Z30, Z31, Z26)

#define ADD_RC_SBOX(in0, in1, in2, in3, in4, in5, in6, in7) \
	VPBROADCASTD in7, in5                     \
	ADD(in6, in5, Z24, in5, in6)              \
	SBOX_W(in6, in0, in1, in2, in3, in4, in5) \

	// loop over the first full rounds
	MOVQ $0x0000000000000003, R10
	MOVQ SI, R11

loop_5:
	TESTQ R10, R10
	JEQ   done_6
	DECQ  R10
	MOVQ  0(R11), R9
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z0, 0(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z1, 4(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z2, 8(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z3, 12(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z4, 16(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z5, 20(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z6, 24(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z7, 28(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z8, 32(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z9, 36(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z10, 40(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z11, 44(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z12, 48(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z13, 52(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z14, 56(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z15, 60(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z16, 64(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z17, 68(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z18, 72(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z19, 76(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z20, 80(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z21, 84(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z22, 88(R9))
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z23, 92(R9))
	MAT_MUL_EXTERNAL_W(Z27, Z28, Z29, Z30, Z31, Z26)
	ADDQ  $24, R11
	JMP   loop_5

done_6:
	// loop over the partial rounds
	MOVQ $0x0000000000000015, R12
	MOVQ SI, R13
	ADDQ $0x0000000000000048, R13

loop_7:
	TESTQ        R12, R12
	JEQ          done_8
	DECQ         R12
	MOVQ         0(R13), R9
	ADD_RC_SBOX(Z27, Z28, Z29, Z30, Z31, Z26, Z0, 0(R9))
	ADD(Z0, Z1, Z24, Z28, Z27)
	ADD(Z2, Z27, Z24, Z28, Z27)
	ADD(Z3, Z27, Z24, Z28, Z27)
	ADD(Z4, Z27, Z24, Z28, Z27)
	ADD(Z5, Z27, Z24, Z28, Z27)
	ADD(Z6, Z27, Z24, Z28, Z27)
	ADD(Z7, Z27, Z24, Z28, Z27)
	ADD(Z8, Z27, Z24, Z28, Z27)
	ADD(Z9, Z27, Z24, Z28, Z27)
	ADD(Z10, Z27, Z24, Z28, Z27)
	ADD(Z11, Z27, Z24, Z28, Z27)
	ADD(Z12, Z27, Z24, Z28, Z27)
	ADD(Z13, Z27, Z24, Z28, Z27)
	ADD(Z14, Z27, Z24, Z28, Z27)
	ADD(Z15, Z27, Z24, Z28, Z27)
	ADD(Z16, Z27, Z24, Z28, Z27)
	ADD(Z17, Z27, Z24, Z28, Z27)
	ADD(Z18, Z27, Z24, Z28, Z27)
	ADD(Z19, Z27, Z24, Z28, Z27)
	ADD(Z20, Z27, Z24, Z28, Z27)
	ADD(Z21, Z27, Z24, Z28, Z27)
	ADD(Z22, Z27, Z24, Z28, Z27)
	ADD(Z23, Z27, Z24, Z28, Z27)
	DOUBLE(Z0, Z24, Z28, Z29)
	SUB(Z27, Z29, Z24, Z28, Z0)
	ADD(Z27, Z1, Z24, Z28, Z1)
	DOUBLE(Z2, Z24, Z28, Z2)
	ADD(Z2, Z27, Z24, Z28, Z2)
	MOVQ         $1, AX
	VPBROADCASTD AX, Z29
	VPTESTMD     Z3, Z29, K4
	VPADDD       Z3, Z24, K4, Z3
	VPSRLD       $1, Z3, Z3
	ADD(Z27, Z3, Z24, Z28, Z3)
	VPTESTMD     Z6, Z29, K4
	VPADDD       Z6, Z24, K4, Z6
	VPSRLD       $1, Z6, Z6
	SUB(Z27, Z6, Z24, Z28, Z6)
	DOUBLE(Z4, Z24, Z28, Z29)
	ADD(Z4, Z29, Z24, Z28, Z4)
	ADD(Z4, Z27, Z24, Z28, Z4)
	DOUBLE(Z5, Z24, Z28, Z5)
	ADD(Z5, Z5, Z24, Z28, Z5)
	ADD(Z5, Z27, Z24, Z28, Z5)
	DOUBLE(Z7, Z24, Z28, Z29)
	ADD(Z7, Z29, Z24, Z28, Z7)
	SUB(Z27, Z7, Z24, Z28, Z7)
	DOUBLE(Z8, Z24, Z28, Z8)
	DOUBLE(Z8, Z24, Z28, Z8)
	SUB(Z27, Z8, Z24, Z28, Z8)
	MUL2EXPNEGN(Z9, Z28, Z9, $8, $24, Z30, Z31, Z26, Z29)
	ADD(Z9, Z27, Z24, Z28, Z9)
	MUL2EXPNEGN(Z10, Z28, Z10, $2, $30, Z30, Z31, Z26, Z29)
	ADD(Z10, Z27, Z24, Z28, Z10)
	MUL2EXPNEGN(Z11, Z28, Z11, $3, $29, Z30, Z31, Z26, Z29)
	ADD(Z11, Z27, Z24, Z28, Z11)
	MUL2EXPNEGN(Z12, Z28, Z12, $4, $28, Z30, Z31, Z26, Z29)
	ADD(Z12, Z27, Z24, Z28, Z12)
	MUL2EXPNEGN(Z13, Z28, Z13, $5, $27, Z30, Z31, Z26, Z29)
	ADD(Z13, Z27, Z24, Z28, Z13)
	MUL2EXPNEGN(Z14, Z28, Z14, $6, $26, Z30, Z31, Z26, Z29)
	ADD(Z14, Z27, Z24, Z28, Z14)
	MUL2EXPNEGN(Z15, Z28, Z15, $24, $8, Z30, Z31, Z26, Z29)
	ADD(Z15, Z27, Z24, Z28, Z15)
	MUL2EXPNEGN(Z16, Z28, Z16, $8, $24, Z30, Z31, Z26, Z29)
	SUB(Z27, Z16, Z24, Z28, Z16)
	MUL2EXPNEGN(Z17, Z28, Z17, $3, $29, Z30, Z31, Z26, Z29)
	SUB(Z27, Z17, Z24, Z28, Z17)
	MUL2EXPNEGN(Z18, Z28, Z18, $4, $28, Z30, Z31, Z26, Z29)
	SUB(Z27, Z18, Z24, Z28, Z18)
	MUL2EXPNEGN(Z19, Z28, Z19, $5, $27, Z30, Z31, Z26, Z29)
	SUB(Z27, Z19, Z24, Z28, Z19)
	MUL2EXPNEGN(Z20, Z28, Z20, $6, $26, Z30, Z31, Z26, Z29)
	SUB(Z27, Z20, Z24, Z28, Z20)
	MUL2EXPNEGN(Z21, Z28, Z21, $7, $25, Z30, Z31, Z26, Z29)
	SUB(Z27, Z21, Z24, Z28, Z21)
	MUL2EXPNEGN(Z22, Z28, Z22, $9, $23, Z30, Z31, Z26, Z29)
	SUB(Z27, Z22, Z24, Z28, Z22)
	MUL2EXPNEGN(Z23, Z28, Z23, $24, $8, Z30, Z31, Z26, Z29)
	SUB(Z27, Z23, Z24, Z28, Z23)
	ADDQ         $24, R13
	JMP          loop_7

done_8:
	// loop over the final full rounds
	MOVQ $0x0000000000000003, R10
	MOVQ SI, R11
	ADDQ $0x0000000000000240, R11

loop_9:
	TESTQ R10, R10
	JEQ   done_10
	DECQ  R10
	MOVQ  0(R11), R9
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z0, 0(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z1, 4(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z2, 8(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z3, 12(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z4, 16(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z5, 20(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z6, 24(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z7, 28(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z8, 32(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z9, 36(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z10, 40(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z11, 44(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z12, 48(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z13, 52(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z14, 56(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z15, 60(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z16, 64(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z17, 68(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z18, 72(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z19, 76(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z20, 80(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z21, 84(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z22, 88(R9))
	ADD_RC_SBOX(Z30, Z31, Z26, Z29, Z27, Z28, Z23, 92(R9))
	MAT_MUL_EXTERNAL_W(Z30, Z31, Z26, Z29, Z27, Z28)
	ADDQ  $24, R11
	JMP   loop_9

done_10:
	DECQ CX
	JMP  outer_loop_3

outer_loop_end_4:
	VMOVDQU32   0(DI), Z30
	KMOVD       R14, K1
	VPSCATTERDD Z0, K1, 0(BX)(Z30*4)
	KMOVD       R14, K1
	VPSCATTERDD Z1, K1, 4(BX)(Z30*4)
	KMOVD       R14, K1
	VPSCATTERDD Z2, K1, 8(BX)(Z30*4)
	KMOVD       R14, K1
	VPSCATTERDD Z3, K1, 12(BX)(Z30*4)
	KMOVD       R14, K1
	VPSCATTERDD Z4, K1, 16(BX)(Z30*4)
	KMOVD       R14, K1
	VPSCATTERDD Z5, K1, 20(BX)(Z30*4)
	KMOVD       R14, K1
	VPSCATTERDD Z6, K1, 24(BX)(Z30*4)
	KMOVD       R14, K1
	VPSCATTERDD Z7, K1, 28(BX)(Z30*4)
	RET

TEXT ·permutation16_avx512(SB), NOSPLIT, $0-48
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         $1, AX
	KMOVQ        AX, K2
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z1
	MOVQ         input+0(FP), R15
	MOVQ         roundKeys+24(FP), R14
	VMOVDQU32    0(R15), Z2
	MOVQ         ·diag16+0(SB), CX
	VMOVDQU32    0(CX), Z18
	VPSRLQ       $32, Z18, Z19

#define MAT_MUL_EXTERNAL_16() \
	MAT_MUL_M4(Z2, Z6, Z7, Z8, Z0, Z11) \
	VEXTRACTI64X4 $1, Z2, Y16           \
	ADD(Y16, Y2, Y0, Y11, Y16)          \
	VSHUFF64X2    $1, Y16, Y16, Y17     \
	ADD(Y16, Y17, Y0, Y11, Y16)         \
	VINSERTI64X4  $1, Y16, Z16, Z16     \
	ADD(Z2, Z16, Z0, Z11, Z2)           \

#define SBOX_FULL_16() \
	MULD(Z2, Z2, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z8) \
	MULD(Z2, Z8, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z2) \
	REDUCE1Q(Z0, Z2, Z15)                                \

#define SUM_STATE_16() \
	VEXTRACTI64X4 $1, Z2, Y16                   \
	ADD(Y16, Y10, Y0, Y11, Y16)                 \
	VSHUFF64X2    $1, Y16, Y16, Y17             \
	ADD(Y16, Y17, Y0, Y11, Y16)                 \
	VPSHUFD       $0x000000000000004e, Y16, Y17 \
	ADD(Y16, Y17, Y0, Y11, Y16)                 \
	VPSHUFD       $0x00000000000000b1, Y16, Y17 \
	ADD(Y16, Y17, Y0, Y11, Y16)                 \
	VINSERTI64X4  $1, Y16, Z16, Z16             \

#define FULL_ROUND_16() \
	VMOVDQU32 0(BX), Z4      \
	ADD(Z2, Z4, Z0, Z11, Z2) \
	SBOX_FULL_16()           \
	MAT_MUL_EXTERNAL_16()    \

	MAT_MUL_EXTERNAL_16()
	MOVQ 0(R14), BX
	FULL_ROUND_16()
	MOVQ 24(R14), BX
	FULL_ROUND_16()
	MOVQ 48(R14), BX
	FULL_ROUND_16()

	// loop over the partial rounds
	MOVQ $0x0000000000000015, SI // nb partial rounds --> 21
	MOVQ R14, DI
	ADDQ $0x0000000000000048, DI

loop_11:
	TESTQ     SI, SI
	JEQ       done_12
	DECQ      SI
	MOVQ      0(DI), BX
	VMOVD     0(BX), X4
	VMOVDQA32 Z2, Z10
	ADD(X10, X4, X0, X14, X5)
	SBOX_PARTIAL()
	VPBLENDMD Z5, Z10, K2, Z10
	VPSRLQ    $32, Z2, Z12
	VPMULUDQ  Z12, Z19, Z8
	VPMULUDQ  Z8, Z1, Z15
	VPMULUDQ  Z15, Z0, Z15
	VPADDQ    Z8, Z15, Z8
	SUM_STATE_16()
	VPMULUDQ  Z10, Z18, Z6
	VPMULUDQ  Z6, Z1, Z14
	VPMULUDQ  Z14, Z0, Z14
	VPADDQ    Z6, Z14, Z6
	VMOVSHDUP Z6, K3, Z8
	VPSUBD    Z0, Z8, Z11
	VPMINUD   Z8, Z11, Z2
	ADD(Z2, Z16, Z0, Z11, Z2)
	ADDQ      $24, DI
	JMP       loop_11

done_12:
	MOVQ      576(R14), BX
	FULL_ROUND_16()
	MOVQ      600(R14), BX
	FULL_ROUND_16()
	MOVQ      624(R14), BX
	FULL_ROUND_16()
	VMOVDQU32 Z2, 0(R15)
	RET
