//go:build !purego

// Code generated by gnark-crypto/generator. DO NOT EDIT.
// Refer to the generator for more documentation.
// Some sub-functions are derived from Plonky3:
// https://github.com/Plonky3/Plonky3/blob/36e619f3c6526ee86e2e5639a24b3224e1c1700f/monty-31/src/x86_64_avx512/packing.rs#L319

#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

TEXT ·permutation24_avx512(SB), NOSPLIT, $0-48
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         $1, AX
	KMOVQ        AX, K2
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z1
	VPBROADCASTQ AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z2
	MOVQ         input+0(FP), R15
	MOVQ         roundKeys+24(FP), R14
	MOVQ         ·diag24+0(SB), BX
	VMOVDQU32    0(BX), Z17
	VMOVDQU32    64(BX), Y19
	VPSRLQ       $32, Z17, Z18
	VMOVDQU32    0(R15), Z3
	VMOVDQU32    64(R15), Y4

#define ADD(in0, in1, in2, in3, in4) \
	VPADDD  in0, in1, in4 \
	VPSUBD  in2, in4, in3 \
	VPMINUD in4, in3, in4 \

#define MATMULM4(in0, in1, in2, in3, in4, in5) \
	VPSHUFD $0x000000000000004e, in0, in1 \
	ADD(in1, in0, in4, in5, in1)          \
	VPSHUFD $0x00000000000000b1, in1, in2 \
	ADD(in1, in2, in4, in5, in1)          \
	VPSHUFD $0x0000000000000039, in0, in3 \
	VPSLLD  $1, in3, in3                  \
	VPSUBD  in4, in3, in5                 \
	VPMINUD in3, in5, in3                 \
	ADD(in0, in1, in4, in5, in0)          \
	ADD(in0, in3, in4, in5, in0)          \

#define MAT_MUL_EXTERNAL() \
	MATMULM4(Z3, Z8, Z9, Z10, Z1, Z12) \
	MATMULM4(Y4, Y8, Y9, Y10, Y1, Y12) \
	VEXTRACTI64X4 $1, Z3, Y20          \
	ADD(Y20, Y3, Y1, Y12, Y20)         \
	ADD(Y20, Y4, Y1, Y12, Y20)         \
	VEXTRACTI64X2 $1, Y20, X21         \
	ADD(X20, X21, X1, X12, X20)        \
	VINSERTI64X2  $1, X20, Y20, Y20    \
	VINSERTI64X4  $1, Y20, Z20, Z20    \
	ADD(Y4, Y20, Y1, Y12, Y4)          \
	ADD(Z3, Z20, Z1, Z12, Z3)          \

#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10) \
	VPSRLQ    $32, in0, in2  \
	VPSRLQ    $32, in1, in3  \
	VPMULUDQ  in0, in1, in4  \
	VPMULUDQ  in2, in3, in5  \
	VPMULUDQ  in4, in9, in6  \
	VPMULUDQ  in5, in9, in7  \
	VPMULUDQ  in6, in8, in6  \
	VPADDQ    in4, in6, in4  \
	VPMULUDQ  in7, in8, in7  \
	VPADDQ    in5, in7, in10 \
	VMOVSHDUP in4, K3, in10  \

#define REDUCE1Q(in0, in1, in2) \
	VPSUBD  in0, in1, in2 \
	VPMINUD in1, in2, in1 \

#define SBOX() \
	MULD(Z3, Z3, Z13, Z14, Z8, Z9, Z15, Z16, Z1, Z2, Z10) \
	MULD(Z3, Z10, Z13, Z14, Z8, Z9, Z15, Z16, Z1, Z2, Z3) \
	REDUCE1Q(Z1, Z3, Z16)                                 \
	MULD(Y4, Y4, Y13, Y14, Y8, Y9, Y15, Y16, Y1, Y2, Y9)  \
	MULD(Y4, Y9, Y13, Y14, Y8, Y9, Y15, Y16, Y1, Y2, Y4)  \
	REDUCE1Q(Y1, Y4, Y16)                                 \

#define SUMSTATE() \
	VEXTRACTI64X4 $1, Z3, Y20                   \
	ADD(Y20, Y4, Y1, Y12, Y20)                  \
	ADD(Y20, Y5, Y1, Y12, Y20)                  \
	VSHUFF64X2    $1, Y20, Y20, Y22             \
	ADD(Y20, Y22, Y1, Y12, Y20)                 \
	VPSHUFD       $0x000000000000004e, Y20, Y22 \
	ADD(Y20, Y22, Y1, Y12, Y20)                 \
	VPSHUFD       $0x00000000000000b1, Y20, Y22 \
	ADD(Y20, Y22, Y1, Y12, Y20)                 \
	VINSERTI64X4  $1, Y20, Z20, Z20             \

#define FULLROUND() \
	VMOVDQU32 0(CX), Z6      \
	VMOVDQU32 64(CX), Y7     \
	ADD(Z3, Z6, Z1, Z12, Z3) \
	ADD(Y4, Y7, Y1, Y10, Y4) \
	SBOX()                   \
	MAT_MUL_EXTERNAL()       \

#define PARTIALROUND() \
	VMOVD     0(CX), X6                                    \
	VMOVDQA32 Z3, Z5                                       \
	VPADDD    X5, X6, X7                                   \
	VPSUBD    X1, X7, X15                                  \
	VPMINUD   X7, X15, X7                                  \
	VPMULUDQ  X7, X7, X8                                   \
	VPMULUDQ  X8, X2, X15                                  \
	VPMULUDQ  X15, X1, X15                                 \
	VPADDQ    X8, X15, X8                                  \
	VPSRLQ    $32, X8, X10                                 \
	VPMULUDQ  X7, X10, X8                                  \
	VPMULUDQ  X8, X2, X15                                  \
	VPMULUDQ  X15, X1, X15                                 \
	VPADDQ    X8, X15, X8                                  \
	VPSRLQ    $32, X8, X7                                  \
	VPSUBD    X1, X7, X15                                  \
	VPMINUD   X7, X15, X7                                  \
	VPBLENDMD Z7, Z5, K2, Z5                               \
	MULD(Y4, Y19, Y13, Y14, Y8, Y9, Y15, Y16, Y1, Y2, Y11) \
	REDUCE1Q(Y1, Y11, Y16)                                 \
	VPSRLQ    $32, Z3, Z13                                 \
	VPMULUDQ  Z13, Z18, Z10                                \
	VPMULUDQ  Z10, Z2, Z16                                 \
	VPMULUDQ  Z16, Z1, Z16                                 \
	VPADDQ    Z10, Z16, Z10                                \
	SUMSTATE()                                             \
	VPMULUDQ  Z5, Z17, Z8                                  \
	VPMULUDQ  Z8, Z2, Z15                                  \
	VPMULUDQ  Z15, Z1, Z15                                 \
	VPADDQ    Z8, Z15, Z8                                  \
	VMOVSHDUP Z8, K3, Z10                                  \
	VPSUBD    Z1, Z10, Z12                                 \
	VPMINUD   Z10, Z12, Z3                                 \
	ADD(Z3, Z20, Z1, Z12, Z3)                              \
	ADD(Y11, Y20, Y1, Y7, Y4)                              \

	MAT_MUL_EXTERNAL()
	MOVQ 0(R14), CX
	FULLROUND()
	MOVQ 24(R14), CX
	FULLROUND()
	MOVQ 48(R14), CX
	FULLROUND()

	// loop over the partial rounds
	MOVQ $0x0000000000000015, SI // nb partial rounds --> 21
	MOVQ R14, DI
	ADDQ $0x0000000000000048, DI

loop_1:
	TESTQ SI, SI
	JEQ   done_2
	MOVQ  0(DI), CX
	PARTIALROUND()
	ADDQ  $24, DI
	DECQ  SI
	JMP   loop_1

done_2:
	MOVQ      576(R14), CX
	FULLROUND()
	MOVQ      600(R14), CX
	FULLROUND()
	MOVQ      624(R14), CX
	FULLROUND()
	VMOVDQU32 Z3, 0(R15)
	VMOVDQU32 Y4, 64(R15)
	RET
