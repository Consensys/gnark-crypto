//go:build !purego

// Code generated by gnark-crypto/generator. DO NOT EDIT.
// Refer to the generator for more documentation.
// Some sub-functions are derived from Plonky3:
// https://github.com/Plonky3/Plonky3/blob/36e619f3c6526ee86e2e5639a24b3224e1c1700f/monty-31/src/x86_64_avx512/packing.rs#L319

#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

TEXT ·permutation24_avx512(SB), NOSPLIT, $0-48
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	MOVQ         $1, AX
	KMOVQ        AX, K2
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z1
	VPBROADCASTQ AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z2
	MOVQ         input+0(FP), R15
	MOVQ         roundKeys+24(FP), R14
	MOVQ         ·diag24+0(SB), BX
	VMOVDQU32    0(BX), Z16
	VMOVDQU32    64(BX), Y17
	VMOVDQU32    0(R15), Z3
	VMOVDQU32    64(R15), Y4

#define ADD(in0, in1, in2, in3) \
	VPADDD  in0, in1, in0 \
	VPSUBD  in2, in0, in3 \
	VPMINUD in0, in3, in0 \

#define ADDINTO(in0, in1, in2, in3, in4) \
	VPADDD  in0, in1, in4 \
	VPSUBD  in2, in4, in3 \
	VPMINUD in4, in3, in4 \

#define MATMULM4(in0, in1, in2, in3, in4, in5) \
	VPSHUFD $0x000000000000004e, in0, in1 \
	ADD(in1, in0, in4, in5)               \
	VPSHUFD $0x00000000000000b1, in1, in2 \
	ADD(in1, in2, in4, in5)               \
	VPSHUFD $0x0000000000000039, in0, in3 \
	VPSLLD  $1, in3, in3                  \
	VPSUBD  in4, in3, in5                 \
	VPMINUD in3, in5, in3                 \
	ADD(in0, in1, in4, in5)               \
	ADD(in0, in3, in4, in5)               \

#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10) \
	VPSRLQ    $32, in0, in2  \
	VPSRLQ    $32, in1, in3  \
	VPMULUDQ  in0, in1, in4  \
	VPMULUDQ  in2, in3, in5  \
	VPMULUDQ  in4, in9, in6  \
	VPMULUDQ  in5, in9, in7  \
	VPMULUDQ  in6, in8, in6  \
	VPADDQ    in4, in6, in4  \
	VPMULUDQ  in7, in8, in7  \
	VPADDQ    in5, in7, in10 \
	VMOVSHDUP in4, K3, in10  \

	MATMULM4(Z3, Z7, Z8, Z9, Z1, Z11)
	MATMULM4(Y4, Y7, Y8, Y9, Y1, Y11)
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y3, Y1, Y11)
	ADD(Y18, Y4, Y1, Y11)
	VEXTRACTI64X2 $1, Y18, X19
	ADD(X18, X19, X1, X11)
	VINSERTI64X2  $1, X18, Y18, Y18
	VINSERTI64X4  $1, Y18, Z18, Z18
	ADD(Y4, Y18, Y1, Y11)
	ADD(Z3, Z18, Z1, Z11)
	MOVQ          0(R14), CX
	VMOVDQU32     0(CX), Z5
	VMOVDQU32     64(CX), Y6
	ADD(Z3, Z5, Z1, Z11)
	ADD(Y4, Y6, Y1, Y9)
	MULD(Z3, Z3, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z9)
	MULD(Z3, Z9, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	MULD(Y4, Y4, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y9)
	MULD(Y4, Y9, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y4)
	VPSUBD        Y1, Y4, Y15
	VPMINUD       Y4, Y15, Y4
	MATMULM4(Z3, Z7, Z8, Z9, Z1, Z11)
	MATMULM4(Y4, Y7, Y8, Y9, Y1, Y11)
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y3, Y1, Y11)
	ADD(Y18, Y4, Y1, Y11)
	VEXTRACTI64X2 $1, Y18, X19
	ADD(X18, X19, X1, X11)
	VINSERTI64X2  $1, X18, Y18, Y18
	VINSERTI64X4  $1, Y18, Z18, Z18
	ADD(Y4, Y18, Y1, Y11)
	ADD(Z3, Z18, Z1, Z11)
	MOVQ          24(R14), CX
	VMOVDQU32     0(CX), Z5
	VMOVDQU32     64(CX), Y6
	ADD(Z3, Z5, Z1, Z11)
	ADD(Y4, Y6, Y1, Y9)
	MULD(Z3, Z3, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z9)
	MULD(Z3, Z9, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	MULD(Y4, Y4, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y9)
	MULD(Y4, Y9, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y4)
	VPSUBD        Y1, Y4, Y15
	VPMINUD       Y4, Y15, Y4
	MATMULM4(Z3, Z7, Z8, Z9, Z1, Z11)
	MATMULM4(Y4, Y7, Y8, Y9, Y1, Y11)
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y3, Y1, Y11)
	ADD(Y18, Y4, Y1, Y11)
	VEXTRACTI64X2 $1, Y18, X19
	ADD(X18, X19, X1, X11)
	VINSERTI64X2  $1, X18, Y18, Y18
	VINSERTI64X4  $1, Y18, Z18, Z18
	ADD(Y4, Y18, Y1, Y11)
	ADD(Z3, Z18, Z1, Z11)
	MOVQ          48(R14), CX
	VMOVDQU32     0(CX), Z5
	VMOVDQU32     64(CX), Y6
	ADD(Z3, Z5, Z1, Z11)
	ADD(Y4, Y6, Y1, Y9)
	MULD(Z3, Z3, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z9)
	MULD(Z3, Z9, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	MULD(Y4, Y4, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y9)
	MULD(Y4, Y9, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y4)
	VPSUBD        Y1, Y4, Y15
	VPMINUD       Y4, Y15, Y4
	MATMULM4(Z3, Z7, Z8, Z9, Z1, Z11)
	MATMULM4(Y4, Y7, Y8, Y9, Y1, Y11)
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y3, Y1, Y11)
	ADD(Y18, Y4, Y1, Y11)
	VEXTRACTI64X2 $1, Y18, X19
	ADD(X18, X19, X1, X11)
	VINSERTI64X2  $1, X18, Y18, Y18
	VINSERTI64X4  $1, Y18, Z18, Z18
	ADD(Y4, Y18, Y1, Y11)
	ADD(Z3, Z18, Z1, Z11)
	MOVQ          72(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          96(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          120(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          144(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          168(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          192(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          216(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          240(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          264(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          288(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          312(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          336(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          360(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          384(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          408(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          432(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          456(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          480(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          504(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          528(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          552(R14), CX
	VMOVD         0(CX), X5
	VMOVDQA32     X3, X6
	ADD(X6, X5, X1, X11)
	MULD(X6, X6, X12, X13, X7, X8, X14, X15, X1, X2, X9)
	MULD(X6, X9, X12, X13, X7, X8, X14, X15, X1, X2, X6)
	VPSUBD        X1, X6, X15
	VPMINUD       X6, X15, X6
	MULD(Y4, Y17, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y10)
	VPSUBD        Y1, Y10, Y15
	VPMINUD       Y10, Y15, Y10
	VPBLENDMD     Z6, Z3, K2, Z21
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y4, Y1, Y11)
	ADD(Y18, Y21, Y1, Y11)
	VSHUFF64X2    $1, Y18, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x000000000000004e, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VPSHUFD       $0x00000000000000b1, Y18, Y20
	ADD(Y18, Y20, Y1, Y11)
	VINSERTI64X4  $1, Y18, Z18, Z18
	MULD(Z21, Z16, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	ADD(Z3, Z18, Z1, Z11)
	ADDINTO(Y10, Y18, Y1, Y6, Y4)
	MOVQ          576(R14), CX
	VMOVDQU32     0(CX), Z5
	VMOVDQU32     64(CX), Y6
	ADD(Z3, Z5, Z1, Z11)
	ADD(Y4, Y6, Y1, Y11)
	MULD(Z3, Z3, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z9)
	MULD(Z3, Z9, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	MULD(Y4, Y4, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y9)
	MULD(Y4, Y9, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y4)
	VPSUBD        Y1, Y4, Y15
	VPMINUD       Y4, Y15, Y4
	MATMULM4(Z3, Z7, Z8, Z9, Z1, Z11)
	MATMULM4(Y4, Y7, Y8, Y9, Y1, Y11)
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y3, Y1, Y11)
	ADD(Y18, Y4, Y1, Y11)
	VEXTRACTI64X2 $1, Y18, X19
	ADD(X18, X19, X1, X11)
	VINSERTI64X2  $1, X18, Y18, Y18
	VINSERTI64X4  $1, Y18, Z18, Z18
	ADD(Y4, Y18, Y1, Y11)
	ADD(Z3, Z18, Z1, Z11)
	MOVQ          600(R14), CX
	VMOVDQU32     0(CX), Z5
	VMOVDQU32     64(CX), Y6
	ADD(Z3, Z5, Z1, Z11)
	ADD(Y4, Y6, Y1, Y11)
	MULD(Z3, Z3, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z9)
	MULD(Z3, Z9, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	MULD(Y4, Y4, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y9)
	MULD(Y4, Y9, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y4)
	VPSUBD        Y1, Y4, Y15
	VPMINUD       Y4, Y15, Y4
	MATMULM4(Z3, Z7, Z8, Z9, Z1, Z11)
	MATMULM4(Y4, Y7, Y8, Y9, Y1, Y11)
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y3, Y1, Y11)
	ADD(Y18, Y4, Y1, Y11)
	VEXTRACTI64X2 $1, Y18, X19
	ADD(X18, X19, X1, X11)
	VINSERTI64X2  $1, X18, Y18, Y18
	VINSERTI64X4  $1, Y18, Z18, Z18
	ADD(Y4, Y18, Y1, Y11)
	ADD(Z3, Z18, Z1, Z11)
	MOVQ          624(R14), CX
	VMOVDQU32     0(CX), Z5
	VMOVDQU32     64(CX), Y6
	ADD(Z3, Z5, Z1, Z11)
	ADD(Y4, Y6, Y1, Y11)
	MULD(Z3, Z3, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z9)
	MULD(Z3, Z9, Z12, Z13, Z7, Z8, Z14, Z15, Z1, Z2, Z3)
	VPSUBD        Z1, Z3, Z15
	VPMINUD       Z3, Z15, Z3
	MULD(Y4, Y4, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y9)
	MULD(Y4, Y9, Y12, Y13, Y7, Y8, Y14, Y15, Y1, Y2, Y4)
	VPSUBD        Y1, Y4, Y15
	VPMINUD       Y4, Y15, Y4
	MATMULM4(Z3, Z7, Z8, Z9, Z1, Z11)
	MATMULM4(Y4, Y7, Y8, Y9, Y1, Y11)
	VEXTRACTI64X4 $1, Z3, Y18
	ADD(Y18, Y3, Y1, Y11)
	ADD(Y18, Y4, Y1, Y11)
	VEXTRACTI64X2 $1, Y18, X19
	ADD(X18, X19, X1, X11)
	VINSERTI64X2  $1, X18, Y18, Y18
	VINSERTI64X4  $1, Y18, Z18, Z18
	ADD(Y4, Y18, Y1, Y11)
	ADD(Z3, Z18, Z1, Z11)
	VMOVDQU32     Z3, 0(R15)
	VMOVDQU32     Y4, 64(R15)
	RET
