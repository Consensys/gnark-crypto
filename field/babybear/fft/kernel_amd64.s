//go:build !purego
// Code generated by gnark-crypto/generator. DO NOT EDIT.
#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

// performs a butterfly between 2 vectors of dwords
// in0 = (in0 + in1) mod q
// in1 = (in0 - in1) mod 2q
// in2: q broadcasted on all dwords lanes
// in3: temporary Z register
#define BUTTERFLYD2Q(in0, in1, in2, in3) \
	VPADDD  in0, in1, in3 \
	VPSUBD  in1, in0, in1 \
	VPSUBD  in2, in3, in0 \
	VPMINUD in3, in0, in0 \
	VPADDD  in2, in1, in1 \

// same as butterflyD2Q but reduces in1 to [0,q)
#define BUTTERFLYD1Q(in0, in1, in2, in3, in4) \
	VPADDD  in0, in1, in3 \
	VPSUBD  in1, in0, in1 \
	VPSUBD  in2, in3, in0 \
	VPMINUD in3, in0, in0 \
	VPADDD  in2, in1, in4 \
	VPMINUD in4, in1, in1 \

// same as butterflyD2Q but for qwords
// in2: must be broadcasted on all qwords lanes
#define BUTTERFLYQ2Q(in0, in1, in2, in3) \
	VPADDQ  in0, in1, in3 \
	VPSUBQ  in1, in0, in1 \
	VPSUBQ  in2, in3, in0 \
	VPMINUQ in3, in0, in0 \
	VPADDQ  in2, in1, in1 \

#define BUTTERFLYQ1Q(in0, in1, in2, in3, in4) \
	VPADDQ  in0, in1, in3 \
	VPSUBQ  in1, in0, in1 \
	VPSUBQ  in2, in3, in0 \
	VPMINUQ in3, in0, in0 \
	VPADDQ  in2, in1, in4 \
	VPMINUQ in4, in1, in1 \

// performs a multiplication in place between 2 vectors of qwords (values should be dwords zero extended)
// in0 = (in0 * in1) mod q
// in1: second operand
// in2: mask for low dword in each qword
// in3: q broadcasted on all qwords lanes
// in4: qInvNeg broadcasted on all qwords lanes
// in5: temporary Z register
// in6: temporary Z register
#define MULQ(in0, in1, in2, in3, in4, in5, in6) \
	VPMULUDQ in0, in1, in5 \
	VPANDQ   in2, in5, in6 \
	VPMULUDQ in6, in4, in6 \
	VPANDQ   in2, in6, in6 \
	VPMULUDQ in6, in3, in6 \
	VPADDQ   in5, in6, in5 \
	VPSRLQ   $32, in5, in5 \
	VPSUBQ   in3, in5, in6 \
	VPMINUQ  in5, in6, in0 \

#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10) \
	VMOVSHDUP in0, in2       \
	VMOVSHDUP in1, in3       \
	VPMULUDQ  in0, in1, in4  \
	VPMULUDQ  in2, in3, in5  \
	VPMULUDQ  in4, in9, in6  \
	VPMULUDQ  in5, in9, in7  \
	VPMULUDQ  in6, in8, in6  \
	VPMULUDQ  in7, in8, in7  \
	VPADDQ    in4, in6, in4  \
	VPADDQ    in5, in7, in5  \
	VMOVSHDUP in4, in10, in5 \
	VPSUBD    in8, in5, in7  \
	VPMINUD   in5, in7, in0  \

// goes from
// Z1 = A A A A B B B B
// Z2 = C C C C D D D D
// we want
// Z1 = A A A A C C C C
// Z2 = B B B B D D D D
#define PERMUTE8X8(in0, in1, in2, in3) \
	VSHUFI64X2 $0x000000000000004e, in1, in0, in2 \
	VPBLENDMQ  in0, in2, in3, in0                 \
	VPBLENDMQ  in2, in1, in3, in1                 \

// Z1 = A A B B C C D D
// Z2 = L L M M N N O O
// we want
// Z1 = A A L L C C N N
// Z2 = B B M M D D O O
#define PERMUTE4X4(in0, in1, in2, in3, in4) \
	VMOVDQA64 in2, in3           \
	VPERMI2Q  in1, in0, in3      \
	VPBLENDMQ in0, in3, in4, in0 \
	VPBLENDMQ in3, in1, in4, in1 \

#define PERMUTE2X2(in0, in1, in2, in3) \
	VSHUFPD   $0x0000000000000055, in1, in0, in2 \
	VPBLENDMQ in0, in2, in3, in0                 \
	VPBLENDMQ in2, in1, in3, in1                 \

#define PERMUTE1X1(in0, in1, in2, in3) \
	VPSHRDQ   $32, in1, in0, in2 \
	VPBLENDMD in0, in2, in3, in0 \
	VPBLENDMD in2, in1, in3, in1 \

#define PACK_DWORDS(in0, in1, in2, in3) \
	VPMOVQD      in0, in1          \
	VPMOVQD      in2, in3          \
	VINSERTI64X4 $1, in3, in0, in0 \

TEXT ·innerDITWithTwiddles_avx512(SB), NOSPLIT, $0-72
	// prepare constants needed for mul and reduce ops
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z4
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z5

	// load arguments
	MOVQ  a+0(FP), R15
	MOVQ  twiddles+24(FP), CX
	MOVQ  end+56(FP), SI
	MOVQ  m+64(FP), BX
	MOVQ  $0x0000000000005555, AX
	KMOVD AX, K3
	CMPQ  BX, $0x0000000000000010
	JL    smallerThan16_1         // m < 16
	SHRQ  $4, SI                  // we are processing 16 elements at a time
	SHLQ  $2, BX                  // offset = m * 4bytes
	MOVQ  R15, DX
	ADDQ  BX, DX

loop_3:
	TESTQ     SI, SI
	JEQ       done_2     // n == 0, we are done
	VMOVDQU32 0(R15), Z0 // load a[i]
	VMOVDQU32 0(DX), Z1  // load a[i+m]
	VMOVDQU32 0(CX), Z6
	MULD(Z1, Z6, Z7, Z8, Z2, Z3, Z9, Z10, Z4, Z5, K3)
	BUTTERFLYD1Q(Z0, Z1, Z4, Z2, Z3)
	VMOVDQU32 Z0, 0(R15) // store a[i]
	VMOVDQU32 Z1, 0(DX)  // store a[i+m]
	ADDQ      $64, R15
	ADDQ      $64, DX
	ADDQ      $64, CX
	DECQ      SI         // decrement n
	JMP       loop_3

done_2:
	RET

smallerThan16_1:
	// m < 16, we call the generic one
	// note that this should happen only when doing a FFT smaller than the smallest generated kernel
	MOVQ a+0(FP), AX
	MOVQ AX, (SP)
	MOVQ twiddles+24(FP), AX
	MOVQ AX, 24(SP)
	MOVQ start+48(FP), AX
	MOVQ AX, 48(SP)
	MOVQ end+56(FP), AX
	MOVQ AX, 56(SP)
	MOVQ m+64(FP), AX
	MOVQ AX, 64(SP)
	CALL ·innerDITWithTwiddlesGeneric(SB)
	RET

TEXT ·innerDIFWithTwiddles_avx512(SB), NOSPLIT, $0-72
	// prepare constants needed for mul and reduce ops
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z2
	VPBROADCASTD AX, Z4
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z5

	// load arguments
	MOVQ  a+0(FP), R15
	MOVQ  twiddles+24(FP), CX
	MOVQ  end+56(FP), SI
	MOVQ  m+64(FP), BX
	CMPQ  BX, $0x0000000000000010
	JL    smallerThan16_4         // m < 16
	SHRQ  $4, SI                  // we are processing 16 elements at a time
	SHLQ  $2, BX                  // offset = m * 4bytes
	MOVQ  R15, DX
	ADDQ  BX, DX
	MOVQ  $0x0000000000005555, AX
	KMOVD AX, K3

loop_6:
	TESTQ     SI, SI
	JEQ       done_5     // n == 0, we are done
	VMOVDQU32 0(R15), Z0 // load a[i]
	VMOVDQU32 0(DX), Z1  // load a[i+m]
	BUTTERFLYD2Q(Z0, Z1, Z2, Z3)
	VMOVDQU32 Z0, 0(R15) // store a[i]
	VMOVDQU32 0(CX), Z6
	MULD(Z1, Z6, Z7, Z8, Z3, Z9, Z10, Z11, Z4, Z5, K3)
	VMOVDQU32 Z1, 0(DX)
	ADDQ      $64, R15
	ADDQ      $64, DX
	ADDQ      $64, CX
	DECQ      SI         // decrement n
	JMP       loop_6

done_5:
	RET

smallerThan16_4:
	// m < 16, we call the generic one
	// note that this should happen only when doing a FFT smaller than the smallest generated kernel
	MOVQ a+0(FP), AX
	MOVQ AX, (SP)
	MOVQ twiddles+24(FP), AX
	MOVQ AX, 24(SP)
	MOVQ start+48(FP), AX
	MOVQ AX, 48(SP)
	MOVQ end+56(FP), AX
	MOVQ AX, 56(SP)
	MOVQ m+64(FP), AX
	MOVQ AX, 64(SP)
	CALL ·innerDIFWithTwiddlesGeneric(SB)
	RET

// kerDIFNP_256_avx512(a []{{ .FF }}.Element, twiddles [][]{{ .FF }}.Element, stage int)
TEXT ·kerDIFNP_256_avx512(SB), NOSPLIT, $0-56
	// prepare constants needed for mul and reduce ops
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z16
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z17

	// load arguments
	MOVQ         a+0(FP), R15
	MOVQ         twiddles+24(FP), CX
	MOVQ         stage+48(FP), AX
	IMULQ        $24, AX
	ADDQ         AX, CX                             // we want twiddles[stage] as starting point
	MOVQ         $0x0000000000000f0f, AX
	KMOVQ        AX, K1
	MOVQ         $0x0000000000000033, AX
	KMOVQ        AX, K2
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3
	VMOVDQU32    0(R15), Z0                         // load a[0]
	VMOVDQU32    64(R15), Z1                        // load a[1]
	VMOVDQU32    128(R15), Z2                       // load a[2]
	VMOVDQU32    192(R15), Z3                       // load a[3]
	VMOVDQU32    256(R15), Z4                       // load a[4]
	VMOVDQU32    320(R15), Z5                       // load a[5]
	VMOVDQU32    384(R15), Z6                       // load a[6]
	VMOVDQU32    448(R15), Z7                       // load a[7]
	VMOVDQU32    512(R15), Z8                       // load a[8]
	VMOVDQU32    576(R15), Z9                       // load a[9]
	VMOVDQU32    640(R15), Z10                      // load a[10]
	VMOVDQU32    704(R15), Z11                      // load a[11]
	VMOVDQU32    768(R15), Z12                      // load a[12]
	VMOVDQU32    832(R15), Z13                      // load a[13]
	VMOVDQU32    896(R15), Z14                      // load a[14]
	VMOVDQU32    960(R15), Z15                      // load a[15]
	MOVQ         0(CX), DI
	VMOVDQU32    0(DI), Z18
	VMOVDQU32    64(DI), Z19
	VMOVDQU32    128(DI), Z20
	VMOVDQU32    192(DI), Z21
	VMOVDQU32    256(DI), Z22
	VMOVDQU32    320(DI), Z23
	VMOVDQU32    384(DI), Z24
	VMOVDQU32    448(DI), Z25
	BUTTERFLYD2Q(Z0, Z8, Z16, Z26)
	MULD(Z8, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z1, Z9, Z16, Z26)
	MULD(Z9, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z2, Z10, Z16, Z26)
	MULD(Z10, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z3, Z11, Z16, Z26)
	MULD(Z11, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z4, Z12, Z16, Z26)
	MULD(Z12, Z22, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z5, Z13, Z16, Z26)
	MULD(Z13, Z23, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z6, Z14, Z16, Z26)
	MULD(Z14, Z24, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z7, Z15, Z16, Z26)
	MULD(Z15, Z25, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	ADDQ         $24, CX
	MOVQ         0(CX), DI
	VMOVDQU32    0(DI), Z18
	VMOVDQU32    64(DI), Z19
	VMOVDQU32    128(DI), Z20
	VMOVDQU32    192(DI), Z21
	BUTTERFLYD2Q(Z0, Z4, Z16, Z26)
	MULD(Z4, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z1, Z5, Z16, Z26)
	MULD(Z5, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z2, Z6, Z16, Z26)
	MULD(Z6, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z3, Z7, Z16, Z26)
	MULD(Z7, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z8, Z12, Z16, Z26)
	MULD(Z12, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z9, Z13, Z16, Z26)
	MULD(Z13, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z10, Z14, Z16, Z26)
	MULD(Z14, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z11, Z15, Z16, Z26)
	MULD(Z15, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	ADDQ         $24, CX
	MOVQ         0(CX), DI
	VMOVDQU32    0(DI), Z18
	VMOVDQU32    64(DI), Z19
	BUTTERFLYD2Q(Z0, Z2, Z16, Z26)
	MULD(Z2, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z1, Z3, Z16, Z26)
	MULD(Z3, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z4, Z6, Z16, Z26)
	MULD(Z6, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z5, Z7, Z16, Z26)
	MULD(Z7, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z8, Z10, Z16, Z26)
	MULD(Z10, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z9, Z11, Z16, Z26)
	MULD(Z11, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z12, Z14, Z16, Z26)
	MULD(Z14, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z13, Z15, Z16, Z26)
	MULD(Z15, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	ADDQ         $24, CX
	MOVQ         0(CX), DI
	VMOVDQU32    0(DI), Z18
	BUTTERFLYD2Q(Z0, Z1, Z16, Z26)
	MULD(Z1, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z2, Z3, Z16, Z26)
	MULD(Z3, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z4, Z5, Z16, Z26)
	MULD(Z5, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z6, Z7, Z16, Z26)
	MULD(Z7, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z8, Z9, Z16, Z26)
	MULD(Z9, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z10, Z11, Z16, Z26)
	MULD(Z11, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z12, Z13, Z16, Z26)
	MULD(Z13, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	BUTTERFLYD2Q(Z14, Z15, Z16, Z26)
	MULD(Z15, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	ADDQ         $24, CX
	MOVQ         ·vInterleaveIndices+0(SB), R8
	VMOVDQU64    0(R8), Z22
	MOVQ         0(CX), DI
	VMOVDQU32    0(DI), Y18
	VINSERTI64X4 $1, Y18, Z18, Z18
	MOVQ         24(CX), DI
	VMOVDQU32    0(DI), X19
	VINSERTI64X2 $1, X19, Z19, Z19
	VINSERTI64X2 $0x0000000000000002, X19, Z19, Z19
	VINSERTI64X2 $0x0000000000000003, X19, Z19, Z19
	MOVQ         48(CX), DI
	VPBROADCASTD 0(DI), Z20
	VPBROADCASTD 4(DI), Z21
	VPBLENDMD    Z20, Z21, K3, Z20
	PERMUTE8X8(Z0, Z1, Z26, K1)
	BUTTERFLYD2Q(Z0, Z1, Z16, Z26)
	MULD(Z1, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE8X8(Z2, Z3, Z26, K1)
	BUTTERFLYD2Q(Z2, Z3, Z16, Z26)
	MULD(Z3, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE8X8(Z4, Z5, Z26, K1)
	BUTTERFLYD2Q(Z4, Z5, Z16, Z26)
	MULD(Z5, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE8X8(Z6, Z7, Z26, K1)
	BUTTERFLYD2Q(Z6, Z7, Z16, Z26)
	MULD(Z7, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE8X8(Z8, Z9, Z26, K1)
	BUTTERFLYD2Q(Z8, Z9, Z16, Z26)
	MULD(Z9, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE8X8(Z10, Z11, Z26, K1)
	BUTTERFLYD2Q(Z10, Z11, Z16, Z26)
	MULD(Z11, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE8X8(Z12, Z13, Z26, K1)
	BUTTERFLYD2Q(Z12, Z13, Z16, Z26)
	MULD(Z13, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE8X8(Z14, Z15, Z26, K1)
	BUTTERFLYD2Q(Z14, Z15, Z16, Z26)
	MULD(Z15, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE4X4(Z0, Z1, Z22, Z26, K2)
	BUTTERFLYD2Q(Z0, Z1, Z16, Z26)
	MULD(Z1, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE4X4(Z2, Z3, Z22, Z26, K2)
	BUTTERFLYD2Q(Z2, Z3, Z16, Z26)
	MULD(Z3, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE4X4(Z4, Z5, Z22, Z26, K2)
	BUTTERFLYD2Q(Z4, Z5, Z16, Z26)
	MULD(Z5, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE4X4(Z6, Z7, Z22, Z26, K2)
	BUTTERFLYD2Q(Z6, Z7, Z16, Z26)
	MULD(Z7, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE4X4(Z8, Z9, Z22, Z26, K2)
	BUTTERFLYD2Q(Z8, Z9, Z16, Z26)
	MULD(Z9, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE4X4(Z10, Z11, Z22, Z26, K2)
	BUTTERFLYD2Q(Z10, Z11, Z16, Z26)
	MULD(Z11, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE4X4(Z12, Z13, Z22, Z26, K2)
	BUTTERFLYD2Q(Z12, Z13, Z16, Z26)
	MULD(Z13, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE4X4(Z14, Z15, Z22, Z26, K2)
	BUTTERFLYD2Q(Z14, Z15, Z16, Z26)
	MULD(Z15, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE2X2(Z0, Z1, Z26, K3)
	BUTTERFLYD2Q(Z0, Z1, Z16, Z26)
	MULD(Z1, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE2X2(Z2, Z3, Z26, K3)
	BUTTERFLYD2Q(Z2, Z3, Z16, Z26)
	MULD(Z3, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE2X2(Z4, Z5, Z26, K3)
	BUTTERFLYD2Q(Z4, Z5, Z16, Z26)
	MULD(Z5, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE2X2(Z6, Z7, Z26, K3)
	BUTTERFLYD2Q(Z6, Z7, Z16, Z26)
	MULD(Z7, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE2X2(Z8, Z9, Z26, K3)
	BUTTERFLYD2Q(Z8, Z9, Z16, Z26)
	MULD(Z9, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE2X2(Z10, Z11, Z26, K3)
	BUTTERFLYD2Q(Z10, Z11, Z16, Z26)
	MULD(Z11, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE2X2(Z12, Z13, Z26, K3)
	BUTTERFLYD2Q(Z12, Z13, Z16, Z26)
	MULD(Z13, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE2X2(Z14, Z15, Z26, K3)
	BUTTERFLYD2Q(Z14, Z15, Z16, Z26)
	MULD(Z15, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17, K3)
	PERMUTE1X1(Z0, Z1, Z26, K3)
	BUTTERFLYD1Q(Z0, Z1, Z16, Z26, Z27)
	PERMUTE1X1(Z2, Z3, Z26, K3)
	BUTTERFLYD1Q(Z2, Z3, Z16, Z26, Z27)
	PERMUTE1X1(Z4, Z5, Z26, K3)
	BUTTERFLYD1Q(Z4, Z5, Z16, Z26, Z27)
	PERMUTE1X1(Z6, Z7, Z26, K3)
	BUTTERFLYD1Q(Z6, Z7, Z16, Z26, Z27)
	PERMUTE1X1(Z8, Z9, Z26, K3)
	BUTTERFLYD1Q(Z8, Z9, Z16, Z26, Z27)
	PERMUTE1X1(Z10, Z11, Z26, K3)
	BUTTERFLYD1Q(Z10, Z11, Z16, Z26, Z27)
	PERMUTE1X1(Z12, Z13, Z26, K3)
	BUTTERFLYD1Q(Z12, Z13, Z16, Z26, Z27)
	PERMUTE1X1(Z14, Z15, Z26, K3)
	BUTTERFLYD1Q(Z14, Z15, Z16, Z26, Z27)
	VPUNPCKLDQ   Z1, Z0, Z23
	VPUNPCKHDQ   Z1, Z0, Z1
	VMOVDQA32    Z23, Z0
	PERMUTE4X4(Z0, Z1, Z22, Z23, K2)
	PERMUTE8X8(Z0, Z1, Z23, K1)
	VMOVDQU32    Z0, 0(R15)
	VMOVDQU32    Z1, 64(R15)
	VPUNPCKLDQ   Z3, Z2, Z23
	VPUNPCKHDQ   Z3, Z2, Z3
	VMOVDQA32    Z23, Z2
	PERMUTE4X4(Z2, Z3, Z22, Z23, K2)
	PERMUTE8X8(Z2, Z3, Z23, K1)
	VMOVDQU32    Z2, 128(R15)
	VMOVDQU32    Z3, 192(R15)
	VPUNPCKLDQ   Z5, Z4, Z23
	VPUNPCKHDQ   Z5, Z4, Z5
	VMOVDQA32    Z23, Z4
	PERMUTE4X4(Z4, Z5, Z22, Z23, K2)
	PERMUTE8X8(Z4, Z5, Z23, K1)
	VMOVDQU32    Z4, 256(R15)
	VMOVDQU32    Z5, 320(R15)
	VPUNPCKLDQ   Z7, Z6, Z23
	VPUNPCKHDQ   Z7, Z6, Z7
	VMOVDQA32    Z23, Z6
	PERMUTE4X4(Z6, Z7, Z22, Z23, K2)
	PERMUTE8X8(Z6, Z7, Z23, K1)
	VMOVDQU32    Z6, 384(R15)
	VMOVDQU32    Z7, 448(R15)
	VPUNPCKLDQ   Z9, Z8, Z23
	VPUNPCKHDQ   Z9, Z8, Z9
	VMOVDQA32    Z23, Z8
	PERMUTE4X4(Z8, Z9, Z22, Z23, K2)
	PERMUTE8X8(Z8, Z9, Z23, K1)
	VMOVDQU32    Z8, 512(R15)
	VMOVDQU32    Z9, 576(R15)
	VPUNPCKLDQ   Z11, Z10, Z23
	VPUNPCKHDQ   Z11, Z10, Z11
	VMOVDQA32    Z23, Z10
	PERMUTE4X4(Z10, Z11, Z22, Z23, K2)
	PERMUTE8X8(Z10, Z11, Z23, K1)
	VMOVDQU32    Z10, 640(R15)
	VMOVDQU32    Z11, 704(R15)
	VPUNPCKLDQ   Z13, Z12, Z23
	VPUNPCKHDQ   Z13, Z12, Z13
	VMOVDQA32    Z23, Z12
	PERMUTE4X4(Z12, Z13, Z22, Z23, K2)
	PERMUTE8X8(Z12, Z13, Z23, K1)
	VMOVDQU32    Z12, 768(R15)
	VMOVDQU32    Z13, 832(R15)
	VPUNPCKLDQ   Z15, Z14, Z23
	VPUNPCKHDQ   Z15, Z14, Z15
	VMOVDQA32    Z23, Z14
	PERMUTE4X4(Z14, Z15, Z22, Z23, K2)
	PERMUTE8X8(Z14, Z15, Z23, K1)
	VMOVDQU32    Z14, 896(R15)
	VMOVDQU32    Z15, 960(R15)
	RET

TEXT ·SISToRefactor(SB), $1088-120
	MOVQ SP, DI
	ANDQ $-64, DI

	// prepare constants needed for mul and reduce ops
	MOVD         $const_q, AX
	VPBROADCASTD AX, Z0
	MOVD         $const_qInvNeg, AX
	VPBROADCASTD AX, Z1
	MOVQ         k256+0(FP), R15
	MOVQ         cosets+24(FP), CX
	MOVQ         twiddles+48(FP), BX
	MOVQ         rag+72(FP), R14
	MOVQ         res+96(FP), R8
	MOVQ         0(BX), SI               // twiddles[0]
	MOVQ         R15, DX
	MOVQ         CX, R9
	ADDQ         $0x0000000000000200, DX
	ADDQ         $0x0000000000000400, R9
	MOVQ         $0x0000000000000f0f, AX
	KMOVQ        AX, K1
	MOVQ         $0x0000000000000033, AX
	KMOVQ        AX, K2
	MOVQ         $0x0000000000005555, AX
	KMOVD        AX, K3

#define FROMMONTGOMERY(in0, in1, in2, in3, in4, in5, in6, in7) \
	VPANDD.Z  in0, in0, K3, in1 \
	VPSRLQ    $32, in0, in2     \
	VPMULUDQ  in1, in6, in3     \
	VPMULUDQ  in2, in6, in4     \
	VPMULUDQ  in3, in5, in3     \
	VPMULUDQ  in4, in5, in4     \
	VPADDQ    in1, in3, in1     \
	VPADDQ    in2, in4, in2     \
	VMOVSHDUP in1, in7, in2     \
	VPSUBD    in5, in2, in4     \
	VPMINUD   in2, in4, in0     \

	VMOVDQU32     0(R15), Z12
	FROMMONTGOMERY(Z12, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z12, Y13
	VPMOVZXWD     Y12, Z12
	VPMOVZXWD     Y13, Z13
	VMOVDQU32     0(CX), Z10
	VMOVDQU32     64(CX), Z8
	MULD(Z12, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z13, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQU32     0(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     0(R9), Z10
	VMOVDQU32     64(R9), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	BUTTERFLYD2Q(Z12, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z13, Z9, Z0, Z2)
	VMOVDQU32     0(SI), Z10
	VMOVDQU32     64(SI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQA32     Z11, 0(DI)
	VMOVDQA32     Z9, 64(DI)
	VMOVDQU32     64(R15), Z14
	FROMMONTGOMERY(Z14, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z14, Y15
	VPMOVZXWD     Y14, Z14
	VPMOVZXWD     Y15, Z15
	VMOVDQU32     128(CX), Z10
	VMOVDQU32     192(CX), Z8
	MULD(Z14, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z15, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQU32     64(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     128(R9), Z10
	VMOVDQU32     192(R9), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	BUTTERFLYD2Q(Z14, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z15, Z9, Z0, Z2)
	VMOVDQU32     128(SI), Z10
	VMOVDQU32     192(SI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQA32     Z11, 128(DI)
	VMOVDQA32     Z9, 192(DI)
	VMOVDQU32     128(R15), Z16
	FROMMONTGOMERY(Z16, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z16, Y17
	VPMOVZXWD     Y16, Z16
	VPMOVZXWD     Y17, Z17
	VMOVDQU32     256(CX), Z10
	VMOVDQU32     320(CX), Z8
	MULD(Z16, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z17, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQU32     128(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     256(R9), Z10
	VMOVDQU32     320(R9), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	BUTTERFLYD2Q(Z16, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z17, Z9, Z0, Z2)
	VMOVDQU32     256(SI), Z10
	VMOVDQU32     320(SI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQA32     Z11, 256(DI)
	VMOVDQA32     Z9, 320(DI)
	VMOVDQU32     192(R15), Z18
	FROMMONTGOMERY(Z18, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z18, Y19
	VPMOVZXWD     Y18, Z18
	VPMOVZXWD     Y19, Z19
	VMOVDQU32     384(CX), Z10
	VMOVDQU32     448(CX), Z8
	MULD(Z18, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z19, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQU32     192(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     384(R9), Z10
	VMOVDQU32     448(R9), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	BUTTERFLYD2Q(Z18, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z19, Z9, Z0, Z2)
	VMOVDQU32     384(SI), Z10
	VMOVDQU32     448(SI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQA32     Z11, 384(DI)
	VMOVDQA32     Z9, 448(DI)
	VMOVDQU32     256(R15), Z20
	FROMMONTGOMERY(Z20, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z20, Y21
	VPMOVZXWD     Y20, Z20
	VPMOVZXWD     Y21, Z21
	VMOVDQU32     512(CX), Z10
	VMOVDQU32     576(CX), Z8
	MULD(Z20, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z21, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQU32     256(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     512(R9), Z10
	VMOVDQU32     576(R9), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	BUTTERFLYD2Q(Z20, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z21, Z9, Z0, Z2)
	VMOVDQU32     512(SI), Z10
	VMOVDQU32     576(SI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQA32     Z11, 512(DI)
	VMOVDQA32     Z9, 576(DI)
	VMOVDQU32     320(R15), Z22
	FROMMONTGOMERY(Z22, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z22, Y23
	VPMOVZXWD     Y22, Z22
	VPMOVZXWD     Y23, Z23
	VMOVDQU32     640(CX), Z10
	VMOVDQU32     704(CX), Z8
	MULD(Z22, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z23, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQU32     320(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     640(R9), Z10
	VMOVDQU32     704(R9), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	BUTTERFLYD2Q(Z22, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z23, Z9, Z0, Z2)
	VMOVDQU32     640(SI), Z10
	VMOVDQU32     704(SI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQA32     Z11, 640(DI)
	VMOVDQA32     Z9, 704(DI)
	VMOVDQU32     384(R15), Z24
	FROMMONTGOMERY(Z24, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z24, Y25
	VPMOVZXWD     Y24, Z24
	VPMOVZXWD     Y25, Z25
	VMOVDQU32     768(CX), Z10
	VMOVDQU32     832(CX), Z8
	MULD(Z24, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z25, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQU32     384(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     768(R9), Z10
	VMOVDQU32     832(R9), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	BUTTERFLYD2Q(Z24, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z25, Z9, Z0, Z2)
	VMOVDQU32     768(SI), Z10
	VMOVDQU32     832(SI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQA32     Z11, 768(DI)
	VMOVDQA32     Z9, 832(DI)
	VMOVDQU32     448(R15), Z26
	FROMMONTGOMERY(Z26, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z26, Y27
	VPMOVZXWD     Y26, Z26
	VPMOVZXWD     Y27, Z27
	VMOVDQU32     896(CX), Z10
	VMOVDQU32     960(CX), Z8
	MULD(Z26, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z27, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQU32     448(DX), Z11
	FROMMONTGOMERY(Z11, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VEXTRACTI64X4 $1, Z11, Y9
	VPMOVZXWD     Y11, Z11
	VPMOVZXWD     Y9, Z9
	VMOVDQU32     896(R9), Z10
	VMOVDQU32     960(R9), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	BUTTERFLYD2Q(Z26, Z11, Z0, Z2)
	BUTTERFLYD2Q(Z27, Z9, Z0, Z2)
	VMOVDQU32     896(SI), Z10
	VMOVDQU32     960(SI), Z8
	MULD(Z11, Z10, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	MULD(Z9, Z8, Z4, Z5, Z2, Z3, Z6, Z7, Z0, Z1, K3)
	VMOVDQA32     Z11, 896(DI)
	VMOVDQA32     Z9, 960(DI)
	ADDQ          $24, BX
	MOVQ          BX, SI
	MOVQ          $2, R10

fft256_8:
	MOVQ         0(BX), R11
	VMOVDQU32    0(R11), Z28
	VMOVDQU32    64(R11), Z29
	VMOVDQU32    128(R11), Z30
	VMOVDQU32    192(R11), Z31
	VMOVDQU32    256(R11), Z2
	VMOVDQU32    320(R11), Z3
	VMOVDQU32    384(R11), Z4
	VMOVDQU32    448(R11), Z5
	BUTTERFLYD2Q(Z12, Z20, Z0, Z6)
	MULD(Z20, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z13, Z21, Z0, Z6)
	MULD(Z21, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z14, Z22, Z0, Z6)
	MULD(Z22, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z15, Z23, Z0, Z6)
	MULD(Z23, Z31, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z16, Z24, Z0, Z6)
	MULD(Z24, Z2, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z17, Z25, Z0, Z6)
	MULD(Z25, Z3, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z18, Z26, Z0, Z6)
	MULD(Z26, Z4, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z19, Z27, Z0, Z6)
	MULD(Z27, Z5, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	ADDQ         $24, BX
	MOVQ         0(BX), R11
	VMOVDQU32    0(R11), Z28
	VMOVDQU32    64(R11), Z29
	VMOVDQU32    128(R11), Z30
	VMOVDQU32    192(R11), Z31
	BUTTERFLYD2Q(Z12, Z16, Z0, Z6)
	MULD(Z16, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z13, Z17, Z0, Z6)
	MULD(Z17, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z14, Z18, Z0, Z6)
	MULD(Z18, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z15, Z19, Z0, Z6)
	MULD(Z19, Z31, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z20, Z24, Z0, Z6)
	MULD(Z24, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z21, Z25, Z0, Z6)
	MULD(Z25, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z22, Z26, Z0, Z6)
	MULD(Z26, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z23, Z27, Z0, Z6)
	MULD(Z27, Z31, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	ADDQ         $24, BX
	MOVQ         0(BX), R11
	VMOVDQU32    0(R11), Z28
	VMOVDQU32    64(R11), Z29
	BUTTERFLYD2Q(Z12, Z14, Z0, Z6)
	MULD(Z14, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z13, Z15, Z0, Z6)
	MULD(Z15, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z16, Z18, Z0, Z6)
	MULD(Z18, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z17, Z19, Z0, Z6)
	MULD(Z19, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z20, Z22, Z0, Z6)
	MULD(Z22, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z21, Z23, Z0, Z6)
	MULD(Z23, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z24, Z26, Z0, Z6)
	MULD(Z26, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z25, Z27, Z0, Z6)
	MULD(Z27, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	ADDQ         $24, BX
	MOVQ         0(BX), R11
	VMOVDQU32    0(R11), Z28
	BUTTERFLYD2Q(Z12, Z13, Z0, Z6)
	MULD(Z13, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z14, Z15, Z0, Z6)
	MULD(Z15, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z16, Z17, Z0, Z6)
	MULD(Z17, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z18, Z19, Z0, Z6)
	MULD(Z19, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z20, Z21, Z0, Z6)
	MULD(Z21, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z22, Z23, Z0, Z6)
	MULD(Z23, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z24, Z25, Z0, Z6)
	MULD(Z25, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	BUTTERFLYD2Q(Z26, Z27, Z0, Z6)
	MULD(Z27, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	ADDQ         $24, BX
	MOVQ         ·vInterleaveIndices+0(SB), R12
	VMOVDQU64    0(R12), Z2
	MOVQ         0(BX), R11
	VMOVDQU32    0(R11), Y28
	VINSERTI64X4 $1, Y28, Z28, Z28
	MOVQ         24(BX), R11
	VMOVDQU32    0(R11), X29
	VINSERTI64X2 $1, X29, Z29, Z29
	VINSERTI64X2 $0x0000000000000002, X29, Z29, Z29
	VINSERTI64X2 $0x0000000000000003, X29, Z29, Z29
	MOVQ         48(BX), R11
	VPBROADCASTD 0(R11), Z30
	VPBROADCASTD 4(R11), Z31
	VPBLENDMD    Z30, Z31, K3, Z30
	PERMUTE8X8(Z12, Z13, Z6, K1)
	BUTTERFLYD2Q(Z12, Z13, Z0, Z6)
	MULD(Z13, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE8X8(Z14, Z15, Z6, K1)
	BUTTERFLYD2Q(Z14, Z15, Z0, Z6)
	MULD(Z15, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE8X8(Z16, Z17, Z6, K1)
	BUTTERFLYD2Q(Z16, Z17, Z0, Z6)
	MULD(Z17, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE8X8(Z18, Z19, Z6, K1)
	BUTTERFLYD2Q(Z18, Z19, Z0, Z6)
	MULD(Z19, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE8X8(Z20, Z21, Z6, K1)
	BUTTERFLYD2Q(Z20, Z21, Z0, Z6)
	MULD(Z21, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE8X8(Z22, Z23, Z6, K1)
	BUTTERFLYD2Q(Z22, Z23, Z0, Z6)
	MULD(Z23, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE8X8(Z24, Z25, Z6, K1)
	BUTTERFLYD2Q(Z24, Z25, Z0, Z6)
	MULD(Z25, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE8X8(Z26, Z27, Z6, K1)
	BUTTERFLYD2Q(Z26, Z27, Z0, Z6)
	MULD(Z27, Z28, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE4X4(Z12, Z13, Z2, Z6, K2)
	BUTTERFLYD2Q(Z12, Z13, Z0, Z6)
	MULD(Z13, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE4X4(Z14, Z15, Z2, Z6, K2)
	BUTTERFLYD2Q(Z14, Z15, Z0, Z6)
	MULD(Z15, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE4X4(Z16, Z17, Z2, Z6, K2)
	BUTTERFLYD2Q(Z16, Z17, Z0, Z6)
	MULD(Z17, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE4X4(Z18, Z19, Z2, Z6, K2)
	BUTTERFLYD2Q(Z18, Z19, Z0, Z6)
	MULD(Z19, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE4X4(Z20, Z21, Z2, Z6, K2)
	BUTTERFLYD2Q(Z20, Z21, Z0, Z6)
	MULD(Z21, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE4X4(Z22, Z23, Z2, Z6, K2)
	BUTTERFLYD2Q(Z22, Z23, Z0, Z6)
	MULD(Z23, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE4X4(Z24, Z25, Z2, Z6, K2)
	BUTTERFLYD2Q(Z24, Z25, Z0, Z6)
	MULD(Z25, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE4X4(Z26, Z27, Z2, Z6, K2)
	BUTTERFLYD2Q(Z26, Z27, Z0, Z6)
	MULD(Z27, Z29, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE2X2(Z12, Z13, Z6, K3)
	BUTTERFLYD2Q(Z12, Z13, Z0, Z6)
	MULD(Z13, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE2X2(Z14, Z15, Z6, K3)
	BUTTERFLYD2Q(Z14, Z15, Z0, Z6)
	MULD(Z15, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE2X2(Z16, Z17, Z6, K3)
	BUTTERFLYD2Q(Z16, Z17, Z0, Z6)
	MULD(Z17, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE2X2(Z18, Z19, Z6, K3)
	BUTTERFLYD2Q(Z18, Z19, Z0, Z6)
	MULD(Z19, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE2X2(Z20, Z21, Z6, K3)
	BUTTERFLYD2Q(Z20, Z21, Z0, Z6)
	MULD(Z21, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE2X2(Z22, Z23, Z6, K3)
	BUTTERFLYD2Q(Z22, Z23, Z0, Z6)
	MULD(Z23, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE2X2(Z24, Z25, Z6, K3)
	BUTTERFLYD2Q(Z24, Z25, Z0, Z6)
	MULD(Z25, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE2X2(Z26, Z27, Z6, K3)
	BUTTERFLYD2Q(Z26, Z27, Z0, Z6)
	MULD(Z27, Z30, Z8, Z9, Z6, Z7, Z10, Z11, Z0, Z1, K3)
	PERMUTE1X1(Z12, Z13, Z6, K3)
	BUTTERFLYD1Q(Z12, Z13, Z0, Z6, Z7)
	PERMUTE1X1(Z14, Z15, Z6, K3)
	BUTTERFLYD1Q(Z14, Z15, Z0, Z6, Z7)
	PERMUTE1X1(Z16, Z17, Z6, K3)
	BUTTERFLYD1Q(Z16, Z17, Z0, Z6, Z7)
	PERMUTE1X1(Z18, Z19, Z6, K3)
	BUTTERFLYD1Q(Z18, Z19, Z0, Z6, Z7)
	PERMUTE1X1(Z20, Z21, Z6, K3)
	BUTTERFLYD1Q(Z20, Z21, Z0, Z6, Z7)
	PERMUTE1X1(Z22, Z23, Z6, K3)
	BUTTERFLYD1Q(Z22, Z23, Z0, Z6, Z7)
	PERMUTE1X1(Z24, Z25, Z6, K3)
	BUTTERFLYD1Q(Z24, Z25, Z0, Z6, Z7)
	PERMUTE1X1(Z26, Z27, Z6, K3)
	BUTTERFLYD1Q(Z26, Z27, Z0, Z6, Z7)
	VPUNPCKLDQ   Z13, Z12, Z3
	VPUNPCKHDQ   Z13, Z12, Z13
	VMOVDQA32    Z3, Z12
	PERMUTE4X4(Z12, Z13, Z2, Z3, K2)
	PERMUTE8X8(Z12, Z13, Z3, K1)
	VPUNPCKLDQ   Z15, Z14, Z3
	VPUNPCKHDQ   Z15, Z14, Z15
	VMOVDQA32    Z3, Z14
	PERMUTE4X4(Z14, Z15, Z2, Z3, K2)
	PERMUTE8X8(Z14, Z15, Z3, K1)
	VPUNPCKLDQ   Z17, Z16, Z3
	VPUNPCKHDQ   Z17, Z16, Z17
	VMOVDQA32    Z3, Z16
	PERMUTE4X4(Z16, Z17, Z2, Z3, K2)
	PERMUTE8X8(Z16, Z17, Z3, K1)
	VPUNPCKLDQ   Z19, Z18, Z3
	VPUNPCKHDQ   Z19, Z18, Z19
	VMOVDQA32    Z3, Z18
	PERMUTE4X4(Z18, Z19, Z2, Z3, K2)
	PERMUTE8X8(Z18, Z19, Z3, K1)
	VPUNPCKLDQ   Z21, Z20, Z3
	VPUNPCKHDQ   Z21, Z20, Z21
	VMOVDQA32    Z3, Z20
	PERMUTE4X4(Z20, Z21, Z2, Z3, K2)
	PERMUTE8X8(Z20, Z21, Z3, K1)
	VPUNPCKLDQ   Z23, Z22, Z3
	VPUNPCKHDQ   Z23, Z22, Z23
	VMOVDQA32    Z3, Z22
	PERMUTE4X4(Z22, Z23, Z2, Z3, K2)
	PERMUTE8X8(Z22, Z23, Z3, K1)
	VPUNPCKLDQ   Z25, Z24, Z3
	VPUNPCKHDQ   Z25, Z24, Z25
	VMOVDQA32    Z3, Z24
	PERMUTE4X4(Z24, Z25, Z2, Z3, K2)
	PERMUTE8X8(Z24, Z25, Z3, K1)
	VPUNPCKLDQ   Z27, Z26, Z3
	VPUNPCKHDQ   Z27, Z26, Z27
	VMOVDQA32    Z3, Z26
	PERMUTE4X4(Z26, Z27, Z2, Z3, K2)
	PERMUTE8X8(Z26, Z27, Z3, K1)
	VMOVDQU32    0(R14), Z31
	VMOVDQU32    64(R14), Z6
	MULD(Z12, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	MULD(Z13, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	VMOVDQU32    128(R14), Z31
	VMOVDQU32    192(R14), Z6
	MULD(Z14, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	MULD(Z15, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	VMOVDQU32    256(R14), Z31
	VMOVDQU32    320(R14), Z6
	MULD(Z16, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	MULD(Z17, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	VMOVDQU32    384(R14), Z31
	VMOVDQU32    448(R14), Z6
	MULD(Z18, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	MULD(Z19, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	VMOVDQU32    512(R14), Z31
	VMOVDQU32    576(R14), Z6
	MULD(Z20, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	MULD(Z21, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	VMOVDQU32    640(R14), Z31
	VMOVDQU32    704(R14), Z6
	MULD(Z22, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	MULD(Z23, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	VMOVDQU32    768(R14), Z31
	VMOVDQU32    832(R14), Z6
	MULD(Z24, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	MULD(Z25, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	VMOVDQU32    896(R14), Z31
	VMOVDQU32    960(R14), Z6
	MULD(Z26, Z31, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	MULD(Z27, Z6, Z5, Z28, Z3, Z4, Z29, Z30, Z0, Z1, K3)
	VMOVDQU32    0(R8), Z31
	VMOVDQU32    64(R8), Z6
	VPADDD       Z31, Z12, Z12
	VPSUBD       Z0, Z12, Z29
	VPMINUD      Z29, Z12, Z12
	VPADDD       Z6, Z13, Z13
	VPSUBD       Z0, Z13, Z30
	VPMINUD      Z30, Z13, Z13
	VMOVDQU32    Z12, 0(R8)
	VMOVDQU32    Z13, 64(R8)
	VMOVDQU32    128(R8), Z31
	VMOVDQU32    192(R8), Z6
	VPADDD       Z31, Z14, Z14
	VPSUBD       Z0, Z14, Z29
	VPMINUD      Z29, Z14, Z14
	VPADDD       Z6, Z15, Z15
	VPSUBD       Z0, Z15, Z30
	VPMINUD      Z30, Z15, Z15
	VMOVDQU32    Z14, 128(R8)
	VMOVDQU32    Z15, 192(R8)
	VMOVDQU32    256(R8), Z31
	VMOVDQU32    320(R8), Z6
	VPADDD       Z31, Z16, Z16
	VPSUBD       Z0, Z16, Z29
	VPMINUD      Z29, Z16, Z16
	VPADDD       Z6, Z17, Z17
	VPSUBD       Z0, Z17, Z30
	VPMINUD      Z30, Z17, Z17
	VMOVDQU32    Z16, 256(R8)
	VMOVDQU32    Z17, 320(R8)
	VMOVDQU32    384(R8), Z31
	VMOVDQU32    448(R8), Z6
	VPADDD       Z31, Z18, Z18
	VPSUBD       Z0, Z18, Z29
	VPMINUD      Z29, Z18, Z18
	VPADDD       Z6, Z19, Z19
	VPSUBD       Z0, Z19, Z30
	VPMINUD      Z30, Z19, Z19
	VMOVDQU32    Z18, 384(R8)
	VMOVDQU32    Z19, 448(R8)
	VMOVDQU32    512(R8), Z31
	VMOVDQU32    576(R8), Z6
	VPADDD       Z31, Z20, Z20
	VPSUBD       Z0, Z20, Z29
	VPMINUD      Z29, Z20, Z20
	VPADDD       Z6, Z21, Z21
	VPSUBD       Z0, Z21, Z30
	VPMINUD      Z30, Z21, Z21
	VMOVDQU32    Z20, 512(R8)
	VMOVDQU32    Z21, 576(R8)
	VMOVDQU32    640(R8), Z31
	VMOVDQU32    704(R8), Z6
	VPADDD       Z31, Z22, Z22
	VPSUBD       Z0, Z22, Z29
	VPMINUD      Z29, Z22, Z22
	VPADDD       Z6, Z23, Z23
	VPSUBD       Z0, Z23, Z30
	VPMINUD      Z30, Z23, Z23
	VMOVDQU32    Z22, 640(R8)
	VMOVDQU32    Z23, 704(R8)
	VMOVDQU32    768(R8), Z31
	VMOVDQU32    832(R8), Z6
	VPADDD       Z31, Z24, Z24
	VPSUBD       Z0, Z24, Z29
	VPMINUD      Z29, Z24, Z24
	VPADDD       Z6, Z25, Z25
	VPSUBD       Z0, Z25, Z30
	VPMINUD      Z30, Z25, Z25
	VMOVDQU32    Z24, 768(R8)
	VMOVDQU32    Z25, 832(R8)
	VMOVDQU32    896(R8), Z31
	VMOVDQU32    960(R8), Z6
	VPADDD       Z31, Z26, Z26
	VPSUBD       Z0, Z26, Z29
	VPMINUD      Z29, Z26, Z26
	VPADDD       Z6, Z27, Z27
	VPSUBD       Z0, Z27, Z30
	VPMINUD      Z30, Z27, Z27
	VMOVDQU32    Z26, 896(R8)
	VMOVDQU32    Z27, 960(R8)
	DECQ         R10
	TESTQ        R10, R10
	JEQ          done_7
	MOVQ         SI, BX
	VMOVDQU32    0(DI), Z12
	VMOVDQU32    64(DI), Z13
	VMOVDQU32    128(DI), Z14
	VMOVDQU32    192(DI), Z15
	VMOVDQU32    256(DI), Z16
	VMOVDQU32    320(DI), Z17
	VMOVDQU32    384(DI), Z18
	VMOVDQU32    448(DI), Z19
	VMOVDQU32    512(DI), Z20
	VMOVDQU32    576(DI), Z21
	VMOVDQU32    640(DI), Z22
	VMOVDQU32    704(DI), Z23
	VMOVDQU32    768(DI), Z24
	VMOVDQU32    832(DI), Z25
	VMOVDQU32    896(DI), Z26
	VMOVDQU32    960(DI), Z27
	ADDQ         $0x0000000000000400, R14
	ADDQ         $0x0000000000000400, R8
	JMP          fft256_8

done_7:
	RET
