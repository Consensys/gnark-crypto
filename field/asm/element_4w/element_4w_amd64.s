// Code generated by gnark-crypto/generator. DO NOT EDIT.
#include "textflag.h"
#include "funcdata.h"
#include "go_asm.h"

#define REDUCE(ra0, ra1, ra2, ra3, rb0, rb1, rb2, rb3, q0, q1, q2, q3) \
	MOVQ    ra0, rb0; \
	SUBQ    q0, ra0;  \
	MOVQ    ra1, rb1; \
	SBBQ    q1, ra1;  \
	MOVQ    ra2, rb2; \
	SBBQ    q2, ra2;  \
	MOVQ    ra3, rb3; \
	SBBQ    q3, ra3;  \
	CMOVQCS rb0, ra0; \
	CMOVQCS rb1, ra1; \
	CMOVQCS rb2, ra2; \
	CMOVQCS rb3, ra3; \

TEXT ·reduce(SB), NOSPLIT, $0-8
	MOVQ res+0(FP), AX
	MOVQ 0(AX), DX
	MOVQ 8(AX), CX
	MOVQ 16(AX), BX
	MOVQ 24(AX), SI

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ DX, 0(AX)
	MOVQ CX, 8(AX)
	MOVQ BX, 16(AX)
	MOVQ SI, 24(AX)
	RET

// MulBy3(x *Element)
TEXT ·MulBy3(SB), NOSPLIT, $0-8
	MOVQ x+0(FP), AX
	MOVQ 0(AX), DX
	MOVQ 8(AX), CX
	MOVQ 16(AX), BX
	MOVQ 24(AX), SI
	ADDQ DX, DX
	ADCQ CX, CX
	ADCQ BX, BX
	ADCQ SI, SI

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	ADDQ 0(AX), DX
	ADCQ 8(AX), CX
	ADCQ 16(AX), BX
	ADCQ 24(AX), SI

	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ DX, 0(AX)
	MOVQ CX, 8(AX)
	MOVQ BX, 16(AX)
	MOVQ SI, 24(AX)
	RET

// MulBy5(x *Element)
TEXT ·MulBy5(SB), NOSPLIT, $0-8
	MOVQ x+0(FP), AX
	MOVQ 0(AX), DX
	MOVQ 8(AX), CX
	MOVQ 16(AX), BX
	MOVQ 24(AX), SI
	ADDQ DX, DX
	ADCQ CX, CX
	ADCQ BX, BX
	ADCQ SI, SI

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	ADDQ DX, DX
	ADCQ CX, CX
	ADCQ BX, BX
	ADCQ SI, SI

	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	ADDQ 0(AX), DX
	ADCQ 8(AX), CX
	ADCQ 16(AX), BX
	ADCQ 24(AX), SI

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ DX, 0(AX)
	MOVQ CX, 8(AX)
	MOVQ BX, 16(AX)
	MOVQ SI, 24(AX)
	RET

// MulBy13(x *Element)
TEXT ·MulBy13(SB), NOSPLIT, $0-8
	MOVQ x+0(FP), AX
	MOVQ 0(AX), DX
	MOVQ 8(AX), CX
	MOVQ 16(AX), BX
	MOVQ 24(AX), SI
	ADDQ DX, DX
	ADCQ CX, CX
	ADCQ BX, BX
	ADCQ SI, SI

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	ADDQ DX, DX
	ADCQ CX, CX
	ADCQ BX, BX
	ADCQ SI, SI

	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ DX, R11
	MOVQ CX, R12
	MOVQ BX, R13
	MOVQ SI, R14
	ADDQ DX, DX
	ADCQ CX, CX
	ADCQ BX, BX
	ADCQ SI, SI

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	ADDQ R11, DX
	ADCQ R12, CX
	ADCQ R13, BX
	ADCQ R14, SI

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	ADDQ 0(AX), DX
	ADCQ 8(AX), CX
	ADCQ 16(AX), BX
	ADCQ 24(AX), SI

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ DX, 0(AX)
	MOVQ CX, 8(AX)
	MOVQ BX, 16(AX)
	MOVQ SI, 24(AX)
	RET

// Butterfly(a, b *Element) sets a = a + b; b = a - b
TEXT ·Butterfly(SB), NOSPLIT, $0-16
	MOVQ    a+0(FP), R15
	MOVQ    0(R15), DX
	MOVQ    8(R15), CX
	MOVQ    16(R15), BX
	MOVQ    24(R15), SI
	MOVQ    DX, DI
	MOVQ    CX, R8
	MOVQ    BX, R9
	MOVQ    SI, R10
	XORQ    R15, R15
	MOVQ    b+8(FP), AX
	ADDQ    0(AX), DX
	ADCQ    8(AX), CX
	ADCQ    16(AX), BX
	ADCQ    24(AX), SI
	SUBQ    0(AX), DI
	SBBQ    8(AX), R8
	SBBQ    16(AX), R9
	SBBQ    24(AX), R10
	MOVQ    $const_q0, R11
	MOVQ    $const_q1, R12
	MOVQ    $const_q2, R13
	MOVQ    $const_q3, R14
	CMOVQCC R15, R11
	CMOVQCC R15, R12
	CMOVQCC R15, R13
	CMOVQCC R15, R14
	ADDQ    R11, DI
	ADCQ    R12, R8
	ADCQ    R13, R9
	ADCQ    R14, R10
	MOVQ    DI, 0(AX)
	MOVQ    R8, 8(AX)
	MOVQ    R9, 16(AX)
	MOVQ    R10, 24(AX)

	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ a+0(FP), R15
	MOVQ DX, 0(R15)
	MOVQ CX, 8(R15)
	MOVQ BX, 16(R15)
	MOVQ SI, 24(R15)
	RET

// mul(res, x, y *Element)
TEXT ·mul(SB), $24-24

	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
	// See github.com/consensys/gnark-crypto/internal/generator/field for more comments.

	NO_LOCAL_POINTERS
	CMPB ·supportAdx(SB), $1
	JNE  noAdx_1
	MOVQ x+8(FP), SI

	// x[0] -> DI
	// x[1] -> R8
	// x[2] -> R9
	// x[3] -> R10
	MOVQ 0(SI), DI
	MOVQ 8(SI), R8
	MOVQ 16(SI), R9
	MOVQ 24(SI), R10
	MOVQ y+16(FP), R11

	// A -> BP
	// t[0] -> R14
	// t[1] -> R13
	// t[2] -> CX
	// t[3] -> BX
#define MACC(in0, in1, in2) \
	ADCXQ in0, in1     \
	MULXQ in2, AX, in0 \
	ADOXQ AX, in1      \

#define DIV_SHIFT() \
	MOVQ  $const_qInvNeg, DX        \
	IMULQ R14, DX                   \
	XORQ  AX, AX                    \
	MULXQ ·qElement+0(SB), AX, R12  \
	ADCXQ R14, AX                   \
	MOVQ  R12, R14                  \
	MACC(R13, R14, ·qElement+8(SB)) \
	MACC(CX, R13, ·qElement+16(SB)) \
	MACC(BX, CX, ·qElement+24(SB))  \
	MOVQ  $0, AX                    \
	ADCXQ AX, BX                    \
	ADOXQ BP, BX                    \

#define MUL_WORD_0() \
	XORQ  AX, AX       \
	MULXQ DI, R14, R13 \
	MULXQ R8, AX, CX   \
	ADOXQ AX, R13      \
	MULXQ R9, AX, BX   \
	ADOXQ AX, CX       \
	MULXQ R10, AX, BP  \
	ADOXQ AX, BX       \
	MOVQ  $0, AX       \
	ADOXQ AX, BP       \
	DIV_SHIFT()        \

#define MUL_WORD_N() \
	XORQ  AX, AX      \
	MULXQ DI, AX, BP  \
	ADOXQ AX, R14     \
	MACC(BP, R13, R8) \
	MACC(BP, CX, R9)  \
	MACC(BP, BX, R10) \
	MOVQ  $0, AX      \
	ADCXQ AX, BP      \
	ADOXQ AX, BP      \
	DIV_SHIFT()       \

	// mul body
	MOVQ 0(R11), DX
	MUL_WORD_0()
	MOVQ 8(R11), DX
	MUL_WORD_N()
	MOVQ 16(R11), DX
	MUL_WORD_N()
	MOVQ 24(R11), DX
	MUL_WORD_N()

	// reduce element(R14,R13,CX,BX) using temp registers (SI,R12,R11,DI)
	REDUCE(R14,R13,CX,BX,SI,R12,R11,DI,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ res+0(FP), AX
	MOVQ R14, 0(AX)
	MOVQ R13, 8(AX)
	MOVQ CX, 16(AX)
	MOVQ BX, 24(AX)
	RET

noAdx_1:
	MOVQ res+0(FP), AX
	MOVQ AX, (SP)
	MOVQ x+8(FP), AX
	MOVQ AX, 8(SP)
	MOVQ y+16(FP), AX
	MOVQ AX, 16(SP)
	CALL ·_mulGeneric(SB)
	RET

TEXT ·fromMont(SB), $8-8
	NO_LOCAL_POINTERS

	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
	// when y = 1 we have:
	// for i=0 to N-1
	// 		t[i] = x[i]
	// for i=0 to N-1
	// 		m := t[0]*q'[0] mod W
	// 		C,_ := t[0] + m*q[0]
	// 		for j=1 to N-1
	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
	// 		t[N-1] = C
	CMPB ·supportAdx(SB), $1
	JNE  noAdx_2
	MOVQ res+0(FP), DX
	MOVQ 0(DX), R13
	MOVQ 8(DX), R14
	MOVQ 16(DX), CX
	MOVQ 24(DX), BX

#define FROMMONT_STEP() \
	XORQ  DX, DX                   \
	MOVQ  $const_qInvNeg, DX       \
	IMULQ R13, DX                  \
	XORQ  AX, AX                   \
	MULXQ ·qElement+0(SB), AX, BP  \
	ADCXQ R13, AX                  \
	MOVQ  BP, R13                  \
	ADCXQ R14, R13                 \
	MULXQ ·qElement+8(SB), AX, R14 \
	ADOXQ AX, R13                  \
	ADCXQ CX, R14                  \
	MULXQ ·qElement+16(SB), AX, CX \
	ADOXQ AX, R14                  \
	ADCXQ BX, CX                   \
	MULXQ ·qElement+24(SB), AX, BX \
	ADOXQ AX, CX                   \
	MOVQ  $0, AX                   \
	ADCXQ AX, BX                   \
	ADOXQ AX, BX                   \

	FROMMONT_STEP()
	FROMMONT_STEP()
	FROMMONT_STEP()
	FROMMONT_STEP()

	// reduce element(R13,R14,CX,BX) using temp registers (SI,DI,R8,R9)
	REDUCE(R13,R14,CX,BX,SI,DI,R8,R9,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ res+0(FP), AX
	MOVQ R13, 0(AX)
	MOVQ R14, 8(AX)
	MOVQ CX, 16(AX)
	MOVQ BX, 24(AX)
	RET

noAdx_2:
	MOVQ res+0(FP), AX
	MOVQ AX, (SP)
	CALL ·_fromMontGeneric(SB)
	RET

// Vector operations are partially derived from Dag Arne Osvik's work in github.com/a16z/vectorized-fields

// addVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] + b[0...n]
TEXT ·addVec(SB), NOSPLIT, $0-32
	MOVQ res+0(FP), CX
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), DX
	MOVQ n+24(FP), BX

loop_3:
	TESTQ BX, BX
	JEQ   done_4 // n == 0, we are done

	// a[0] -> SI
	// a[1] -> DI
	// a[2] -> R8
	// a[3] -> R9
	MOVQ       0(AX), SI
	MOVQ       8(AX), DI
	MOVQ       16(AX), R8
	MOVQ       24(AX), R9
	ADDQ       0(DX), SI
	ADCQ       8(DX), DI
	ADCQ       16(DX), R8
	ADCQ       24(DX), R9
	PREFETCHT0 2048(AX)
	PREFETCHT0 2048(DX)

	// reduce element(SI,DI,R8,R9) using temp registers (R10,R11,R12,R13)
	REDUCE(SI,DI,R8,R9,R10,R11,R12,R13,·qElement+0(SB),·qElement+8(SB),·qElement+16(SB),·qElement+24(SB))

	MOVQ SI, 0(CX)
	MOVQ DI, 8(CX)
	MOVQ R8, 16(CX)
	MOVQ R9, 24(CX)

	// increment pointers to visit next element
	ADDQ $32, AX
	ADDQ $32, DX
	ADDQ $32, CX
	DECQ BX      // decrement n
	JMP  loop_3

done_4:
	RET

// subVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] - b[0...n]
TEXT ·subVec(SB), NOSPLIT, $0-32
	MOVQ res+0(FP), CX
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), DX
	MOVQ n+24(FP), BX
	XORQ SI, SI

loop_5:
	TESTQ BX, BX
	JEQ   done_6 // n == 0, we are done

	// a[0] -> DI
	// a[1] -> R8
	// a[2] -> R9
	// a[3] -> R10
	MOVQ       0(AX), DI
	MOVQ       8(AX), R8
	MOVQ       16(AX), R9
	MOVQ       24(AX), R10
	SUBQ       0(DX), DI
	SBBQ       8(DX), R8
	SBBQ       16(DX), R9
	SBBQ       24(DX), R10
	PREFETCHT0 2048(AX)
	PREFETCHT0 2048(DX)

	// reduce (a-b) mod q
	// q[0] -> R11
	// q[1] -> R12
	// q[2] -> R13
	// q[3] -> R14
	MOVQ    $const_q0, R11
	MOVQ    $const_q1, R12
	MOVQ    $const_q2, R13
	MOVQ    $const_q3, R14
	CMOVQCC SI, R11
	CMOVQCC SI, R12
	CMOVQCC SI, R13
	CMOVQCC SI, R14

	// add registers (q or 0) to a, and set to result
	ADDQ R11, DI
	ADCQ R12, R8
	ADCQ R13, R9
	ADCQ R14, R10
	MOVQ DI, 0(CX)
	MOVQ R8, 8(CX)
	MOVQ R9, 16(CX)
	MOVQ R10, 24(CX)

	// increment pointers to visit next element
	ADDQ $32, AX
	ADDQ $32, DX
	ADDQ $32, CX
	DECQ BX      // decrement n
	JMP  loop_5

done_6:
	RET

// sumVec(t *[8]uint64, a *Element, n uint64) t = raw accumulators
// Accumulates elements into 8 qword accumulators for reduction in Go
//
// When we move an element into a Z register using VPMOVZXDQ,
// each 32-bit dword is zero-extended to a 64-bit qword.
// We can safely add up to 2^32 elements without overflow.
TEXT ·sumVec(SB), NOSPLIT, $0-24
	MOVQ t+0(FP), R14
	MOVQ a+8(FP), R13
	MOVQ n+16(FP), CX

	// initialize accumulators
	VXORPS    Z0, Z0, Z0
	VMOVDQA64 Z0, Z1
	VMOVDQA64 Z0, Z2
	VMOVDQA64 Z0, Z3
	VMOVDQA64 Z0, Z4
	VMOVDQA64 Z0, Z5
	VMOVDQA64 Z0, Z6
	VMOVDQA64 Z0, Z7

	// n % 8 -> BX
	// n / 8 -> CX
	MOVQ CX, BX
	ANDQ $7, BX
	SHRQ $3, CX

loop_7:
	TESTQ     BX, BX
	JEQ       done_8
	DECQ      BX
	VPMOVZXDQ 0(R13), Z8
	VPADDQ    Z8, Z0, Z0
	ADDQ      $32, R13
	JMP       loop_7

done_8:
loop_9:
	TESTQ      CX, CX
	JEQ        done_10
	DECQ       CX
	VPMOVZXDQ  0(R13), Z8
	VPMOVZXDQ  32(R13), Z9
	VPMOVZXDQ  64(R13), Z10
	VPMOVZXDQ  96(R13), Z11
	VPMOVZXDQ  128(R13), Z12
	VPMOVZXDQ  160(R13), Z13
	VPMOVZXDQ  192(R13), Z14
	VPMOVZXDQ  224(R13), Z15
	PREFETCHT0 4096(R13)
	VPADDQ     Z8, Z0, Z0
	VPADDQ     Z9, Z1, Z1
	VPADDQ     Z10, Z2, Z2
	VPADDQ     Z11, Z3, Z3
	VPADDQ     Z12, Z4, Z4
	VPADDQ     Z13, Z5, Z5
	VPADDQ     Z14, Z6, Z6
	VPADDQ     Z15, Z7, Z7
	ADDQ       $256, R13     // increment pointer by 8 elements
	JMP        loop_9

done_10:
	// accumulate the 8 Z registers into Z0
	VPADDQ Z7, Z6, Z6
	VPADDQ Z6, Z5, Z5
	VPADDQ Z5, Z4, Z4
	VPADDQ Z4, Z3, Z3
	VPADDQ Z3, Z2, Z2
	VPADDQ Z2, Z1, Z1
	VPADDQ Z1, Z0, Z0

	// store the 8 qwords to the output buffer
	VMOVDQU64 Z0, 0(R14)
	RET

// innerProdVec(res, a,b *Element, n uint64) res = sum(a[0...n] * b[0...n])
TEXT ·innerProdVec(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), R13
	MOVQ b+16(FP), R14
	MOVQ n+24(FP), CX

	// Create mask for low dword in each qword
	VPCMPEQB  Y0, Y0, Y0
	VPMOVZXDQ Y0, Z5
	VPXORQ    Z16, Z16, Z16
	VMOVDQA64 Z16, Z17
	VMOVDQA64 Z16, Z18
	VMOVDQA64 Z16, Z19
	VMOVDQA64 Z16, Z20
	VMOVDQA64 Z16, Z21
	VMOVDQA64 Z16, Z22
	VMOVDQA64 Z16, Z23
	VMOVDQA64 Z16, Z24
	VMOVDQA64 Z16, Z25
	VMOVDQA64 Z16, Z26
	VMOVDQA64 Z16, Z27
	VMOVDQA64 Z16, Z28
	VMOVDQA64 Z16, Z29
	VMOVDQA64 Z16, Z30
	VMOVDQA64 Z16, Z31
	TESTQ     CX, CX
	JEQ       done_12       // n == 0, we are done

loop_11:
	TESTQ     CX, CX
	JEQ       accumulate_13 // n == 0 we can accumulate
	VPMOVZXDQ (R14), Z4
	ADDQ      $32, R14

	// we multiply and accumulate partial products of 4 bytes * 32 bytes
#define MAC(in0, in1, in2) \
	VPMULUDQ.BCST in0, Z4, Z2  \
	VPSRLQ        $32, Z2, Z3  \
	VPANDQ        Z5, Z2, Z2   \
	VPADDQ        Z2, in1, in1 \
	VPADDQ        Z3, in2, in2 \

	MAC(0*4(R13), Z16, Z24)
	MAC(1*4(R13), Z17, Z25)
	MAC(2*4(R13), Z18, Z26)
	MAC(3*4(R13), Z19, Z27)
	MAC(4*4(R13), Z20, Z28)
	MAC(5*4(R13), Z21, Z29)
	MAC(6*4(R13), Z22, Z30)
	MAC(7*4(R13), Z23, Z31)
	ADDQ $32, R13
	DECQ CX       // decrement n
	JMP  loop_11

accumulate_13:
	// we accumulate the partial products into 544bits in Z1:Z0
	MOVQ  $0x0000000000001555, AX
	KMOVD AX, K1
	MOVQ  $1, AX
	KMOVD AX, K2

	// store the least significant 32 bits of ACC (starts with A0L) in Z0
	VALIGND.Z $16, Z16, Z16, K2, Z0
	KSHIFTLW  $1, K2, K2
	VPSRLQ    $32, Z16, Z2
	VALIGND.Z $2, Z16, Z16, K1, Z16
	VPADDQ    Z2, Z16, Z16
	VPANDQ    Z5, Z24, Z2
	VPADDQ    Z2, Z16, Z16
	VPANDQ    Z5, Z17, Z2
	VPADDQ    Z2, Z16, Z16
	VALIGND   $15, Z16, Z16, K2, Z0
	KSHIFTLW  $1, K2, K2

	// macro to add partial products and store the result in Z0
#define ADDPP(in0, in1, in2, in3, in4) \
	VPSRLQ    $32, Z16, Z2              \
	VALIGND.Z $2, Z16, Z16, K1, Z16     \
	VPADDQ    Z2, Z16, Z16              \
	VPSRLQ    $32, in0, in0             \
	VPADDQ    in0, Z16, Z16             \
	VPSRLQ    $32, in1, in1             \
	VPADDQ    in1, Z16, Z16             \
	VPANDQ    Z5, in2, Z2               \
	VPADDQ    Z2, Z16, Z16              \
	VPANDQ    Z5, in3, Z2               \
	VPADDQ    Z2, Z16, Z16              \
	VALIGND   $16-in4, Z16, Z16, K2, Z0 \
	KADDW     K2, K2, K2                \

	ADDPP(Z24, Z17, Z25, Z18, 2)
	ADDPP(Z25, Z18, Z26, Z19, 3)
	ADDPP(Z26, Z19, Z27, Z20, 4)
	ADDPP(Z27, Z20, Z28, Z21, 5)
	ADDPP(Z28, Z21, Z29, Z22, 6)
	ADDPP(Z29, Z22, Z30, Z23, 7)
	VPSRLQ    $32, Z16, Z2
	VALIGND.Z $2, Z16, Z16, K1, Z16
	VPADDQ    Z2, Z16, Z16
	VPSRLQ    $32, Z30, Z30
	VPADDQ    Z30, Z16, Z16
	VPSRLQ    $32, Z23, Z23
	VPADDQ    Z23, Z16, Z16
	VPANDQ    Z5, Z31, Z2
	VPADDQ    Z2, Z16, Z16
	VALIGND   $16-8, Z16, Z16, K2, Z0
	KSHIFTLW  $1, K2, K2
	VPSRLQ    $32, Z16, Z2
	VALIGND.Z $2, Z16, Z16, K1, Z16
	VPADDQ    Z2, Z16, Z16
	VPSRLQ    $32, Z31, Z31
	VPADDQ    Z31, Z16, Z16
	VALIGND   $16-9, Z16, Z16, K2, Z0
	KSHIFTLW  $1, K2, K2

#define ADDPP2(in0) \
	VPSRLQ    $32, Z16, Z2              \
	VALIGND.Z $2, Z16, Z16, K1, Z16     \
	VPADDQ    Z2, Z16, Z16              \
	VALIGND   $16-in0, Z16, Z16, K2, Z0 \
	KSHIFTLW  $1, K2, K2                \

	ADDPP2(10)
	ADDPP2(11)
	ADDPP2(12)
	ADDPP2(13)
	ADDPP2(14)
	ADDPP2(15)
	VPSRLQ      $32, Z16, Z2
	VALIGND.Z   $2, Z16, Z16, K1, Z16
	VPADDQ      Z2, Z16, Z16
	VMOVDQA64.Z Z16, K1, Z1

	// Extract the 4 least significant qwords of Z0
	VMOVQ   X0, SI
	VALIGNQ $1, Z0, Z1, Z0
	VMOVQ   X0, DI
	VALIGNQ $1, Z0, Z0, Z0
	VMOVQ   X0, R8
	VALIGNQ $1, Z0, Z0, Z0
	VMOVQ   X0, R9
	VALIGNQ $1, Z0, Z0, Z0
	XORQ    BX, BX
	MOVQ    $const_qInvNeg, DX
	MULXQ   SI, DX, R10
	MULXQ   ·qElement+0(SB), AX, R10
	ADDQ    AX, SI
	ADCQ    R10, DI
	MULXQ   ·qElement+16(SB), AX, R10
	ADCQ    AX, R8
	ADCQ    R10, R9
	ADCQ    $0, BX
	MULXQ   ·qElement+8(SB), AX, R10
	ADDQ    AX, DI
	ADCQ    R10, R8
	MULXQ   ·qElement+24(SB), AX, R10
	ADCQ    AX, R9
	ADCQ    R10, BX
	ADCQ    $0, SI
	MOVQ    $const_qInvNeg, DX
	MULXQ   DI, DX, R10
	MULXQ   ·qElement+0(SB), AX, R10
	ADDQ    AX, DI
	ADCQ    R10, R8
	MULXQ   ·qElement+16(SB), AX, R10
	ADCQ    AX, R9
	ADCQ    R10, BX
	ADCQ    $0, SI
	MULXQ   ·qElement+8(SB), AX, R10
	ADDQ    AX, R8
	ADCQ    R10, R9
	MULXQ   ·qElement+24(SB), AX, R10
	ADCQ    AX, BX
	ADCQ    R10, SI
	ADCQ    $0, DI
	MOVQ    $const_qInvNeg, DX
	MULXQ   R8, DX, R10
	MULXQ   ·qElement+0(SB), AX, R10
	ADDQ    AX, R8
	ADCQ    R10, R9
	MULXQ   ·qElement+16(SB), AX, R10
	ADCQ    AX, BX
	ADCQ    R10, SI
	ADCQ    $0, DI
	MULXQ   ·qElement+8(SB), AX, R10
	ADDQ    AX, R9
	ADCQ    R10, BX
	MULXQ   ·qElement+24(SB), AX, R10
	ADCQ    AX, SI
	ADCQ    R10, DI
	ADCQ    $0, R8
	MOVQ    $const_qInvNeg, DX
	MULXQ   R9, DX, R10
	MULXQ   ·qElement+0(SB), AX, R10
	ADDQ    AX, R9
	ADCQ    R10, BX
	MULXQ   ·qElement+16(SB), AX, R10
	ADCQ    AX, SI
	ADCQ    R10, DI
	ADCQ    $0, R8
	MULXQ   ·qElement+8(SB), AX, R10
	ADDQ    AX, BX
	ADCQ    R10, SI
	MULXQ   ·qElement+24(SB), AX, R10
	ADCQ    AX, DI
	ADCQ    R10, R8
	ADCQ    $0, R9
	VMOVQ   X0, AX
	ADDQ    AX, BX
	VALIGNQ $1, Z0, Z0, Z0
	VMOVQ   X0, AX
	ADCQ    AX, SI
	VALIGNQ $1, Z0, Z0, Z0
	VMOVQ   X0, AX
	ADCQ    AX, DI
	VALIGNQ $1, Z0, Z0, Z0
	VMOVQ   X0, AX
	ADCQ    AX, R8
	VALIGNQ $1, Z0, Z0, Z0
	VMOVQ   X0, AX
	ADCQ    AX, R9

	// Barrett reduction; see Handbook of Applied Cryptography, Algorithm 14.42.
	MOVQ  R8, AX
	SHRQ  $32, R9, AX
	MOVQ  $const_mu, DX
	MULQ  DX
	MULXQ ·qElement+0(SB), AX, R10
	SUBQ  AX, BX
	SBBQ  R10, SI
	MULXQ ·qElement+16(SB), AX, R10
	SBBQ  AX, DI
	SBBQ  R10, R8
	SBBQ  $0, R9
	MULXQ ·qElement+8(SB), AX, R10
	SUBQ  AX, SI
	SBBQ  R10, DI
	MULXQ ·qElement+24(SB), AX, R10
	SBBQ  AX, R8
	SBBQ  R10, R9

	// we need up to 2 conditional substractions to be < q
	MOVQ res+0(FP), R11
	MOVQ BX, 0(R11)
	MOVQ SI, 8(R11)
	MOVQ DI, 16(R11)
	MOVQ R8, 24(R11)
	SUBQ ·qElement+0(SB), BX
	SBBQ ·qElement+8(SB), SI
	SBBQ ·qElement+16(SB), DI
	SBBQ ·qElement+24(SB), R8
	SBBQ $0, R9
	JCS  done_12
	MOVQ BX, 0(R11)
	MOVQ SI, 8(R11)
	MOVQ DI, 16(R11)
	MOVQ R8, 24(R11)
	SUBQ ·qElement+0(SB), BX
	SBBQ ·qElement+8(SB), SI
	SBBQ ·qElement+16(SB), DI
	SBBQ ·qElement+24(SB), R8
	SBBQ $0, R9
	JCS  done_12
	MOVQ BX, 0(R11)
	MOVQ SI, 8(R11)
	MOVQ DI, 16(R11)
	MOVQ R8, 24(R11)

done_12:
	RET

// AVX-512 IFMA vector multiplication (requires Ice Lake+ or Zen4+)

// Permutation index for IFMA transpose: [0, 2, 1, 3, 4, 6, 5, 7]
// This swaps positions 1<->2 and 5<->6 to fix even/odd interleaving
DATA ·permuteIdxIFMA<>+0(SB)/8, $0
DATA ·permuteIdxIFMA<>+8(SB)/8, $2
DATA ·permuteIdxIFMA<>+16(SB)/8, $1
DATA ·permuteIdxIFMA<>+24(SB)/8, $3
DATA ·permuteIdxIFMA<>+32(SB)/8, $4
DATA ·permuteIdxIFMA<>+40(SB)/8, $6
DATA ·permuteIdxIFMA<>+48(SB)/8, $5
DATA ·permuteIdxIFMA<>+56(SB)/8, $7
GLOBL ·permuteIdxIFMA<>(SB), RODATA|NOPTR, $64

#define CARRY_PROP(in0, in1, in2, in3, in4) \
	VPSRLQ $52, in0, in4 \
	VPANDQ in3, in0, in1 \
	VPADDQ in4, in2, in2 \

#define BORROW_PROP(in0, in1, in2, in3) \
	VPSRAQ $63, in0, in3 \
	VPANDQ in2, in0, in0 \
	VPADDQ in3, in1, in1 \

#define IFMA_MUL_ACC_LH(in0, in1, in2, in3) \
	VPMADD52LUQ in0, in1, in2 \
	VPMADD52HUQ in0, in1, in3 \

#define COND_SELECT(in0, in1, in2) \
	VPTERNLOGQ $0xE2, in0, in1, in2 \

#define ZERO_REG(in0) \
	VPXORQ in0, in0, in0 \

// mulVec(res, a, b *Element, n uint64)
// Performs n multiplications using AVX-512 IFMA instructions
// Processes 8 elements in parallel using radix-52 representation
TEXT ·mulVec(SB), NOSPLIT, $0-32
	MOVQ res+0(FP), R14
	MOVQ a+8(FP), R13
	MOVQ b+16(FP), CX
	MOVQ n+24(FP), BX

	// Load constants for radix-52 conversion and reduction
	MOVQ         $0xFFFFFFFFFFFFF, R15 // 52-bit mask in R15
	VPBROADCASTQ R15, Z31              // Z31 = mask52
	MOVQ         $const_qInvNeg, AX
	ANDQ         R15, AX               // keep low 52 bits
	VPBROADCASTQ AX, Z30               // Z30 = qInvNeg52

	// Load modulus in radix-52 form
	// q in radix-52: Z25=ql0, Z26=ql1, Z27=ql2, Z28=ql3, Z29=ql4
	// Load q0-q3 and convert to radix-52
	MOVQ         $const_q0, R9
	MOVQ         $const_q1, R10
	MOVQ         $const_q2, R11
	MOVQ         $const_q3, R12
	MOVQ         R9, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z25
	SHRQ         $52, R9
	MOVQ         R10, R8
	SHLQ         $12, R8
	ORQ          R9, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z26
	SHRQ         $40, R10
	MOVQ         R11, R8
	SHLQ         $24, R8
	ORQ          R10, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z27
	SHRQ         $28, R11
	MOVQ         R12, R8
	SHLQ         $36, R8
	ORQ          R11, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z28
	SHRQ         $16, R12
	VPBROADCASTQ R12, Z29

loop_14:
	TESTQ BX, BX
	JEQ   done_15 // n == 0, we are done

	// Process 8 elements in parallel
	// Load and convert 8 elements from a[] to radix-52
	// Load 8 elements from R13
	// Load element words using gather pattern
	VMOVDQU64 0(R13), Z10
	VMOVDQU64 64(R13), Z11
	VMOVDQU64 128(R13), Z12
	VMOVDQU64 192(R13), Z13

	// Transpose 8 elements for vertical SIMD processing
	// 8x4 transpose using AVX-512 shuffles
	VPUNPCKLQDQ Z11, Z10, Z18              // [e0.a0, e2.a0, e0.a2, e2.a2, e1.a0, e3.a0, e1.a2, e3.a2]
	VPUNPCKHQDQ Z11, Z10, Z19              // [e0.a1, e2.a1, e0.a3, e2.a3, e1.a1, e3.a1, e1.a3, e3.a3]
	VPUNPCKLQDQ Z13, Z12, Z20              // [e4.a0, e6.a0, e4.a2, e6.a2, e5.a0, e7.a0, e5.a2, e7.a2]
	VPUNPCKHQDQ Z13, Z12, Z21              // [e4.a1, e6.a1, e4.a3, e6.a3, e5.a1, e7.a1, e5.a3, e7.a3]
	VSHUFI64X2  $0x88, Z20, Z18, Z14       // a0: lanes 0,2 from Z18 and Z20
	VSHUFI64X2  $0xDD, Z20, Z18, Z16       // a2: lanes 1,3 from Z18 and Z20
	VSHUFI64X2  $0x88, Z21, Z19, Z15       // a1: lanes 0,2 from Z19 and Z21
	VSHUFI64X2  $0xDD, Z21, Z19, Z17       // a3: lanes 1,3 from Z19 and Z21
	VMOVDQU64   ·permuteIdxIFMA<>(SB), Z22
	VPERMQ      Z14, Z22, Z14
	VPERMQ      Z15, Z22, Z15
	VPERMQ      Z16, Z22, Z16
	VPERMQ      Z17, Z22, Z17

	// Convert to radix-52
	VPANDQ Z31, Z14, Z0
	VPSRLQ $52, Z14, Z18
	VPSLLQ $12, Z15, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z1
	VPSRLQ $40, Z15, Z18
	VPSLLQ $24, Z16, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z2
	VPSRLQ $28, Z16, Z18
	VPSLLQ $36, Z17, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z3
	VPSRLQ $16, Z17, Z4

	// Load and convert 8 elements from b[] to radix-52
	// Load 8 elements from CX
	// Load element words using gather pattern
	VMOVDQU64 0(CX), Z10
	VMOVDQU64 64(CX), Z11
	VMOVDQU64 128(CX), Z12
	VMOVDQU64 192(CX), Z13

	// Transpose 8 elements for vertical SIMD processing
	// 8x4 transpose using AVX-512 shuffles
	VPUNPCKLQDQ Z11, Z10, Z18              // [e0.a0, e2.a0, e0.a2, e2.a2, e1.a0, e3.a0, e1.a2, e3.a2]
	VPUNPCKHQDQ Z11, Z10, Z19              // [e0.a1, e2.a1, e0.a3, e2.a3, e1.a1, e3.a1, e1.a3, e3.a3]
	VPUNPCKLQDQ Z13, Z12, Z20              // [e4.a0, e6.a0, e4.a2, e6.a2, e5.a0, e7.a0, e5.a2, e7.a2]
	VPUNPCKHQDQ Z13, Z12, Z21              // [e4.a1, e6.a1, e4.a3, e6.a3, e5.a1, e7.a1, e5.a3, e7.a3]
	VSHUFI64X2  $0x88, Z20, Z18, Z14       // a0: lanes 0,2 from Z18 and Z20
	VSHUFI64X2  $0xDD, Z20, Z18, Z16       // a2: lanes 1,3 from Z18 and Z20
	VSHUFI64X2  $0x88, Z21, Z19, Z15       // a1: lanes 0,2 from Z19 and Z21
	VSHUFI64X2  $0xDD, Z21, Z19, Z17       // a3: lanes 1,3 from Z19 and Z21
	VMOVDQU64   ·permuteIdxIFMA<>(SB), Z22
	VPERMQ      Z14, Z22, Z14
	VPERMQ      Z15, Z22, Z15
	VPERMQ      Z16, Z22, Z16
	VPERMQ      Z17, Z22, Z17

	// Convert to radix-52
	VPANDQ Z31, Z14, Z5
	VPSRLQ $52, Z14, Z18
	VPSLLQ $12, Z15, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z6
	VPSRLQ $40, Z15, Z18
	VPSLLQ $24, Z16, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z7
	VPSRLQ $28, Z16, Z18
	VPSLLQ $36, Z17, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z8
	VPSRLQ $16, Z17, Z9

	// Montgomery multiplication using IFMA (CIOS variant)
	// A = [Z0-Z4], B = [Z5-Z9], result in [Z0-Z4]
	ZERO_REG(Z10)
	ZERO_REG(Z11)
	ZERO_REG(Z12)
	ZERO_REG(Z13)
	ZERO_REG(Z14)
	ZERO_REG(Z15)

	// CIOS Round 0
	VPMADD52LUQ Z5, Z0, Z10
	VPMADD52HUQ Z5, Z0, Z11
	VPMADD52LUQ Z5, Z1, Z11
	VPMADD52HUQ Z5, Z1, Z12
	VPMADD52LUQ Z5, Z2, Z12
	VPMADD52HUQ Z5, Z2, Z13
	VPMADD52LUQ Z5, Z3, Z13
	VPMADD52HUQ Z5, Z3, Z14
	VPMADD52LUQ Z5, Z4, Z14
	VPMADD52HUQ Z5, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// CIOS Round 1
	VPMADD52LUQ Z6, Z0, Z10
	VPMADD52HUQ Z6, Z0, Z11
	VPMADD52LUQ Z6, Z1, Z11
	VPMADD52HUQ Z6, Z1, Z12
	VPMADD52LUQ Z6, Z2, Z12
	VPMADD52HUQ Z6, Z2, Z13
	VPMADD52LUQ Z6, Z3, Z13
	VPMADD52HUQ Z6, Z3, Z14
	VPMADD52LUQ Z6, Z4, Z14
	VPMADD52HUQ Z6, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// CIOS Round 2
	VPMADD52LUQ Z7, Z0, Z10
	VPMADD52HUQ Z7, Z0, Z11
	VPMADD52LUQ Z7, Z1, Z11
	VPMADD52HUQ Z7, Z1, Z12
	VPMADD52LUQ Z7, Z2, Z12
	VPMADD52HUQ Z7, Z2, Z13
	VPMADD52LUQ Z7, Z3, Z13
	VPMADD52HUQ Z7, Z3, Z14
	VPMADD52LUQ Z7, Z4, Z14
	VPMADD52HUQ Z7, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// CIOS Round 3
	VPMADD52LUQ Z8, Z0, Z10
	VPMADD52HUQ Z8, Z0, Z11
	VPMADD52LUQ Z8, Z1, Z11
	VPMADD52HUQ Z8, Z1, Z12
	VPMADD52LUQ Z8, Z2, Z12
	VPMADD52HUQ Z8, Z2, Z13
	VPMADD52LUQ Z8, Z3, Z13
	VPMADD52HUQ Z8, Z3, Z14
	VPMADD52LUQ Z8, Z4, Z14
	VPMADD52HUQ Z8, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// CIOS Round 4
	VPMADD52LUQ Z9, Z0, Z10
	VPMADD52HUQ Z9, Z0, Z11
	VPMADD52LUQ Z9, Z1, Z11
	VPMADD52HUQ Z9, Z1, Z12
	VPMADD52LUQ Z9, Z2, Z12
	VPMADD52HUQ Z9, Z2, Z13
	VPMADD52LUQ Z9, Z3, Z13
	VPMADD52HUQ Z9, Z3, Z14
	VPMADD52LUQ Z9, Z4, Z14
	VPMADD52HUQ Z9, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// Fused x16 shift + normalization
	VPSLLQ $4, Z10, Z0
	VPSLLQ $4, Z11, Z1
	VPSLLQ $4, Z12, Z2
	VPSLLQ $4, Z13, Z3
	VPSLLQ $4, Z14, Z4
	VPSRLQ $52, Z0, Z20
	VPSRLQ $52, Z1, Z21
	VPSRLQ $52, Z2, Z22
	VPSRLQ $52, Z3, Z23
	VPANDQ Z31, Z0, Z0
	VPANDQ Z31, Z1, Z1
	VPANDQ Z31, Z2, Z2
	VPANDQ Z31, Z3, Z3
	VPADDQ Z20, Z1, Z1
	VPADDQ Z21, Z2, Z2
	VPADDQ Z22, Z3, Z3
	VPADDQ Z23, Z4, Z4

	// Barrett reduction from [0, 32q) to [0, q)
	// k = (l4 * mu) >> 58, subtract k*q, then conditional subtract q
	MOVQ         $const_muBarrett52, AX
	VPBROADCASTQ AX, Z5
	VPSRLQ       $20, Z4, Z6
	VPMULUDQ     Z5, Z6, Z5
	VPSRLQ       $38, Z5, Z5

	// k*q using VPMADD52
	ZERO_REG(Z6)
	ZERO_REG(Z7)
	ZERO_REG(Z8)
	ZERO_REG(Z9)
	ZERO_REG(Z10)
	ZERO_REG(Z15)
	ZERO_REG(Z16)
	ZERO_REG(Z17)
	ZERO_REG(Z18)
	ZERO_REG(Z19)
	VPMADD52LUQ Z25, Z5, Z6
	VPMADD52LUQ Z26, Z5, Z7
	VPMADD52LUQ Z27, Z5, Z8
	VPMADD52LUQ Z28, Z5, Z9
	VPMADD52LUQ Z29, Z5, Z10
	VPMADD52HUQ Z25, Z5, Z15
	VPMADD52HUQ Z26, Z5, Z16
	VPMADD52HUQ Z27, Z5, Z17
	VPMADD52HUQ Z28, Z5, Z18
	VPMADD52HUQ Z29, Z5, Z19

	// Subtract k*q
	VPSUBQ Z6, Z0, Z0
	VPSUBQ Z7, Z1, Z1
	VPSUBQ Z8, Z2, Z2
	VPSUBQ Z9, Z3, Z3
	VPSUBQ Z10, Z4, Z4
	VPSUBQ Z15, Z1, Z1
	VPSUBQ Z16, Z2, Z2
	VPSUBQ Z17, Z3, Z3
	VPSUBQ Z18, Z4, Z4

	// Propagate borrows
	BORROW_PROP(Z0, Z1, Z31, Z15)
	BORROW_PROP(Z1, Z2, Z31, Z15)
	BORROW_PROP(Z2, Z3, Z31, Z15)
	BORROW_PROP(Z3, Z4, Z31, Z15)
	VPANDQ Z31, Z4, Z4

	// Final conditional subtraction of q
	VPSUBQ Z25, Z0, Z10
	VPSUBQ Z26, Z1, Z11
	VPSUBQ Z27, Z2, Z12
	VPSUBQ Z28, Z3, Z13
	VPSUBQ Z29, Z4, Z14
	VPSRAQ $63, Z10, Z20
	VPADDQ Z20, Z11, Z11
	VPSRAQ $63, Z11, Z20
	VPADDQ Z20, Z12, Z12
	VPSRAQ $63, Z12, Z20
	VPADDQ Z20, Z13, Z13
	VPSRAQ $63, Z13, Z20
	VPADDQ Z20, Z14, Z14
	VPSRAQ $63, Z14, Z20
	VPANDQ Z31, Z10, Z10
	VPANDQ Z31, Z11, Z11
	VPANDQ Z31, Z12, Z12
	VPANDQ Z31, Z13, Z13
	VPANDQ Z31, Z14, Z14
	COND_SELECT(Z10, Z20, Z0)
	COND_SELECT(Z11, Z20, Z1)
	COND_SELECT(Z12, Z20, Z2)
	COND_SELECT(Z13, Z20, Z3)
	COND_SELECT(Z14, Z20, Z4)

	// Convert result from radix-52 back to radix-64
	// Convert from radix-52 to radix-64
	VPSLLQ $52, Z1, Z18
	VPORQ  Z18, Z0, Z14
	VPSRLQ $12, Z1, Z18
	VPSLLQ $40, Z2, Z19
	VPORQ  Z19, Z18, Z15
	VPSRLQ $24, Z2, Z18
	VPSLLQ $28, Z3, Z19
	VPORQ  Z19, Z18, Z16
	VPSRLQ $36, Z3, Z18
	VPSLLQ $16, Z4, Z19
	VPORQ  Z19, Z18, Z17

	// Transpose back to AoS format and store
	// 4x8 reverse transpose (SoA to AoS)
	VMOVDQU64   ·permuteIdxIFMA<>(SB), Z22
	VPERMQ      Z14, Z22, Z14
	VPERMQ      Z15, Z22, Z15
	VPERMQ      Z16, Z22, Z16
	VPERMQ      Z17, Z22, Z17
	VPUNPCKLQDQ Z15, Z14, Z18              // pairs (a0,a1) for elements 0,1,4,5
	VPUNPCKHQDQ Z15, Z14, Z19              // pairs (a0,a1) for elements 2,3,6,7
	VPUNPCKLQDQ Z17, Z16, Z20              // pairs (a2,a3) for elements 0,1,4,5
	VPUNPCKHQDQ Z17, Z16, Z21              // pairs (a2,a3) for elements 2,3,6,7
	VSHUFI64X2  $0x44, Z20, Z18, Z10
	VSHUFI64X2  $0x44, Z21, Z19, Z11
	VSHUFI64X2  $0xEE, Z20, Z18, Z12
	VSHUFI64X2  $0xEE, Z21, Z19, Z13
	VSHUFI64X2  $0xD8, Z10, Z10, Z10
	VSHUFI64X2  $0xD8, Z11, Z11, Z11
	VSHUFI64X2  $0xD8, Z12, Z12, Z12
	VSHUFI64X2  $0xD8, Z13, Z13, Z13
	VMOVDQU64   Z10, 0(R14)
	VMOVDQU64   Z11, 64(R14)
	VMOVDQU64   Z12, 128(R14)
	VMOVDQU64   Z13, 192(R14)

	// Advance pointers
	ADDQ $256, R13
	ADDQ $256, CX
	ADDQ $256, R14
	DECQ BX        // processed 1 group of 8 elements
	JMP  loop_14

done_15:
	RET

// scalarMulVec(res, a, b *Element, n uint64)
// Performs n scalar multiplications using AVX-512 IFMA instructions
// Processes 8 elements in parallel using radix-52 representation
TEXT ·scalarMulVec(SB), NOSPLIT, $0-32
	MOVQ res+0(FP), R14
	MOVQ a+8(FP), R13
	MOVQ b+16(FP), CX
	MOVQ n+24(FP), BX

	// Load constants for radix-52 conversion and reduction
	MOVQ         $0xFFFFFFFFFFFFF, R15 // 52-bit mask in R15
	VPBROADCASTQ R15, Z31              // Z31 = mask52
	MOVQ         $const_qInvNeg, AX
	ANDQ         R15, AX               // keep low 52 bits
	VPBROADCASTQ AX, Z30               // Z30 = qInvNeg52

	// Load modulus in radix-52 form
	// q in radix-52: Z25=ql0, Z26=ql1, Z27=ql2, Z28=ql3, Z29=ql4
	// Load q0-q3 and convert to radix-52
	MOVQ         $const_q0, R9
	MOVQ         $const_q1, R10
	MOVQ         $const_q2, R11
	MOVQ         $const_q3, R12
	MOVQ         R9, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z25
	SHRQ         $52, R9
	MOVQ         R10, R8
	SHLQ         $12, R8
	ORQ          R9, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z26
	SHRQ         $40, R10
	MOVQ         R11, R8
	SHLQ         $24, R8
	ORQ          R10, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z27
	SHRQ         $28, R11
	MOVQ         R12, R8
	SHLQ         $36, R8
	ORQ          R11, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z28
	SHRQ         $16, R12
	VPBROADCASTQ R12, Z29

loop_16:
	TESTQ BX, BX
	JEQ   done_17 // n == 0, we are done

	// Process 8 elements in parallel
	// Load and convert 8 elements from a[] to radix-52
	// Load 8 elements from R13
	// Load element words using gather pattern
	VMOVDQU64 0(R13), Z10
	VMOVDQU64 64(R13), Z11
	VMOVDQU64 128(R13), Z12
	VMOVDQU64 192(R13), Z13

	// Transpose 8 elements for vertical SIMD processing
	// 8x4 transpose using AVX-512 shuffles
	VPUNPCKLQDQ Z11, Z10, Z18              // [e0.a0, e2.a0, e0.a2, e2.a2, e1.a0, e3.a0, e1.a2, e3.a2]
	VPUNPCKHQDQ Z11, Z10, Z19              // [e0.a1, e2.a1, e0.a3, e2.a3, e1.a1, e3.a1, e1.a3, e3.a3]
	VPUNPCKLQDQ Z13, Z12, Z20              // [e4.a0, e6.a0, e4.a2, e6.a2, e5.a0, e7.a0, e5.a2, e7.a2]
	VPUNPCKHQDQ Z13, Z12, Z21              // [e4.a1, e6.a1, e4.a3, e6.a3, e5.a1, e7.a1, e5.a3, e7.a3]
	VSHUFI64X2  $0x88, Z20, Z18, Z14       // a0: lanes 0,2 from Z18 and Z20
	VSHUFI64X2  $0xDD, Z20, Z18, Z16       // a2: lanes 1,3 from Z18 and Z20
	VSHUFI64X2  $0x88, Z21, Z19, Z15       // a1: lanes 0,2 from Z19 and Z21
	VSHUFI64X2  $0xDD, Z21, Z19, Z17       // a3: lanes 1,3 from Z19 and Z21
	VMOVDQU64   ·permuteIdxIFMA<>(SB), Z22
	VPERMQ      Z14, Z22, Z14
	VPERMQ      Z15, Z22, Z15
	VPERMQ      Z16, Z22, Z16
	VPERMQ      Z17, Z22, Z17

	// Convert to radix-52
	VPANDQ Z31, Z14, Z0
	VPSRLQ $52, Z14, Z18
	VPSLLQ $12, Z15, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z1
	VPSRLQ $40, Z15, Z18
	VPSLLQ $24, Z16, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z2
	VPSRLQ $28, Z16, Z18
	VPSLLQ $36, Z17, Z19
	VPORQ  Z18, Z19, Z18
	VPANDQ Z31, Z18, Z3
	VPSRLQ $16, Z17, Z4

	// Load scalar and convert to radix-52 (broadcast)
	MOVQ         0(CX), R9
	MOVQ         8(CX), R10
	MOVQ         16(CX), R11
	MOVQ         24(CX), R12
	MOVQ         R9, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z5
	SHRQ         $52, R9
	MOVQ         R10, R8
	SHLQ         $12, R8
	ORQ          R9, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z6
	SHRQ         $40, R10
	MOVQ         R11, R8
	SHLQ         $24, R8
	ORQ          R10, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z7
	SHRQ         $28, R11
	MOVQ         R12, R8
	SHLQ         $36, R8
	ORQ          R11, R8
	ANDQ         R15, R8
	VPBROADCASTQ R8, Z8
	SHRQ         $16, R12
	VPBROADCASTQ R12, Z9

	// Montgomery multiplication using IFMA (CIOS variant)
	// A = [Z0-Z4], B = [Z5-Z9], result in [Z0-Z4]
	ZERO_REG(Z10)
	ZERO_REG(Z11)
	ZERO_REG(Z12)
	ZERO_REG(Z13)
	ZERO_REG(Z14)
	ZERO_REG(Z15)

	// CIOS Round 0
	VPMADD52LUQ Z5, Z0, Z10
	VPMADD52HUQ Z5, Z0, Z11
	VPMADD52LUQ Z5, Z1, Z11
	VPMADD52HUQ Z5, Z1, Z12
	VPMADD52LUQ Z5, Z2, Z12
	VPMADD52HUQ Z5, Z2, Z13
	VPMADD52LUQ Z5, Z3, Z13
	VPMADD52HUQ Z5, Z3, Z14
	VPMADD52LUQ Z5, Z4, Z14
	VPMADD52HUQ Z5, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// CIOS Round 1
	VPMADD52LUQ Z6, Z0, Z10
	VPMADD52HUQ Z6, Z0, Z11
	VPMADD52LUQ Z6, Z1, Z11
	VPMADD52HUQ Z6, Z1, Z12
	VPMADD52LUQ Z6, Z2, Z12
	VPMADD52HUQ Z6, Z2, Z13
	VPMADD52LUQ Z6, Z3, Z13
	VPMADD52HUQ Z6, Z3, Z14
	VPMADD52LUQ Z6, Z4, Z14
	VPMADD52HUQ Z6, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// CIOS Round 2
	VPMADD52LUQ Z7, Z0, Z10
	VPMADD52HUQ Z7, Z0, Z11
	VPMADD52LUQ Z7, Z1, Z11
	VPMADD52HUQ Z7, Z1, Z12
	VPMADD52LUQ Z7, Z2, Z12
	VPMADD52HUQ Z7, Z2, Z13
	VPMADD52LUQ Z7, Z3, Z13
	VPMADD52HUQ Z7, Z3, Z14
	VPMADD52LUQ Z7, Z4, Z14
	VPMADD52HUQ Z7, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// CIOS Round 3
	VPMADD52LUQ Z8, Z0, Z10
	VPMADD52HUQ Z8, Z0, Z11
	VPMADD52LUQ Z8, Z1, Z11
	VPMADD52HUQ Z8, Z1, Z12
	VPMADD52LUQ Z8, Z2, Z12
	VPMADD52HUQ Z8, Z2, Z13
	VPMADD52LUQ Z8, Z3, Z13
	VPMADD52HUQ Z8, Z3, Z14
	VPMADD52LUQ Z8, Z4, Z14
	VPMADD52HUQ Z8, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// CIOS Round 4
	VPMADD52LUQ Z9, Z0, Z10
	VPMADD52HUQ Z9, Z0, Z11
	VPMADD52LUQ Z9, Z1, Z11
	VPMADD52HUQ Z9, Z1, Z12
	VPMADD52LUQ Z9, Z2, Z12
	VPMADD52HUQ Z9, Z2, Z13
	VPMADD52LUQ Z9, Z3, Z13
	VPMADD52HUQ Z9, Z3, Z14
	VPMADD52LUQ Z9, Z4, Z14
	VPMADD52HUQ Z9, Z4, Z15
	VPSRLQ      $52, Z10, Z20
	VPANDQ      Z31, Z10, Z10
	VPADDQ      Z20, Z11, Z11
	VPXORQ      Z20, Z20, Z20
	VPMADD52LUQ Z30, Z10, Z20
	VPANDQ      Z31, Z20, Z20
	VPMADD52LUQ Z25, Z20, Z10
	VPMADD52HUQ Z25, Z20, Z11
	VPMADD52LUQ Z26, Z20, Z11
	VPMADD52HUQ Z26, Z20, Z12
	VPMADD52LUQ Z27, Z20, Z12
	VPMADD52HUQ Z27, Z20, Z13
	VPMADD52LUQ Z28, Z20, Z13
	VPMADD52HUQ Z28, Z20, Z14
	VPMADD52LUQ Z29, Z20, Z14
	VPMADD52HUQ Z29, Z20, Z15
	VPSRLQ      $52, Z10, Z20
	VPADDQ      Z20, Z11, Z10
	VMOVDQA64   Z12, Z11
	VMOVDQA64   Z13, Z12
	VMOVDQA64   Z14, Z13
	VMOVDQA64   Z15, Z14
	VPXORQ      Z15, Z15, Z15

	// Fused x16 shift + normalization
	VPSLLQ $4, Z10, Z0
	VPSLLQ $4, Z11, Z1
	VPSLLQ $4, Z12, Z2
	VPSLLQ $4, Z13, Z3
	VPSLLQ $4, Z14, Z4
	VPSRLQ $52, Z0, Z20
	VPSRLQ $52, Z1, Z21
	VPSRLQ $52, Z2, Z22
	VPSRLQ $52, Z3, Z23
	VPANDQ Z31, Z0, Z0
	VPANDQ Z31, Z1, Z1
	VPANDQ Z31, Z2, Z2
	VPANDQ Z31, Z3, Z3
	VPADDQ Z20, Z1, Z1
	VPADDQ Z21, Z2, Z2
	VPADDQ Z22, Z3, Z3
	VPADDQ Z23, Z4, Z4

	// Barrett reduction from [0, 32q) to [0, q)
	// k = (l4 * mu) >> 58, subtract k*q, then conditional subtract q
	MOVQ         $const_muBarrett52, AX
	VPBROADCASTQ AX, Z5
	VPSRLQ       $20, Z4, Z6
	VPMULUDQ     Z5, Z6, Z5
	VPSRLQ       $38, Z5, Z5

	// k*q using VPMADD52
	ZERO_REG(Z6)
	ZERO_REG(Z7)
	ZERO_REG(Z8)
	ZERO_REG(Z9)
	ZERO_REG(Z10)
	ZERO_REG(Z15)
	ZERO_REG(Z16)
	ZERO_REG(Z17)
	ZERO_REG(Z18)
	ZERO_REG(Z19)
	VPMADD52LUQ Z25, Z5, Z6
	VPMADD52LUQ Z26, Z5, Z7
	VPMADD52LUQ Z27, Z5, Z8
	VPMADD52LUQ Z28, Z5, Z9
	VPMADD52LUQ Z29, Z5, Z10
	VPMADD52HUQ Z25, Z5, Z15
	VPMADD52HUQ Z26, Z5, Z16
	VPMADD52HUQ Z27, Z5, Z17
	VPMADD52HUQ Z28, Z5, Z18
	VPMADD52HUQ Z29, Z5, Z19

	// Subtract k*q
	VPSUBQ Z6, Z0, Z0
	VPSUBQ Z7, Z1, Z1
	VPSUBQ Z8, Z2, Z2
	VPSUBQ Z9, Z3, Z3
	VPSUBQ Z10, Z4, Z4
	VPSUBQ Z15, Z1, Z1
	VPSUBQ Z16, Z2, Z2
	VPSUBQ Z17, Z3, Z3
	VPSUBQ Z18, Z4, Z4

	// Propagate borrows
	BORROW_PROP(Z0, Z1, Z31, Z15)
	BORROW_PROP(Z1, Z2, Z31, Z15)
	BORROW_PROP(Z2, Z3, Z31, Z15)
	BORROW_PROP(Z3, Z4, Z31, Z15)
	VPANDQ Z31, Z4, Z4

	// Final conditional subtraction of q
	VPSUBQ Z25, Z0, Z10
	VPSUBQ Z26, Z1, Z11
	VPSUBQ Z27, Z2, Z12
	VPSUBQ Z28, Z3, Z13
	VPSUBQ Z29, Z4, Z14
	VPSRAQ $63, Z10, Z20
	VPADDQ Z20, Z11, Z11
	VPSRAQ $63, Z11, Z20
	VPADDQ Z20, Z12, Z12
	VPSRAQ $63, Z12, Z20
	VPADDQ Z20, Z13, Z13
	VPSRAQ $63, Z13, Z20
	VPADDQ Z20, Z14, Z14
	VPSRAQ $63, Z14, Z20
	VPANDQ Z31, Z10, Z10
	VPANDQ Z31, Z11, Z11
	VPANDQ Z31, Z12, Z12
	VPANDQ Z31, Z13, Z13
	VPANDQ Z31, Z14, Z14
	COND_SELECT(Z10, Z20, Z0)
	COND_SELECT(Z11, Z20, Z1)
	COND_SELECT(Z12, Z20, Z2)
	COND_SELECT(Z13, Z20, Z3)
	COND_SELECT(Z14, Z20, Z4)

	// Convert result from radix-52 back to radix-64
	// Convert from radix-52 to radix-64
	VPSLLQ $52, Z1, Z18
	VPORQ  Z18, Z0, Z14
	VPSRLQ $12, Z1, Z18
	VPSLLQ $40, Z2, Z19
	VPORQ  Z19, Z18, Z15
	VPSRLQ $24, Z2, Z18
	VPSLLQ $28, Z3, Z19
	VPORQ  Z19, Z18, Z16
	VPSRLQ $36, Z3, Z18
	VPSLLQ $16, Z4, Z19
	VPORQ  Z19, Z18, Z17

	// Transpose back to AoS format and store
	// 4x8 reverse transpose (SoA to AoS)
	VMOVDQU64   ·permuteIdxIFMA<>(SB), Z22
	VPERMQ      Z14, Z22, Z14
	VPERMQ      Z15, Z22, Z15
	VPERMQ      Z16, Z22, Z16
	VPERMQ      Z17, Z22, Z17
	VPUNPCKLQDQ Z15, Z14, Z18              // pairs (a0,a1) for elements 0,1,4,5
	VPUNPCKHQDQ Z15, Z14, Z19              // pairs (a0,a1) for elements 2,3,6,7
	VPUNPCKLQDQ Z17, Z16, Z20              // pairs (a2,a3) for elements 0,1,4,5
	VPUNPCKHQDQ Z17, Z16, Z21              // pairs (a2,a3) for elements 2,3,6,7
	VSHUFI64X2  $0x44, Z20, Z18, Z10
	VSHUFI64X2  $0x44, Z21, Z19, Z11
	VSHUFI64X2  $0xEE, Z20, Z18, Z12
	VSHUFI64X2  $0xEE, Z21, Z19, Z13
	VSHUFI64X2  $0xD8, Z10, Z10, Z10
	VSHUFI64X2  $0xD8, Z11, Z11, Z11
	VSHUFI64X2  $0xD8, Z12, Z12, Z12
	VSHUFI64X2  $0xD8, Z13, Z13, Z13
	VMOVDQU64   Z10, 0(R14)
	VMOVDQU64   Z11, 64(R14)
	VMOVDQU64   Z12, 128(R14)
	VMOVDQU64   Z13, 192(R14)

	// Advance pointers
	ADDQ $256, R13
	ADDQ $256, R14
	DECQ BX        // processed 1 group of 8 elements
	JMP  loop_16

done_17:
	RET
